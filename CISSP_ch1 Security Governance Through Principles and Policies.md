# Principles of Security Models, Design, and Capabilities

- Domain 3.0: Security Architecture and Engineering

## Secure Design Principlesï¼ˆ**å®‰å…¨è®¾è®¡åŸåˆ™**ï¼‰

Security should be a consideration at every stage of a systemâ€™s development. Programmers, developers, engineers, and so on should strive to build security into every application or system they develop, with greater levels of security provided to critical applications and those that process sensitive information. Itâ€™s extremely important to consider the security implications of a development project in the early stages because itâ€™s much easier to build security into a system during development than it is to add security to an existing system. Developers should research, implement, and manage engineering processes using secure design principles.

### 1. Objects and Subjects

è®¿é—®æ§åˆ¶æ¨¡å‹ï¼ˆå¦‚ MACã€DACã€RBACï¼‰å‡æ˜¯å›´ç»• Subject/Object å’Œ Access æƒé™è®¾è®¡ã€‚

| æ¦‚å¿µ                | æè¿°                                                         |
| ------------------- | ------------------------------------------------------------ |
| **Subjectï¼ˆä¸»ä½“ï¼‰** | ä¸»åŠ¨å‘èµ·è®¿é—®è¯·æ±‚çš„å®ä½“ï¼Œå¦‚ï¼šç”¨æˆ·ã€è¿›ç¨‹ã€æœåŠ¡ã€ç¨‹åºã€ä¸»æœºç­‰ã€‚ |
| **Objectï¼ˆå¯¹è±¡ï¼‰**  | è¢«è®¿é—®çš„èµ„æºï¼Œå¦‚ï¼šæ–‡ä»¶ã€æ•°æ®åº“ã€å†…å­˜ã€æ‰“å°æœºã€æœåŠ¡ç­‰ã€‚       |
| **Accessï¼ˆè®¿é—®ï¼‰**  | æ˜¯ Subject ä¸ Object ä¹‹é—´çš„å…³ç³»ï¼Œå¦‚è¯»å–ã€å†™å…¥ã€åˆ é™¤ã€å¤‡ä»½ã€æ‰“å°ç­‰è¡Œä¸ºã€‚ |

Controlling access to any resource in a secure system involves **two entities.** 

#### Subject

The ***subject*** is the active entity that makes a request to access a resource. A subject is commonly a user, but it can also be a process, program, computer, or organization. 

#### Object

The ***object*** is the passive entity that the subject wants to access. An object is commonly a resource, such as a file or printer, but it can also be a user, process, program, computer, or organization.

You want to keep a broad understanding of the terms of subject and object, rather than only considering users and files. 

**Access is the relationship between a subject and object,** which could include reading, writing, changing, deleting, printing, moving, backing up, and many other operations or activities.

Keep in mind that the actual entities referenced by the terms subject and object are specific to an individual access request. The entity serving as the object in one access event could serve as the subject in another. 

#### Transitive trustï¼ˆä¼ é€’ä¿¡ä»»ï¼‰

- ä¼ é€’ä¿¡ä»» = A ä¿¡ä»» Bï¼ŒB åˆä¿¡ä»» C â‡’ A é—´æ¥ä¿¡ä»» Cï¼›
- å®‰å…¨é£é™©ï¼šå¦‚æœ B è¢«æ”»å‡»ï¼ŒA å¯èƒ½åœ¨ä¸çŸ¥æƒ…æƒ…å†µä¸‹å°†ä¿¡ä»»ä¼ é€’ç»™ä¸å®‰å…¨çš„ Cï¼›
- ä¾‹å­ï¼šSSO ç³»ç»Ÿé“¾å¼è·³è½¬ï¼›è”åˆèº«ä»½è®¤è¯ï¼›B2B API ä¿¡ä»»é“¾ã€‚

It  is the concept that if A trusts B and B trusts C, then A inherits trust of C through the transitive property. When A requests data from B and then B requests data from C, the data that A receives is essentially from C. Transitive trust is a serious security concern because it may enable bypassing of restrictions or limitations between A and C, especially if A and C both support interaction with B.

### 2. Closed and Open Systems

Systems are designed and built according to one of two differing philosophies. 

#### Closed systemï¼ˆé—­åˆç³»ç»Ÿï¼‰

It  is designed to work well with a narrow range of other systems, generally all from the same manufacturer. The standards for closed systems are often proprietary and not normally disclosed. 

Closed systems are harder to integrate with unlike systems, but this â€œfeatureâ€ could make them more secure. A closed system is often composed of proprietary hardware and software that does not incorporate industry standards or offer an open API. This lack of integration ease means that attacks that typically focus on generic system components either will not work or must be customized to be successful. In many cases, attacking a closed system is harder than launching an attack on an open system, since a unique exploit of a unique vulnerability would be required. In addition to the lack of known vulnerable components on a closed system, it is often necessary to possess more in-depth knowledge of the specific target system to launch a successful attack.

#### Open systemï¼ˆå¼€æ”¾ç³»ç»Ÿï¼‰

 It is designed using agreed-upon industry standards. Open systems are much easier to integrate with systems from different manufacturers that support the same standards or that use compatible *application programming interfaces (APIs)*.

Open systems are generally far easier to integrate with other open systems. It is easy, for example, to create a local area network (LAN) with a Microsoft Windows Server machine, a Linux machine, and a Macintosh machine. Although all three computers use different operating systems and could represent up to three different hardware architectures, each supports industry standards and open APIs, which makes it easy for network (or other) communications to occur. This ease of interoperability comes at a price, however. Because standard communications components are incorporated into each of these three open systems, there are far more predictable entry points and methods for launching attacks. In general, their openness makes them more vulnerable to attack, and their widespread availability makes it possible for attackers to find plenty of potential targets. Also, open systems are more popular and widely deployed than closed systems and thus attract more attention from attackers. An attacker who develops basic attacking skills will find more targets that are open systems than closed ones. Inarguably, thereâ€™s a greater body of shared experience and knowledge on how to attack open systems than there is for closed systems. The security of an open system is therefore more dependent on the use of secure and defensive coding practices and a thoughtful defense-in-depth deployment strategy.

#### é—­åˆç³»ç»Ÿ vs. å¼€æ”¾ç³»ç»Ÿ

| ç‰¹æ€§     | **Closed System**          | **Open System**          |
| -------- | -------------------------- | ------------------------ |
| æ¥å£æ ‡å‡† | ä¸“æœ‰ï¼Œéå…¬å¼€               | è¡Œä¸šæ ‡å‡†ï¼Œå…¬å¼€           |
| å¯æ‰©å±•æ€§ | å·®ï¼Œä¸æ˜“é›†æˆ               | å¼ºï¼Œæ˜“äºæ‰©å±•             |
| å®‰å…¨æ€§   | è¾ƒé«˜ï¼ˆä¸é€æ˜ï¼Œæ”»å‡»é—¨æ§›é«˜ï¼‰ | è¾ƒå¼±ï¼ˆé€æ˜æ€§å¸¦æ¥æ”»å‡»é¢ï¼‰ |
| ç¤ºä¾‹     | å†›ç”¨é€šä¿¡è®¾å¤‡ã€æŸäº›å·¥æ§ç³»ç»Ÿ | ä¼ä¸š IT ç³»ç»Ÿã€äº’è”ç½‘æœåŠ¡ |

**æ€»ç»“**ï¼š

- å°é—­ç³»ç»Ÿå®‰å…¨æ€§æ›´å¼ºï¼Œä½†ç¼ºä¹å…¼å®¹æ€§ï¼›
- å¼€æ”¾ç³»ç»Ÿæ˜“è¢«æ”»å‡»ï¼Œä½†ä¹Ÿæ›´çµæ´»ã€æ›´æ™®åŠï¼Œå› æ­¤éœ€æ›´å¤š**é˜²å¾¡æ€§ç¼–ç¨‹**å’Œ**æ·±åº¦é˜²å¾¡**ç­–ç•¥ã€‚

#### APIs

| ç»´åº¦     | æè¿°                                                   |
| -------- | ------------------------------------------------------ |
| **å®šä¹‰** | ä¸€ç§å…è®¸ä¸åŒç»„ä»¶ï¼ˆæœåŠ¡ã€åº”ç”¨ã€å¹³å°ï¼‰ä¹‹é—´é€šä¿¡çš„æ¥å£æ ‡å‡† |
| **ç»„æˆ** | åŒ…æ‹¬è¯·æ±‚ç»“æ„ã€å‚æ•°ç±»å‹ã€è®¤è¯æ–¹å¼ã€æ•°æ®æ ¼å¼ã€å®‰å…¨æ§åˆ¶ç­‰ |
| **ä½œç”¨** | å®ç°ç³»ç»Ÿä¹‹é—´äº’æ“ä½œï¼›ç°ä»£ç½‘ç»œæ¶æ„çš„â€œè¿æ¥å™¨â€             |

âš ï¸ **API å®‰å…¨å…³æ³¨ç‚¹**ï¼š

- èº«ä»½è®¤è¯ï¼ˆå¦‚ OAuthã€API å¯†é’¥ï¼‰
- è¾“å…¥éªŒè¯ï¼ˆé˜²æ­¢æ³¨å…¥æ”»å‡»ï¼‰
- è®¿é—®æ§åˆ¶ï¼ˆåªå…è®¸æˆæƒèµ„æºè®¿é—®ï¼‰
- æ—¥å¿—å®¡è®¡å’Œé€Ÿç‡é™åˆ¶ï¼ˆé˜²æ»¥ç”¨ï¼‰

An API is a defined set of interactions allowed between computing elements, such as applications, services, networking, firmware, and hardware. An API defines the types of requests that can be made, the exact means to make the requests, the data forms of the exchange, and other related requirements (such as authentication and/or session encryption).

APIs make interoperability of computing elements possible. Without APIs, computing components would be unable to directly interact and information sharing would not be easy. APIs are what make modern computing and the internet possible. The app on your smartphone talks to the phoneâ€™s operating system via an API; the phoneâ€™s operating system talks over the telco or Wi-Fi network via an API to reach the cloud serviceâ€™s API in order to submit a request and receive a response.

#### Open Source vs. Closed Sourceï¼ˆå¼€æº vs. é—­æºï¼‰

| ç±»å‹         | æè¿°                   | ä¼˜åŠ£åˆ†æ                                      |
| ------------ | ---------------------- | --------------------------------------------- |
| **å¼€æºè½¯ä»¶** | ä»£ç å…¬å¼€ï¼Œæ”¯æŒç¤¾åŒºå®¡è®¡ | ä¼˜ï¼šå®¡æŸ¥é€æ˜ï¼ŒååŒä¿®å¤ åŠ£ï¼šæ”»å‡»è€…ä¹Ÿèƒ½çœ‹åˆ°æ¼æ´ |
| **é—­æºè½¯ä»¶** | ä»£ç ä¿å¯†ï¼Œç”±ä¾›åº”å•†ç»´æŠ¤ | ä¼˜ï¼šä¸æ˜“é€†å‘ åŠ£ï¼šâ€œå®‰å…¨é€šè¿‡æ¨¡ç³Šâ€éé•¿ä¹…ä¹‹è®¡     |

âš ï¸ **æ³¨æ„åŒºåˆ†æœ¯è¯­ï¼š**

- â€œå¼€æº/é—­æºâ€ æ˜¯å…³äºâ€œæºç æ˜¯å¦å…¬å¼€â€çš„å±æ€§ï¼›
- â€œå¼€æ”¾/é—­åˆç³»ç»Ÿâ€ æ˜¯å…³äºâ€œç³»ç»Ÿæ¶æ„æ˜¯å¦æ ‡å‡†åŒ–â€çš„å±æ€§ï¼›
- äºŒè€…**ä¸æ˜¯åŒä¸€ä¸ªæ¦‚å¿µ**ï¼Œä½† CISSP ä¼šåˆ»æ„æ··æ·†è€ƒæŸ¥ï¼Œéœ€ç²¾å‡†è¾¨åˆ«ã€‚

An **open source solution** is one where the source code, and other internal logic, is exposed to the public. A closed source solution is one where the source code and other internal logic is hidden from the public. Open source solutions often depend on public inspection and review to improve the product over time. 

**Closed source solutions** are more dependent on the vendor/programmer to revise the product over time. Both open source and closed source solutions can be available for sale or at no charge, but the term commercial typically implies closed source. However, closed source code is sometimes revealed through either vendor compromise or through decompiling or disassembly. The former is always a breach of ethics and often the law, whereas the latter is a standard element in ethical reverse engineering or systems analysis.

It is also the case that a closed source program can be either an open system or a closed system, and an open source program can be either an open system or a closed system. Since these terms are so similar, it is essential to read questions carefully. 

**CISSPè€ƒç‚¹**

| è€ƒç‚¹                                     | ä½ éœ€è¦è®°ä½çš„æ˜¯                           |
| ---------------------------------------- | ---------------------------------------- |
| **Access ä¸‰å…ƒç»„**                        | ä¸»ä½“ Subjectã€å¯¹è±¡ Objectã€è¡Œä¸º Access   |
| **API çš„å…³é”®å®‰å…¨é—®é¢˜**                   | è®¤è¯ã€æˆæƒã€è¾“å…¥éªŒè¯ã€åŠ å¯†ä¼ è¾“ã€æ—¥å¿—å®¡è®¡ |
| **å¼€æ”¾ç³»ç»Ÿå®‰å…¨ç­–ç•¥**                     | é˜²å¾¡æ€§ç¼–ç¨‹ + é›¶ä¿¡ä»» + æ·±åº¦é˜²å¾¡           |
| **å¼€æº vs é—­æº ä¸ç­‰äº å¼€æ”¾ vs é—­åˆç³»ç»Ÿ** | è¯»é¢˜æ—¶æ³¨æ„è¯­ä¹‰åŒºåˆ«                       |
| **ä¼ é€’ä¿¡ä»»çš„é£é™©**                       | å°½é‡é™åˆ¶è·¨ç³»ç»Ÿé—´çš„ä¿¡ä»»é“¾ä¼ æ’­             |

### 3. Secure Defaultsï¼ˆå®‰å…¨é»˜è®¤è®¾ç½®ï¼‰

**é»˜è®¤è®¾ç½® â‰  å®‰å…¨è®¾ç½®**

- **æ ¸å¿ƒç†å¿µ**ï¼š**äº§å“å‡ºå‚é»˜è®¤é…ç½®é€šå¸¸å¹¶ä¸å®‰å…¨ã€‚**
- ä¸»è¦åŸå› ï¼š
  - å‚å•†æ›´æ³¨é‡åŠŸèƒ½å¯ç”¨æ€§ã€å…¼å®¹æ€§ï¼Œè€Œéé»˜è®¤å®‰å…¨æ€§ï¼›
  - å®‰å…¨é…ç½®é€šå¸¸æ„å‘³ç€**ç¦ç”¨æŸäº›ç‰¹æ€§ã€åŠ å¼ºè®¿é—®æ§åˆ¶ã€é™ä½æ˜“ç”¨æ€§**ï¼›
  - é»˜è®¤é…ç½®æ˜¯é¢å‘æ‰€æœ‰ç”¨æˆ·è®¾è®¡çš„ï¼Œä¸èƒ½æ»¡è¶³ç»„ç»‡å®šåˆ¶çš„å®‰å…¨ç­–ç•¥ã€‚

Never assume that the default settings of any product are secure. They typically are not, because secure settings would likely get in the way of existing business tasks or system operations. It is always up to the systemâ€™s administrator and/or company security staff to alter a productâ€™s settings to comply with the organizationâ€™s security policies. Unless your organization hired the developer, that developer did not craft the code or choose settings specifically for your organizationâ€™s use of their product.

A much better assumption is that the default settings of a product are the worst possible options for your organization. Therefore, you need to review each and every setting to determine what it does and what you need it to be configured to do in order to optimize security while supporting business operations.

**Secure by Default ä»£è¡¨ç†å¿µ**

- Microsoft SDL ä¸­çš„ SD3+Cï¼ˆSecure by Design, by Default, by Deployment + Communicationï¼‰ï¼›
- ä¸€äº›ç°ä»£å®‰å…¨äº§å“å·²å°è¯•é»˜è®¤å¯ç”¨æœ€å®‰å…¨çš„é…ç½®ï¼Œå¦‚é»˜è®¤å¯ç”¨ TLS åŠ å¯†ã€é»˜è®¤ç¦ç”¨å¤–éƒ¨ API æ¥å…¥ã€‚

Fortunately, there is some movement toward more secure defaults. Microsoftâ€™s Security Development Lifecycle (SDL) has a motto named SD3+C, which includes the phrase â€œ**Secure by Default**.â€ Some products, especially security products, may now be designed with their most secure settings enabled by default. However, such a locked-down product will have fewer enabled capabilities and will likely be less user friendly. Thus, while being more secure, secure defaults may be an obstacle for those who only want their systems to function.

##### å®‰å…¨ä¸“å®¶çš„èŒè´£

| å®‰å…¨ç®¡ç†å‘˜èŒè´£                   | å¼€å‘è€…èŒè´£                         |
| -------------------------------- | ---------------------------------- |
| å®¡æ ¸æ‰€æœ‰è®¾ç½®é¡¹ï¼Œè¯„ä¼°é£é™©         | è®¾è®¡æ–‡æ¡£è¯´æ˜é…ç½®é¡¹çš„ä½œç”¨ã€å®‰å…¨å½±å“ |
| é…ç½®äº§å“ä½¿å…¶ç¬¦åˆç»„ç»‡å®‰å…¨æ ‡å‡†     | æä¾›â€œå®‰å…¨é»˜è®¤æ¨¡æ¿â€æˆ–é…ç½®å¯¼å…¥æ–‡ä»¶   |
| é‡‡ç”¨â€œæœ€å°æƒé™â€å’Œâ€œåŠŸèƒ½æœ€å°‘åŒ–â€åŸåˆ™ | é‡‡çº³ Secure by Default åŸåˆ™        |

If you are a developer, then it is your responsibility to create detailed explanations of each of the configuration options of your product. You canâ€™t assume that customers know everything about your product, especially what the configuration settings are and what each option does to alter its features, operations, communications, and so forth. 

You may be required to have default settings to make the product as easy to install as possible, but you may be able to provide one or more configurations in either written instructional form or in a file that can be imported or applied. This will go a long way to assist customers with gaining the most advantage from your product while minimizing the security risks.

### 4. Fail Securelyï¼ˆå®‰å…¨æ•…éšœå¤„ç†ï¼‰

**å®šä¹‰ï¼š**ç³»ç»Ÿå‘ç”Ÿé”™è¯¯æˆ–å´©æºƒæ—¶ï¼Œä»èƒ½ç»´æŠ¤æœ€å°å®‰å…¨è¾¹ç•Œï¼Œä¸è®©æ”»å‡»è€…è¶è™šè€Œå…¥ã€‚

System failures can occur due to a wide range of causes. Once the failure event occurs, how the system or environment handles the failure is important. The most desired result is for an application to fail securely. The first type of failure management is programmatic error handling (aka ***exception handling***). This is the process where a programmer codes in mechanisms to anticipate and defend against errors in order to avoid the termination of execution. Error handling is the inclusion of code that will attempt to handle errors when they arise before they can cause harm or interrupt execution.

#### **ç¼–ç¨‹å±‚é¢çš„å®‰å…¨å¤±è´¥ï¼ˆException Handlingï¼‰**

| æ–¹æ³•             | æè¿°                                                 |
| ---------------- | ---------------------------------------------------- |
| `try...catch`    | æ•è·æ½œåœ¨é”™è¯¯ï¼Œä¸è®©ç¨‹åºç›´æ¥å´©æºƒæˆ–æš´éœ²å†…éƒ¨ä¿¡æ¯         |
| **è¾“å…¥éªŒè¯**     | æ£€æŸ¥é•¿åº¦ã€å­—ç¬¦ç±»å‹ã€é»‘åå•è¿‡æ»¤ã€è½¬ä¹‰å…ƒå­—ç¬¦ï¼Œé˜²æ­¢æ³¨å…¥ |
| **é”™è¯¯è¾“å‡ºæ§åˆ¶** | ä¸æ³„éœ²å †æ ˆä¿¡æ¯ã€è·¯å¾„ã€å˜é‡åï¼Œé˜²æ­¢ä¿¡æ¯æ³„éœ²æ¼æ´       |

One such mechanism, which is supported by many languages, is a ***try..catch statement***. This logical block statement is used to place code that could result in an error on the try branch, and then code that will be executed if there is an error on the catch branch. This is similar to if..then..else statements, but it is designed to deftly handle errors.

Other mechanisms are to avoid or prevent errors, especially as related to user input. ***Input sanitization, input filtering, or input validation*** are some of the terms used to refer to this concept. This often includes checking the input for length, filtering against a block list of unwanted input, and escaping metacharacters.

#### ç³»ç»Ÿçº§æ•…éšœå“åº”è®¾è®¡ï¼šFail-Safe vs. Fail-Secure

##### âœ… **Fail-Safeï¼ˆç‰©ç†ä¼˜å…ˆï¼Œä¿æŠ¤äººï¼‰**

- åº”ç”¨äºæ¶‰åŠäººèº«å®‰å…¨çš„ç‰©ç†è®¾å¤‡ï¼Œå¦‚é—¨ç¦ã€ç”µæ¢¯ã€è‡ªåŠ¨ç­ç«è£…ç½®ï¼›
- å¤±æ•ˆçŠ¶æ€ä¸‹ï¼Œç³»ç»Ÿ**è‡ªåŠ¨æ”¾è¡Œ**ï¼Œä¼˜å…ˆä¿éšœäººå‘˜é€ƒç”Ÿã€ç”Ÿå‘½å®‰å…¨ï¼›
- ä¹Ÿç§°ï¼š**Fail-Openï¼ˆåœ¨ç‰©ç†åœºæ™¯ä¸­ï¼‰**

ğŸ“Œ ä¾‹å­ï¼šå¤§æ¥¼ç«ç¾æ—¶ï¼Œç”µç£é”é—¨ç¦å¤±æ•ˆï¼Œè‡ªåŠ¨æ‰“å¼€ã€‚

------

##### âœ… **Fail-Secureï¼ˆæ•°å­—ä¼˜å…ˆï¼Œä¿æŠ¤æ•°æ®ï¼‰**

- åº”ç”¨äºæ•°å­—ç³»ç»Ÿï¼Œç›®æ ‡æ˜¯**ç»´æŠ¤æ•°æ®æœºå¯†æ€§å’Œå®Œæ•´æ€§**ï¼›
- å¤±æ•ˆçŠ¶æ€ä¸‹ï¼Œç³»ç»Ÿ**å…³é—­æœåŠ¡æˆ–è¿æ¥**ï¼Œé˜»æ­¢æœªæˆæƒè®¿é—®ï¼›
- åˆç§°ï¼š**Fail-Closedã€Fail-Safeï¼ˆåœ¨æ•°å­—é¢†åŸŸï¼‰**

ğŸ“Œ ä¾‹å­ï¼šé˜²ç«å¢™æˆ– VPN ç³»ç»Ÿåœ¨ç³»ç»Ÿå¼‚å¸¸æˆ–å´©æºƒæ—¶ä¸­æ–­æ‰€æœ‰è¿æ¥ï¼Œé˜²æ­¢é€šä¿¡æ³„éœ²ã€‚

When a program fails securely, it was able to do so only because it was designed and programmed to. When secure failure is integrated into a system, the designer must make a few difficult choices about what the results of a failure event will be. The first question to be resolved is whether the system can operate in a fail-soft mode. To ***fail-soft*** is to allow a system to continue to operate after a component fails. This is an alternative to having a failure cause a complete system failure. An example is a typical multitasking operating system that can support numerous simultaneous applications. If one application fails, the others can typically continue to operate.

If fail-soft isnâ€™t a viable option, then the designer needs to consider the type of product, its deployment scenarios, and the priorities related to failure response. In other words, when the product fails without a fail-soft design, it will fail completely. The designer/developer needs to decide what type of complete failure to perform and what to protect or sacrifice to achieve the planned failure result. There are numerous scenarios to consider. The initial distinction is whether the product is something that affects the physical world, such as a door locking mechanism, or is it primarily a digital asset focused product, such as a firewall.

If a product can affect the physical world, then the life and safety of humans needs to be considered and likely prioritized. This human protection prioritization is called ***fail-safe***. The idea is that when a failure occurs, the system, device, or product will revert to a state that protects the health and safety of people. For example, a fail-safe door will open easily in an emergency in order to allow people to escape a building. But this implies that the protection of assets may be sacrificed in favor of personnel safety.

##### Fail-Open vs. Fail-Closedï¼ˆæ•°å­—è¯­å¢ƒï¼‰

| æ¨¡å¼                     | ä¼˜å…ˆä¿éšœ        | é£é™©         |
| ------------------------ | --------------- | ------------ |
| **Fail-Open**            | é«˜å¯ç”¨æ€§        | æš´éœ²æ•°æ®é£é™© |
| **Fail-Closed / Secure** | é«˜æœºå¯†æ€§/å®Œæ•´æ€§ | å¯ç”¨æ€§ä¸­æ–­   |

ğŸ“Œ ä¸¾ä¾‹ï¼š

- é˜²ç«å¢™å´©æºƒåç»§ç»­æ”¾è¡Œæ‰€æœ‰æ•°æ®ï¼ˆfail-openï¼‰ï¼šç”¨æˆ·ä½“éªŒå¥½ä½†æ˜“è¢«æ”»å‡»ï¼›
- å´©æºƒåè‡ªåŠ¨æ–­å¼€è¿æ¥ï¼ˆfail-secureï¼‰ï¼šä¿æŠ¤æ•°æ®ä½†å¯èƒ½å½±å“ä¸šåŠ¡ã€‚

In the context of the physical world, the terms **failopen** is a synonym to fail-safe and **fail-closed** is a synonym of fail-secure.

**Fail Terms Definitions Related to Physical and Digital Products**

| Physical       | â† State â†’   | Digital                             |
| -------------- | ----------- | ----------------------------------- |
| Protect People | Fail-Open   | Protect Availability                |
| Protect People | Fail-Safe   | Protect Confidentiality & Integrity |
| Protect Assets | Fail-Closed | Protect Confidentiality & Integrity |
| Protect Assets | Fail-Secure | Protect Confidentiality & Integrity |

If the product is primarily digital, then the focus of security is completely on digital assets. That means the designer must then decide upon the security aspect to prioritizeâ€”namely, availability or confidentiality and integrity. If the priority is for maintaining availability, then when the product fails, the connection or communication is allowed to continue. This is known as fail-open. If the priority is for maintaining confidentiality and integrity, then when the product fails, the connection or communication is cut off. This is known as fail-secure, fail-closed, and/or fail-safe (again, in the context of a digital environment). (Note: the IETF recommends avoiding the use of the term fail-safe when discussing digital only issues as it introduces the concept of human safety which is not a concern in a digital context and thus causes unnecessary confusion).

Take note that when the context switches from the physical world to the digital world, the definition of fail-safe changes. An example could be a firewall, which if designed to failopen would allow communications without filtering, whereas if implement a fail-secure, fail-closed, or fail-safe solution would cut off communications. The fail-open state protects availability through the sacrifice of confidentiality and integrity, whereas the fail-closed state sacrifices availability in order to protect confidentiality and integrity. Another example of a digital environment event following a fail-secure, fail-closed, and/or fail-safe procedure is when an operating system encounters a processing or memory isolation violation, it terminates all executions, then initiates a reboot. This mechanism is known as a stop error, or the Blue Screen of Death (BSoD) in Windows.

##### å¤šä»»åŠ¡ç³»ç»Ÿä¸­çš„ Fail-Soft

- **Fail-Soft**ï¼šæŸç»„ä»¶æ•…éšœä¸å¯¼è‡´å…¨ç³»ç»Ÿå´©æºƒï¼›
- å¦‚ï¼šæ“ä½œç³»ç»Ÿä¸­çš„ä»»åŠ¡éš”ç¦»æœºåˆ¶ï¼Œæµè§ˆå™¨ä¸­çš„å¤šæ ‡ç­¾å´©æºƒéš”ç¦»ã€‚

#### CISSPè€ƒç‚¹æ€»ç»“

| æ¦‚å¿µ                          | è¦ç‚¹                                                     |
| ----------------------------- | -------------------------------------------------------- |
| **é»˜è®¤è®¾ç½®å®‰å…¨æ€§**            | ä¸å¯ä¿¡ï¼Œéœ€è‡ªå®šä¹‰é…ç½®ï¼Œé‡‡ç”¨â€œæœ€å°æƒé™â€ä¸â€œæœ€å°‘åŠŸèƒ½â€åŸåˆ™     |
| **Secure by Default**         | ä¼˜å…ˆå¯ç”¨æœ€å®‰å…¨é…ç½®ï¼Œé™åˆ¶åŠŸèƒ½ï¼Œéœ€è¦è¯¦ç»†è¯´æ˜ä¸è¾…åŠ©å¯¼å…¥é…ç½® |
| **Fail-Safe vs. Fail-Secure** | ç‰©ç† vs. æ•°å­—åœºæ™¯ï¼Œäººå‘˜å®‰å…¨ vs. æ•°æ®å®‰å…¨                 |
| **Fail-Open vs. Fail-Closed** | å¯ç”¨æ€§ vs. æœºå¯†æ€§/å®Œæ•´æ€§ï¼Œæ ¹æ®åº”ç”¨ä¼˜å…ˆçº§æŠ‰æ‹©             |
| **å¼‚å¸¸å¤„ç†æœºåˆ¶**              | è¾“å…¥éªŒè¯ã€try-catchã€é”™è¯¯ä¿¡æ¯æ§åˆ¶ï¼Œé˜²æ­¢æ”»å‡»è€…åˆ©ç”¨å¼‚å¸¸    |

### 5. Keep It Simple

#### **KISSï¼ˆKeep It Simple, Stupidï¼‰åŸåˆ™** 

KISSåŸåˆ™æ˜¯è½¯ä»¶å¼€å‘å’Œå®‰å…¨è®¾è®¡ä¸­çš„ç»å…¸ç†å¿µï¼Œå¼ºè°ƒï¼š**è®¾è®¡è¶Šç®€å•è¶Šå¥½ï¼Œå¤æ‚åº¦æ˜¯å®‰å…¨çš„æ•Œäººã€‚**

##### å®‰å…¨è®¾è®¡ä¸­ä¸ºä½•å¼ºè°ƒâ€œç®€å•åŒ–â€ï¼Ÿ

| å¤æ‚ç³»ç»Ÿé—®é¢˜ | å®‰å…¨é£é™©                             |
| ------------ | ------------------------------------ |
| åŠŸèƒ½è¿‡å¤š     | æ‰©å¤§æ”»å‡»é¢ï¼ˆAttack Surfaceï¼‰         |
| ä»£ç åºå¤§     | æ›´éš¾å®¡è®¡å’Œæµ‹è¯•ï¼ŒBug éš¾ä»¥å‘ç°         |
| é›†æˆç»„ä»¶å¤æ‚ | ç¬¬ä¸‰æ–¹ä¾èµ–è¶Šå¤šï¼Œä¾›åº”é“¾é£é™©è¶Šå¤§       |
| é…ç½®é¡¹ç¹å¤š   | å®‰è£…éƒ¨ç½²å‡ºé”™ç‡æ›´é«˜ï¼Œå®¹æ˜“è¢«è¯¯é…æˆ–è¯¯è§£ |

**ä¸€å¥è¯æ€»ç»“**ï¼š**è¶Šå¤æ‚ â†’ è¶Šéš¾ç®¡ç† â†’ è¶Šå®¹æ˜“å‡ºé”™ â†’ è¶Šå®¹æ˜“è¢«æ”»å‡»ã€‚**

Keep it simple is a shortened form of the classic statement of â€œkeep it simple, stupidâ€ or â€œkeep it stupid simple.â€ This is sometimes called the KISS principle. In the realm of security, this concept is the encouragement to avoid overcomplicating the environment, organization, or product design. The more complex a system, the more difficult it is to secure. The more lines of code, the more challenging it is to thoroughly test it. The more parts there are, the more places there are for things to go wrong. The more features and capabilities, the larger the attack surface.

#### ç›¸å…³å¼€å‘ç†å¿µï¼ˆKISS çš„è¡ç”Ÿæ€æƒ³ï¼‰

| åŸåˆ™                                 | å«ä¹‰                       | å¯¹å®‰å…¨çš„ç›Šå¤„                     |
| ------------------------------------ | -------------------------- | -------------------------------- |
| **DRY (Donâ€™t Repeat Yourself)**      | é¿å…é‡å¤ä»£ç ï¼Œæé«˜ä¸€è‡´æ€§   | ç»Ÿä¸€ä¿®è¡¥é€»è¾‘æ¼æ´ï¼Œä¸æ˜“é—æ¼       |
| **Computing Minimalism**             | æœ€å°‘èµ„æºä¾èµ–å’ŒåŠŸèƒ½å®ç°     | é™ä½æ”»å‡»é¢ï¼Œå‡å°‘é…ç½®é”™è¯¯         |
| **Rule of Least Power**              | ç”¨æœ€å¼±çš„è¯­è¨€/æŠ€æœ¯å®Œæˆä»»åŠ¡  | å‡å°‘ç¨‹åºæ‰§è¡Œæ„å¤–æˆ–è¢«æ»¥ç”¨çš„å¯èƒ½   |
| **Worse Is Better**                  | åŠŸèƒ½è¶Šå°‘ï¼Œè´¨é‡å¯èƒ½è¶Šé«˜     | å°‘å³æ˜¯å¤šï¼Œé¿å…åŠŸèƒ½å †ç Œå¸¦æ¥çš„é£é™© |
| **YAGNI (You Arenâ€™t Gonna Need It)** | ä¸è¦å†™â€œæœªæ¥å¯èƒ½ä¼šç”¨â€çš„åŠŸèƒ½ | é¿å…æœªä½¿ç”¨ä»£ç æš´éœ²æ–°æ¼æ´         |

There are many other concepts that have a similar or related emphasis, such as the following:

- **â€œDonâ€™t Repeat Yourselfâ€ (DRY)** The idea of eliminating redundancy in software by not repeating the same code in multiple places, which would increase the difficulty if changes are needed.
- **Computing Minimalism**  Crafting code so that it uses the least necessary hardware and software resources possible; this is also the goal of the program evaluation and review technique (PERT), which is discussed in Chapter 20.
- **Rule of Least Power**  Use the least powerful programming language that is suitable for the needed solution.
- **â€œWorse Is Betterâ€ (aka New Jersey Style)**  The quality of software does not necessarily increase with an increase in capabilities and functions; there is often a worse software state (i.e., fewer functions), which is the better (i.e., preferred, maybe more secure) option.
- **â€œYou Arenâ€™t Gonna Need Itâ€ (YAGNI)**  Programmers should not add capabilities and functions until they are actually necessary, so rather than create it when you think of it, instead create it only when you actually need it.

It is easy to get caught up in adding complexity to a system, whether that system is a software program or an organizational IT security structure. The KISS principle encourages us all to avoid the overly complex in favor of the streamlined, optimized, and reduced solution. Simpler solutions are easier to secure, easier to troubleshoot, and easier to verify.

#### CISSPè€ƒç‚¹æ€»ç»“

- CISSP å€¡å¯¼ **Security by Design** å’Œ **Defense in Depth**ï¼›

- ä½†ä»»ä½•å®‰å…¨æœºåˆ¶è®¾è®¡éƒ½è¦é¿å…â€œè¿‡åº¦è®¾è®¡â€æˆ–â€œå·¥å…·æ³›æ»¥â€ï¼›

- KISS åŸåˆ™æ˜¯è½å®â€œ**æœ€å°åŒ–æ”»å‡»é¢ï¼ˆMinimize Attack Surfaceï¼‰**â€ä¸â€œ**ç®€åŒ–æ§åˆ¶è·¯å¾„**â€çš„é‡è¦ä¿éšœã€‚

##### å®é™…åº”ç”¨ä¸¾ä¾‹ï¼š

| åœºæ™¯             | KISS åŸåˆ™çš„åº”ç”¨                                 |
| ---------------- | ----------------------------------------------- |
| **å®‰å…¨æ¶æ„è®¾è®¡** | ä¸å¼•å…¥ä¸å¿…è¦çš„ VPN/è·³æ¿å±‚ï¼Œé¿å…é“¾è·¯å†—é•¿éš¾ä»¥æ’æŸ¥ |
| **ä»£ç å®‰å…¨**     | ç®€å•é€»è¾‘å¤„ç†æ¯”å¤æ‚å¤šæ€ç»“æ„æ›´å®‰å…¨ã€æ›´æ˜“å®¡è®¡      |
| **èº«ä»½éªŒè¯ç³»ç»Ÿ** | ä½¿ç”¨é›†ä¸­è®¤è¯ï¼ˆå¦‚ SSOï¼‰ä»£æ›¿é‡å¤çš„è´¦å·ä½“ç³»        |
| **ç­–ç•¥é…ç½®**     | æ¸…æ™°å°‘é‡çš„ ACLã€IAM è§„åˆ™ä¼˜äºå±‚å±‚åµŒå¥—çš„è§„åˆ™ç»„    |

- KISS â‰  ç®€é™‹ï¼Œè€Œæ˜¯ **ç²¾ç‚¼+å®ç”¨+å¯æ§**ï¼›

- CISSP è§£é¢˜ä¸­è‹¥é‡åˆ°â€œè¿‡åº¦é›†æˆåŠŸèƒ½ã€å¤šä½™é€»è¾‘â€ï¼Œä¼˜å…ˆæ’é™¤ï¼›

- å®‰å…¨ç³»ç»Ÿåº”å½“ **è¶Šç®€å•ï¼Œè¶Šå®¹æ˜“åšåˆ°â€œéªŒè¯-æµ‹è¯•-æ§åˆ¶â€**ã€‚

### 6. Zero Trustï¼ˆé›¶ä¿¡ä»»æ¶æ„ï¼‰

ğŸ“Œ æ ¸å¿ƒç†å¿µï¼š**æ°¸ä¸ä¿¡ä»»ï¼Œå§‹ç»ˆéªŒè¯**

- **å‡è®¾å·²ç»è¢«å…¥ä¾µ**ï¼ˆAssume Breachï¼‰
- æ¯ä¸€æ¬¡è®¿é—®éƒ½å¿…é¡»è¢« **è®¤è¯ï¼ˆAuthenticationï¼‰ã€æˆæƒï¼ˆAuthorizationï¼‰ã€åŠ å¯†ï¼ˆEncryptionï¼‰**
- æ²¡æœ‰â€œå†…éƒ¨ä¿¡ä»»â€æˆ–â€œé»˜è®¤ä¿¡ä»»â€ â€”â€” æ‰€æœ‰ç”¨æˆ·ã€è®¾å¤‡ã€æœåŠ¡ **ä¸€å¾‹ä¸ä¿¡ä»»**

å®ç°è¦ç´ 

| æ ¸å¿ƒæœºåˆ¶                        | æè¿°                                 |
| ------------------------------- | ------------------------------------ |
| **èº«ä»½å’Œè®¿é—®ç®¡ç†ï¼ˆIAMï¼‰**       | æ¯æ¬¡è®¿é—®éƒ½éœ€è¦éªŒè¯èº«ä»½               |
| **å¤šå› ç´ è®¤è¯ï¼ˆMFAï¼‰**           | é˜²æ­¢è´¦æˆ·è¢«è½»æ˜“æ¥ç®¡                   |
| **æœ€å°æƒé™åŸåˆ™**                | åªæˆäºˆæ‰§è¡Œä»»åŠ¡æ‰€éœ€æœ€å°‘æƒé™           |
| **å¾®åˆ†æ®µï¼ˆMicrosegmentationï¼‰** | ç½‘ç»œåˆ‡åˆ†æˆå¤šä¸ªå°æ®µï¼Œæ§åˆ¶æ¨ªå‘ç§»åŠ¨     |
| **æŒç»­ç›‘æµ‹ä¸è¡Œä¸ºåˆ†æ**          | å®æ—¶ç›‘æ§ç”¨æˆ·æ´»åŠ¨ï¼Œæ£€æµ‹å¼‚å¸¸è¡Œä¸º       |
| **åŠ¨æ€ç­–ç•¥è¯„ä¼°**                | è€ƒè™‘ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¦‚è®¾å¤‡ã€ä½ç½®ã€æ—¶é—´ï¼‰ |

ğŸ“– æ ‡å‡†æ–‡æ¡£

- **NIST SP 800-207ï¼šZero Trust Architecture**
- å®šä¹‰ ZT çš„æ ¸å¿ƒç»„ä»¶ã€å®æ–½è·¯å¾„ã€ä¸ç°æœ‰ç³»ç»Ÿçš„é›†æˆæ–¹å¼ã€‚

Zero trust is an alternate approach to security where nothing is automatically trusted. Instead, each request for activity or access is assumed to be from an unknown and untrusted location until otherwise verified. The concept is â€œnever trust, always verify.â€ Since anyone and anything could be malicious, every transaction should be verified before it is allowed to occur. The zero trust model is based around â€œassume breach,â€ meaning that you should always assume a security breach has occurred and that whoever or whatever is making a request could be malicious. The goal is to have every access request be authenticated, authorized, and encrypted prior to the access being granted to a resource or asset. The implementation of a zero trust architecture does involve a significant shift from historical security management concepts. This shift typically requires internal microsegmentation and strong adherence to the principle of least privilege. This approach prevents lateral movement so that if there is a breach or even a malicious insider, their ability to move about the environment is severely restricted.

Zero trust is implemented using a wide range of security solutions, including internal segmentation firewalls (ISFWs), multifactor authentication (MFA), identity and access management (IAM), and next-generation endpoint security. A zero trust approach to security management can only be successful if a means to continuously validate and monitor user activities is implemented. If a one-time validation mechanism is used, then the opportunity to abuse the system remains since threats, users, and connection characteristics are always subject to change. Thus, zero trust networking can only work if real-time vetting and visibility into user activities is maintained.

#### Air Gapï¼ˆç‰©ç†éš”ç¦»ï¼‰

- **å®Œå…¨ä¸è¿æ¥ç½‘ç»œ**ï¼Œæ—¢æ— æœ‰çº¿ä¹Ÿæ— æ— çº¿è¿æ¥ï¼›
- é€‚ç”¨äºè¶…é«˜ä¿å¯†æ€§åœºæ™¯ï¼ˆå¦‚æ ¸è®¾æ–½ã€å†›å·¥æ§åˆ¶ç³»ç»Ÿï¼‰ï¼›
- å±äºæ¯” Zero Trust æ›´æç«¯çš„éš”ç¦»æ¨¡å‹ã€‚

In some situations, complete isolation may be needed instead of controlled and filtered interaction. This type of isolation is achieved using an air gap. An air gap is a network security measure employed to ensure that a secure system is physically isolated from other systems. Air gap implies that neither cabled nor wireless network links are available.

In order to implement a zero trust system, an organization must be capable of and willing to abandon some long-held assumptions about security. First and foremost, it must be understood that there is no such thing as a trusted source. No entity, asset, or subjectâ€”internal or externalâ€”is to be trusted by default. Instead, always assume attackers are already on the inside, on every system. From this new â€œno assumed trustâ€ position, it is obvious that traditional default access controls are insufficient. Each and every subject, each and every time, needs to be authenticated, authorized, and encrypted. From there, a continuous real-time monitoring system needs to be established to look for violations and suspicious events. But even with zero trust integrated into the IT architecture, it is only an element of a holistic security strategy that is integrated into the entire organizationâ€™s management processes.

Zero trust has been formalized in NIST SP 800-207, â€œZero Trust Architecture.â€

### 7. Privacy by Designï¼ˆå†…å»ºéšç§ï¼‰

ğŸ“Œ æ ¸å¿ƒç†å¿µï¼š**å°†éšç§ä¿æŠ¤èå…¥ç³»ç»Ÿçš„è®¾è®¡ä¹‹åˆï¼Œè€Œéäº‹åå¼¥è¡¥**

ç±»ä¼¼äºâ€œSecurity by Designâ€ï¼Œå¼ºè°ƒå¼€å‘è¿‡ç¨‹ä»ä¸€å¼€å§‹å°±**ä¸»åŠ¨é˜²æŠ¤éšç§**ã€‚

Privacy by Design (PbD) is a guideline to integrate privacy protections into products during the early design phase rather than attempting to tack it on at the end of development. It is effectively the same overall concept as â€œsecurity by designâ€ or â€œintegrated security,â€ where security is to be an element of design and architecture of a product starting at initiation and being maintained throughout the software development lifecycle (SDLC).

#### ä¸ƒå¤§åŸåˆ™

| åŸåˆ™                | å«ä¹‰                                   |
| ------------------- | -------------------------------------- |
| **1. é¢„é˜²ä¸ºä¸»**     | ä¸»åŠ¨é˜²å¾¡éšç§æ³„éœ²ï¼Œä¸æ˜¯ç­‰å‡ºäº‹å†å¤„ç†     |
| **2. é»˜è®¤éšç§**     | é»˜è®¤è®¾ç½®ä¸‹ç”¨æˆ·éšç§å—ä¿æŠ¤ï¼Œæ— éœ€æ‰‹åŠ¨å¼€å¯ |
| **3. å†…åµŒè®¾è®¡**     | éšç§ä¿æŠ¤æ˜¯æ¶æ„çš„ä¸€éƒ¨åˆ†ï¼Œä¸æ˜¯é™„åŠ åŠŸèƒ½   |
| **4. åŒèµ¢ç­–ç•¥**     | åŠŸèƒ½æ€§å’Œéšç§æ€§å¯ä»¥å…±å­˜ï¼Œä¸æ˜¯é›¶å’Œåšå¼ˆ   |
| **5. ç”Ÿå‘½å‘¨æœŸå®‰å…¨** | æ•°æ®ä»é‡‡é›†åˆ°é”€æ¯å…¨è¿‡ç¨‹ä¿æŠ¤éšç§         |
| **6. é€æ˜å¯è§**     | ç³»ç»Ÿå¿…é¡»å¯¹ç”¨æˆ·å…¬å¼€å¤„ç†æœºåˆ¶ã€ç”¨é€”       |
| **7. å°Šé‡ç”¨æˆ·**     | ç”¨æˆ·å¯¹å…¶ä¸ªäººä¿¡æ¯åº”æœ‰æ§åˆ¶æƒä¸é€‰æ‹©æƒ     |

As described in Ann Cavoukianâ€™s paper â€œPrivacy by Design â€“ The 7 Foundational Principles: Implementation and Mapping of Fair Information Practicesâ€, the PbD framework is based on seven foundational principles:

- Proactive not reactive; preventive not remedial
- Privacy as the default
- Privacy embedded into design
- Full functionality â€“ positive-sum, not zero-sum
- End-to-end security â€“ full lifecycle protection
- Visibility and transparency
- Respect for user privacy

The goal of PbD is to have developers integrate privacy protections into their solutions in order to avoid privacy violations in the first place. The overall concept focuses on preventions rather than remedies for violations.

#### GPS - Global Privacy Standard

- æ¨åŠ¨å…¨çƒç»Ÿä¸€éšç§ç«‹æ³•å’Œç»„ç»‡å†…éƒ¨éšç§è®¾è®¡ï¼›
- ä¸ **GDPR** çš„â€œ**Privacy by Design & Default**â€ç†å¿µä¸€è‡´ã€‚

PbD is also the driving factor behind an initiative to have privacy protections integrated throughout an organization, not just by developers. That business operations and systems design can also integrate privacy protections into their core functions. This in turn has led to the **Global Privacy Standard (GPS),** which was crafted to create a single set of universal and harmonized privacy principles. GPS is to be adopted by countries to use as a guide in developing privacy legislation, used by organizations to integrate privacy protection into their operations, and used by developers to integrate privacy into the products they produce. There is some integration of a few of the principles of PbD in the EUâ€™s GDPR (see gdpr-info.eu/ issues/privacy-by-design and Chapter 4, â€œLaws, Regulations, and Complianceâ€).

### 8. Trust but Verifyï¼ˆä¿¡ä»»ä½†éœ€éªŒè¯ï¼‰

ğŸ“Œ ç‰¹ç‚¹ï¼š**é»˜è®¤ä¿¡ä»» + è¡¥å……éªŒè¯**

- æ›¾æ˜¯è®¸å¤šç»„ç»‡çš„é»˜è®¤å®‰å…¨ç­–ç•¥ï¼›
- ä¸€æ—¦è¿›å…¥â€œå†…éƒ¨ç½‘ç»œâ€å°±è®¤ä¸ºå®‰å…¨ï¼Œé€ æˆäº† **æ¨ªå‘ç§»åŠ¨ï¼ˆLateral Movementï¼‰** çš„å·¨å¤§éšæ‚£ï¼›
- æ— æ³•æœ‰æ•ˆæŠµå¾¡ï¼š
  - **å†…éƒ¨å¨èƒï¼ˆInsider Threatsï¼‰**
  - **è¢«æ”»é™·ç»ˆç«¯ï¼ˆCompromised Hostsï¼‰**
  - **å‡­æ®æ»¥ç”¨ï¼ˆCredential Abuseï¼‰**

A more traditional security approach of trusting subjects and devices within the companyâ€™s security perimeter (i.e., internal entities) automatically can be called â€œtrust but verify.â€ This type of security approach leaves an organization vulnerable to insider attacks and grants intruders the ability to easily perform lateral movement among internal systems. Often the trust but verify approach depended on an initial authentication process to gain access to the internal â€œsecuredâ€ environment, and then relied on generic access control methods. Due to the rapid growth and changes in the modern threatscape, the trust but verify model of security is no longer sufficient. Most security experts now recommend designing organizational security around the zero trust model.

#### é€‚ç”¨æ€§é™ä½

éšç€äº‘è®¡ç®—ã€è¿œç¨‹åŠå…¬ã€ç§»åŠ¨è®¾å¤‡æ¥å…¥å¢å¤šï¼Œ**ä¿¡ä»»ä½†éªŒè¯æ¨¡å‹**å·²ä¸é€‚åº”ç°ä»£å¨èƒæ¨¡å‹ã€‚

å› æ­¤ **CISSP è€ƒè¯•åŠå®é™…å®‰å…¨æ¶æ„æ¨èè½¬å‘ Zero Trust**ï¼š

| æ¨¡å‹                 | æ ¸å¿ƒç‰¹ç‚¹                     | å®‰å…¨æ°´å¹³   |
| -------------------- | ---------------------------- | ---------- |
| **Trust but Verify** | ä»…åˆå§‹éªŒè¯ï¼Œåç»­æ“ä½œä¸å†æ£€æŸ¥ | âš ï¸ ä¸­ç­‰åä½ |
| **Zero Trust**       | æ‰€æœ‰æ“ä½œå…¨ç¨‹éªŒè¯ä¸ç›‘æ§       | âœ… é«˜       |

## Techniques for Ensuring CIAï¼ˆä¿éšœæœºå¯†æ€§ã€å®Œæ•´æ€§ã€å¯ç”¨æ€§çš„æ–¹æ³•ï¼‰

åœ¨ CISSP çš„æ ¸å¿ƒç›®æ ‡ä¸­ï¼Œ**Confidentialityï¼ˆä¿å¯†æ€§ï¼‰**ã€**Integrityï¼ˆå®Œæ•´æ€§ï¼‰** å’Œ **Availabilityï¼ˆå¯ç”¨æ€§ï¼‰** æ˜¯ä¸€åˆ‡å®‰å…¨æ§åˆ¶è®¾è®¡çš„å‡ºå‘ç‚¹ã€‚ä¸ºäº†æœ‰æ•ˆä¿æŠ¤æ•°æ®åŠå…¶å¤„ç†ç³»ç»Ÿï¼Œå¿…é¡»å€ŸåŠ©è½¯ä»¶å·¥ç¨‹å’Œæ“ä½œç³»ç»Ÿä¸­å…·å¤‡çš„â€œå®‰å…¨éš”ç¦»æœºåˆ¶â€å®ç°å¯¹ CIA çš„æŠ€æœ¯ä¿éšœã€‚

To ensure the confidentiality, integrity, and availability (CIA) of data, you must ensure that all components that have access to data are secure and well behaved. Software designers use different techniques to ensure that programs do only what is required and nothing more. Although the concepts we discuss in the following sections all relate to software programs, they are also commonly used in all areas of security. For example, physical confinement guarantees that all physical access to hardware is controlled.

### 1. Confinementï¼ˆé™åˆ¶ï¼‰

åˆç§° **æ²™ç®±ï¼ˆsandboxingï¼‰**ï¼Œæ˜¯æœ€ç›´æ¥ä½“ç°**æœ€å°æƒé™åŸåˆ™**çš„æœºåˆ¶ã€‚

âœ… æ ¸å¿ƒä½œç”¨ï¼š

- é™åˆ¶è¿›ç¨‹æˆ–ç¨‹åºåªèƒ½è®¿é—®å…¶è¢«å…è®¸çš„å†…å­˜ã€æ–‡ä»¶ã€èµ„æºï¼›
- é˜²æ­¢æœªç»æˆæƒçš„æ•°æ®æ³„éœ²æˆ–è¶Šæƒè®¿é—®ã€‚

ğŸ“Œ åº”ç”¨æ–¹å¼ï¼š

| ç±»å‹         | ç¤ºä¾‹                                               |
| ------------ | -------------------------------------------------- |
| æ“ä½œç³»ç»Ÿçº§åˆ« | Linux cgroupsã€Windows AppContainerã€macOS Sandbox |
| ç‹¬ç«‹è½¯ä»¶å·¥å…· | Sandboxie, Docker                                  |
| è™šæ‹ŸåŒ–æ‰‹æ®µ   | VMware, VirtualBox                                 |

CISSPè€ƒç‚¹ï¼š

- **Process confinement** = â€œLeast privilege for processesâ€
- æ²™ç®± â‰  è™šæ‹Ÿæœºï¼Œä½†ç›®çš„ç±»ä¼¼ï¼š**èµ„æºå’Œè¡Œä¸ºéš”ç¦»**

Software designers use process confinement to restrict the actions of a program. Simply put, process confinement allows a process to read from and write to only certain memory locations and resources. This is also known as sandboxing. It is the application of the principle of least privilege to processes. The goal of confinement is to prevent data leakage to unauthorized programs, users, or systems.

The operating system, or some other security component, disallows illegal read/write requests. If a process attempts to initiate an action beyond its granted authority, that action will be denied. In addition, further actions, such as logging the violation attempt, may be taken. Generally, the offending process is terminated. Confinement can be implemented in the operating system itself (such as through process isolation and memory protection), through the use of a confinement application or service (for example, Sandboxie at sandboxie.com), or through a virtualization or hypervisor solution (such as VMware or Oracleâ€™s VirtualBox).

### 2. Boundsï¼ˆè¾¹ç•Œï¼‰

å®šä¹‰è¿›ç¨‹å¯è®¿é—®èµ„æºèŒƒå›´çš„**è¾¹ç•Œæ¡ä»¶**

æ ¸å¿ƒä½œç”¨ï¼š

- å°†ä¸åŒæƒé™çº§åˆ«çš„è¿›ç¨‹åˆ†é…åˆ°å—é™èµ„æºç©ºé—´ï¼›
- é˜²æ­¢æƒé™æå‡ï¼ˆå¦‚ç”¨æˆ·æ¨¡å¼è®¿é—®å†…æ ¸ç©ºé—´ï¼‰ï¼›

ğŸ“Œ è¾¹ç•Œç±»å‹ï¼š

| ç±»å‹         | è¯´æ˜                                           |
| ------------ | ---------------------------------------------- |
| **é€»è¾‘è¾¹ç•Œ** | æ“ä½œç³»ç»Ÿæ§åˆ¶è®¿é—®æƒé™ï¼ˆå¦‚å†…å­˜é¡µä¿æŠ¤ï¼‰           |
| **ç‰©ç†è¾¹ç•Œ** | æ›´å¼ºéš”ç¦»ï¼Œå¦‚ä¸“ç”¨ç¡¬ä»¶åŒºæ®µã€ä¸“ç”¨å†…å­˜èŠ¯ç‰‡ï¼ˆæ˜‚è´µï¼‰ |

è”åŠ¨æœºåˆ¶ï¼š

- **Bounds æ˜¯å®ç° confinement çš„â€œè§„åˆ™â€å±‚é¢**ï¼›
- **Bounds æ˜¯å®ç° isolation çš„â€œåŸºç¡€æ¡ä»¶â€**

Each process that runs on a system is assigned an authority level. The authority level tells the operating system what the process can do. In simple systems, there may be only two authority levels: user and kernel. The authority level tells the operating system how to set the bounds for a process. The bounds of a process consist of limits set on the memory addresses and resources it can access. The bounds state the area within which a process is confined or contained. In most systems, these bounds segment logical areas of memory for each process to use. It is the responsibility of the operating system to enforce these logical bounds and to disallow access to other processes. More secure systems may require physically bounded processes. Physical bounds require each bounded process to run in an area of memory that is physically separated from other bounded processes, not just logically bounded in the same memory space. Physically bounded memory can be very expensive, but itâ€™s also more secure than logical bounds. Bounds can be a means to enforce confinement.

### 3. Isolationï¼ˆéš”ç¦»ï¼‰

ä»æŠ€æœ¯ä¸Š**é˜»æ­¢ä¸€ä¸ªè¿›ç¨‹å½±å“å¦ä¸€ä¸ªè¿›ç¨‹çš„è¡Œä¸ºæˆ–èµ„æºè®¿é—®**

æ ¸å¿ƒä½œç”¨ï¼š

- é™åˆ¶è¿›ç¨‹ç›¸äº’è®¿é—®ï¼›
- å½“è¿›ç¨‹å´©æºƒæ—¶ï¼Œä¸å½±å“å…¶ä»–è¿›ç¨‹æˆ–æ“ä½œç³»ç»Ÿï¼›
- é˜»æ­¢å†…å­˜ç©ºé—´ã€ç³»ç»Ÿè°ƒç”¨ã€æ–‡ä»¶æè¿°ç¬¦çš„éæ³•å…±äº«ã€‚

ğŸ“Œ å…¸å‹å®ç°æ–¹å¼ï¼š

- ç°ä»£æ“ä½œç³»ç»Ÿè¿›ç¨‹éš”ç¦»æœºåˆ¶ï¼ˆå†…æ ¸ vs ç”¨æˆ·æ€ï¼‰
- è™šæ‹ŸåŒ–/å®¹å™¨æŠ€æœ¯ï¼ˆå¦‚ Dockerã€KVMã€Hyper-Vï¼‰
- æµè§ˆå™¨ tab è¿›ç¨‹æ²™ç®±åŒ–

ğŸ” ä¸‰è€…å…³ç³»å›¾è§£ï¼š

````
[Confinement]
    |
    V
[ Bounds ] ---------> å®šä¹‰æƒé™è¾¹ç•Œ
    |
    V
[ Isolation ] -------> ä¿è¯ç‹¬ç«‹è¿è¡Œã€æ•…éšœä¸å½±å“å…¶ä»–æ¨¡å—
````

When a process is confined through enforcing access bounds, that process runs in isolation. Process isolation ensures that any behavior will affect only the memory and resources associated with the isolated process. Isolation is used to protect the operating environment, the kernel of the operating system, and other independent applications. Isolation is an essential component of a stable operating system. Isolation is what prevents an application from accessing the memory or resources of another application, whether for good or ill. Isolation allows for a fail-soft environment so that separate processes can operate normally or fail/crash without interfering or affecting other processes. Isolation is achieved through the enforcement of containment using bounds. 

These three concepts (confinement, bounds, and isolation) make designing secure programs and operating systems more difficult, but they also make it possible to implement more secure systems. Confinement is making sure that an active process can only access specific resources (such as memory). Bounds is the limitation of authorization assigned to a process to limit the resources the process can interact with and the types of interactions allowed. Isolation is the means by which confinement is implemented through the use of bounds. The goals of the concepts is the ensure that the predetermined scope of resource access is not violated and any failure or compromise of a process has minimal to no affect on any other process.

### 4. Access Controlsï¼ˆè®¿é—®æ§åˆ¶ï¼‰

å¯¹ Subject ä¸ Object ä¹‹é—´çš„**è®¿é—®è¡Œä¸ºè¿›è¡Œæ§åˆ¶**

âœ… ä½œç”¨ç›®æ ‡ï¼š

- ä»…å…è®¸è¢«æˆæƒä¸»ä½“è®¿é—®ç›®æ ‡èµ„æºï¼›
- åŒºåˆ†ä¸åŒçš„è®¿é—®æ“ä½œï¼ˆè¯»ã€å†™ã€æ‰§è¡Œã€åˆ é™¤ç­‰ï¼‰ï¼›

ğŸ“Œ ä¸»è¦æ¨¡å‹

| æ¨¡å‹                | ç‰¹ç‚¹                       |
| ------------------- | -------------------------- |
| DACï¼ˆè‡ªä¸»è®¿é—®æ§åˆ¶ï¼‰ | æ‰€æœ‰è€…æ§åˆ¶ï¼Œçµæ´»ä½†é£é™©é«˜   |
| MACï¼ˆå¼ºåˆ¶è®¿é—®æ§åˆ¶ï¼‰ | å®‰å…¨çº§åˆ«åˆ†å±‚ï¼Œå†›äº‹å¸¸ç”¨     |
| RBACï¼ˆåŸºäºè§’è‰²ï¼‰    | ä¼ä¸šåº”ç”¨å¹¿æ³›               |
| ABACï¼ˆåŸºäºå±æ€§ï¼‰    | é«˜çµæ´»æ€§ã€ç°ä»£äº‘ç¯å¢ƒæ”¯æŒå¥½ |

To ensure the security of a system, you need to allow subjects to access only authorized objects. Access controls limit the access of a subject to an object. Access rules state which objects are valid for each subject. Further, an object might be valid for one type of access and be invalid for another type of access. There are a wide range of options for access controls, such as discretionary, role-based, and mandatory. Please see Chapter 14 for an in-depth discussion of access controls.

### 5. Trust and Assuranceï¼ˆä¿¡ä»»ä¸ä¿éšœï¼‰

| é¡¹ç›®          | å®šä¹‰                               | å…³é”®å«ä¹‰                                 |
| ------------- | ---------------------------------- | ---------------------------------------- |
| **Trust**     | å®‰å…¨æœºåˆ¶å­˜åœ¨ï¼ˆSecurity Mechanismï¼‰ | ç³»ç»Ÿå…·å¤‡ä¸€å®šçš„ä¿æŠ¤èƒ½åŠ›                   |
| **Assurance** | å®‰å…¨æœºåˆ¶çš„å¯é æ€§ï¼ˆConfidenceï¼‰     | è¿™äº›ä¿æŠ¤æœºåˆ¶æ˜¯å¦**æœ‰æ•ˆã€æŒç»­ã€å®‰å…¨è¿è¡Œ** |

ä¿éšœæœºåˆ¶ï¼š

- å®‰å…¨åŠŸèƒ½æµ‹è¯•ï¼ˆå¦‚ Common Criteriaã€EALï¼‰
- å˜æ›´ç®¡ç†ã€è¡¥ä¸ç®¡ç†ã€é…ç½®åŸºçº¿ï¼ˆæŒç»­ç»´æŠ¤ assuranceï¼‰
- å¯ä¿¡å¯åŠ¨ï¼ˆTrusted Bootï¼‰æˆ– TPM æ”¯æŒ

ğŸ§  è®°å¿†æç¤ºï¼š

- **Trust æ˜¯â€œä½ æœ‰ä»€ä¹ˆâ€**
- **Assurance æ˜¯â€œä½ æ€ä¹ˆçŸ¥é“å®ƒæœ‰æ•ˆâ€**

A trusted system is one in which all protection mechanisms work together to process sensitive data for many types of users while maintaining a stable and secure computing environment. In other words, trust is the presence of a security mechanism, function, or capability. Assurance is the degree of confidence in satisfaction of security needs. In other words, assurance is how reliable the security mechanisms are at providing security. Assurance must be continually maintained, updated, and reverified. This is true if the secured system experiences a known change (good or badâ€”i.e., a vendor patch or a malicious exploit) or if a significant amount of time has passed. In either case, change has occurred at some level. Change is often the antithesis of security; it often diminishes security. This is why change management, patch management, and configuration management are so important to security management.

Assurance varies from one system to another and often must be established on individual systems. However, there are grades or levels of assurance that can be placed across numerous systems of the same type, systems that support the same services, or systems that are deployed in the same geographic location. Thus, trust can be built into a system by implementing specific security features, whereas assurance is an assessment of the reliability and usability of those security features in a real-world situation.

#### æ€»ç»“è¡¨æ ¼

| æŠ€æœ¯å                | ä½œç”¨è¯´æ˜                             | å…³é”®æœ¯è¯­/æœºåˆ¶                 |
| --------------------- | ------------------------------------ | ----------------------------- |
| **Confinement**       | é™åˆ¶è¿›ç¨‹è®¿é—®èŒƒå›´                     | æ²™ç®±ã€è™šæ‹ŸåŒ–ã€æœ€å°æƒé™        |
| **Bounds**            | å®šä¹‰è¿›ç¨‹è¡Œä¸ºå’Œè®¿é—®è¾¹ç•Œ               | æƒé™çº§åˆ«ã€é€»è¾‘/ç‰©ç†å†…å­˜éš”ç¦»   |
| **Isolation**         | ç¡®ä¿è¿›ç¨‹äº’ä¸å¹²æ‰°ã€ç‹¬ç«‹è¿è¡Œ           | è¿›ç¨‹éš”ç¦»ã€å®¹å™¨ã€è™šæ‹ŸåŒ–        |
| **Access Control**    | æ§åˆ¶è®¿é—®å¯¹è±¡çš„æƒé™å’Œæ–¹å¼             | DACã€MACã€RBACã€ABAC          |
| **Trust & Assurance** | ç³»ç»Ÿå®‰å…¨æœºåˆ¶å­˜åœ¨ä¸å¯é æ€§çš„è¯„ä¼°ä¸éªŒè¯ | TPMã€Common Criteriaã€EALç­‰çº§ |

## Understand the Fundamental Concepts of Security Models

**å®‰å…¨æ¨¡å‹ï¼ˆSecurity Modelï¼‰** æ˜¯ä¿¡æ¯å®‰å…¨é¢†åŸŸä¸­ç”¨äº**å½¢å¼åŒ–å®‰å…¨ç­–ç•¥**çš„ä¸€ç§æ–¹æ³•ã€‚å…¶ä½œç”¨æ˜¯ï¼š

- å°†æŠ½è±¡çš„å®‰å…¨ç­–ç•¥ï¼ˆå¦‚â€œè°å¯ä»¥è®¿é—®ä»€ä¹ˆâ€ï¼‰**æ˜ å°„æˆå…·ä½“çš„ã€å¯æ“ä½œçš„æœºåˆ¶**ï¼›
- ä¸ºç³»ç»Ÿè®¾è®¡è€…æä¾›å¯ä»¥ç”¨æ¥è¡¡é‡å®ç°æ˜¯å¦ç¬¦åˆå®‰å…¨ç›®æ ‡çš„**æŠ€æœ¯è“å›¾**ï¼›
- é€šå¸¸åŒ…æ‹¬ **ä¸€ç»„è§„åˆ™ + ä¸€ç§ç³»ç»Ÿç»“æ„æˆ–æœºåˆ¶**ï¼Œå¯¹â€œè®¿é—®æ§åˆ¶â€ã€â€œä¿¡æ¯æµâ€ã€â€œå®Œæ•´æ€§â€ã€â€œæƒé™æˆæƒâ€ç­‰é—®é¢˜åšå‡ºå®šä¹‰ã€‚

**å®‰å…¨æ¨¡å‹ â‰  å®‰å…¨ç­–ç•¥**

- ç­–ç•¥å®šä¹‰â€œè¦åšä»€ä¹ˆâ€
- æ¨¡å‹å®šä¹‰â€œæ€ä¹ˆåšã€è°æ¥åšã€åšçš„æœºåˆ¶æ˜¯ä»€ä¹ˆâ€

In information security, models provide a way to formalize security policies. Such models can be abstract or intuitive, but all are intended to provide an explicit set of rules that a computer can follow to implement the fundamental security concepts, processes, and procedures of a security policy. A security model provides a way for designers to map abstract statements into a security policy that prescribes the algorithms and data structures necessary to build hardware and software. Thus, a security model gives software designers something against which to measure their design and implementation.

#### Tokens, Capabilities, and Labels

Several different methods are used to describe the necessary security attributes for an object. A security token is a separate object that is associated with a resource and describes its security attributes. This token can communicate security information about an object prior to requesting access to the actual object. In other implementations, various lists are used to store security information about multiple objects. A capabilities list maintains a row of security attributes for each controlled object. Although not as flexible as the token approach, a capabilities list generally offers quicker lookups when a subject requests access to an object. A third common type of attribute storage is called a security label, which is generally a permanent part of the object to which itâ€™s attached. Once a security label is set, it usually cannot be altered. This permanence provides another safeguard against tampering that neither tokens nor capabilities lists provide.

##### 1. Security Token

- **å®šä¹‰**ï¼šä¸€ä¸ª**ç‹¬ç«‹çš„å®‰å…¨å±æ€§å¯¹è±¡**ï¼Œä¸å—æ§èµ„æºå…³è”ï¼›
- **ç”¨æ³•**ï¼šåœ¨è®¿é—®èµ„æºå‰ï¼Œ**å…ˆä¼ é€’ token** ç»™è®¿é—®æ§åˆ¶æœºåˆ¶ï¼Œä»¥ä¾¿éªŒè¯ï¼›
- **ç‰¹ç‚¹**ï¼š
  - å¯æ‹†åˆ†ã€æ˜“äºä¼ é€’ï¼›
  - é€‚åˆ**é¢å‘å¯¹è±¡æˆ–åˆ†å¸ƒå¼ç³»ç»Ÿ**ï¼›
  - å®‰å…¨æ€§éœ€é¢å¤–ä¿éšœï¼ˆtoken æœ¬èº«å¯èƒ½è¢«ä¼ªé€ æˆ–ç¯¡æ”¹ï¼‰ï¼›

ğŸ“Œ **ä¾‹å­**ï¼šOAuth 2.0 è®¿é—®ä»¤ç‰Œã€Kerberos Ticket

##### 2. Capability List

- **å®šä¹‰**ï¼šç³»ç»Ÿä¸ºæ¯ä¸ª**å¯¹è±¡**ç»´æŠ¤ä¸€å¼ èƒ½åŠ›è¡¨ï¼Œè®°å½•å®ƒå¯è¢«å“ªäº›ç”¨æˆ·ï¼ˆæˆ–è¿›ç¨‹ï¼‰è®¿é—®ã€å…·å¤‡ä»€ä¹ˆæƒé™ï¼›
- **æœ¬è´¨**ï¼šä¸€ç§åˆ—å‡ºâ€œè°å¯ä»¥åšä»€ä¹ˆâ€çš„è®¿é—®æ§åˆ¶åˆ—è¡¨ï¼›
- **ç‰¹ç‚¹**ï¼š
  - æŸ¥æ‰¾å¿«ï¼›
  - **ä¸å¤ªçµæ´»**ï¼ˆéœ€è¦é€ä¸ªå¯¹è±¡é…ç½®ï¼‰ï¼›
  - **ä¸æ˜“ç®¡ç†å¤§è§„æ¨¡æƒé™ç»„åˆ**ï¼›

ğŸ“Œ ç±»ä¼¼äº Linux ä¸­çš„ file permission bitsï¼ˆå¦‚ rwxr-xr--ï¼‰

##### 3. Security Label

- **å®šä¹‰**ï¼šæ ‡ç­¾æ˜¯ä¸€ä¸ª**åµŒå…¥å¯¹è±¡æœ¬èº«**çš„å±æ€§ï¼Œæ ‡è®°äº†å…¶å®‰å…¨å±æ€§ï¼ˆå¦‚æœºå¯†çº§åˆ«ï¼‰ï¼›
- **ç‰¹ç‚¹**ï¼š
  - ä¸å¯ç¯¡æ”¹ï¼ˆæˆ–åªèƒ½ç”±ç‰¹æƒç®¡ç†å‘˜ä¿®æ”¹ï¼‰ï¼›
  - å¸¸ç”¨äº**å¼ºåˆ¶è®¿é—®æ§åˆ¶æ¨¡å‹**ï¼ˆMACï¼‰ï¼›
  - å®ç°äº†å¯¹**ä¿¡æ¯æµçš„æ§åˆ¶**ï¼ˆå¦‚é«˜å¯†çº§å¯¹è±¡ä¸èƒ½å‘ä½å¯†çº§å†™ï¼‰ï¼›

ğŸ“Œ **ä¾‹å­**ï¼š

- **Bell-LaPadula æ¨¡å‹** ä½¿ç”¨å®‰å…¨æ ‡ç­¾å®ç°æœºå¯†æ€§ç­‰çº§ï¼ˆTop Secret, Secret, Confidentialï¼‰
- **SELinux** ç”¨æ ‡ç­¾æ§åˆ¶ç³»ç»Ÿè¿›ç¨‹å¯¹èµ„æºçš„è®¿é—®

| æœºåˆ¶ç±»å‹          | å®šä½         | å­˜å‚¨ä½ç½®       | çµæ´»æ€§     | å®‰å…¨æ€§       | åº”ç”¨æ¨¡å‹ç¤ºä¾‹       |
| ----------------- | ------------ | -------------- | ---------- | ------------ | ------------------ |
| Security Token    | å¤–éƒ¨å¯¹è±¡     | åˆ†ç¦»äºèµ„æºæœ¬ä½“ | é«˜         | ä¸­ï¼ˆéœ€åŠ å¯†ï¼‰ | Kerberos, OAuth    |
| Capabilities List | ç³»ç»Ÿçº§æƒé™è¡¨ | ç³»ç»Ÿè¡¨         | ä¸­         | ä¸­           | ACLs, DAC ç³»ç»Ÿ     |
| Security Label    | åµŒå…¥å¯¹è±¡     | å¯¹è±¡è‡ªèº«       | ä½ï¼ˆå›ºå®šï¼‰ | é«˜ï¼ˆé˜²ç¯¡æ”¹ï¼‰ | Bell-LaPadula, MAC |

â€œ**Token æºå¸¦é€šè¡Œè¯ï¼ŒLabel è´´åœ¨å¯¹è±¡ä¸Šï¼ŒCapability è®°åœ¨æˆ‘æ‰‹ä¸Šã€‚**â€

- **Token** â†’ åƒç¥¨è¯ä¸€æ ·éšèº«å¸¦ã€å¯è½¬ç§»
- **Label** â†’ åƒæ ‡ç­¾è´´åœ¨ç‰©ä½“ä¸Šï¼Œä¸æ˜“æ›´æ”¹
- **Capability** â†’ åƒæ¸…å•åœ¨ä½ æ‰‹ä¸­ï¼Œæ¸…æ¥šå†™ç€â€œæˆ‘èƒ½å¹²å•¥â€

CISSP è€ƒè¯•ä¸­çš„è¦ç‚¹ç†è§£

- Security Models æ˜¯å°†ç­–ç•¥ï¼ˆå¦‚ MACã€DACã€RBACï¼‰**æŠ€æœ¯è½åœ°çš„æ¡†æ¶**
- **æ ‡ç­¾æœºåˆ¶ï¼ˆLabelsï¼‰** æ˜¯ MAC çš„å…¸å‹å®ç°å½¢å¼ï¼›
- **Token å’Œ Capability List** é€‚åˆ DAC/RBAC æƒ…å¢ƒï¼›
- è€ƒé¢˜å¸¸é€šè¿‡åœºæ™¯æè¿°ï¼Œè¦æ±‚è¯†åˆ«ä½¿ç”¨çš„æ˜¯å“ªç§æœºåˆ¶ï¼›
- æŒæ¡ä¸‰ç§æœºåˆ¶çš„ç‰¹ç‚¹ã€é€‚ç”¨åœºæ™¯ã€ä¼˜åŠ£å¯¹æ¯”æ˜¯ç­”é¢˜å…³é”®ï¼›

CISSPè€ƒç‚¹

è‹¥é¢˜ç›®æåˆ°â€œ**ä¸€ä¸ªä¸»ä½“å¸¦æœ‰å¯¹è±¡æƒé™çš„åˆ—è¡¨**â€ â†’ Capability List

è‹¥é¢˜ç›®å¼ºè°ƒâ€œ**æ¯ä¸ªå¯¹è±¡æ ‡è®°äº†ä¸€ä¸ªä¸å¯æ›´æ”¹çš„å®‰å…¨çº§åˆ«**â€ â†’ Security Labelï¼ˆMACï¼‰

è‹¥é¢˜ç›®è¯´â€œ**æ¯ä¸ªå¯¹è±¡æºå¸¦ä¸€ä¸ªå¯åˆ†å‘çš„è®¿é—®å‡­è¯æˆ–ä¸´æ—¶æˆæƒ**â€ â†’ Tokenï¼ˆå¸¸è§äºä¸´æ—¶ä¼šè¯æˆ– SSOï¼‰

### 1. Trusted Computing Base (TCB)

**Trusted Computing Base (TCB)** æ˜¯æŒ‡ä¸€ä¸ªè®¡ç®—ç³»ç»Ÿä¸­è¢«ä¿¡ä»»ç”¨æ¥**å¼ºåˆ¶æ‰§è¡Œå®‰å…¨ç­–ç•¥**çš„å…¨éƒ¨è½¯ç¡¬ä»¶ç»„åˆã€‚å®ƒæ˜¯ï¼š

- æ“ä½œç³»ç»Ÿ + å®‰å…¨æœºåˆ¶ä¸­æœ€**æ ¸å¿ƒã€æœ€å¯ä¿¡ã€æœ€å°åŒ–**çš„å­ç³»ç»Ÿï¼›
- å…¶**å®Œæ•´æ€§ä¸æ­£ç¡®æ€§**æ˜¯æ•´ä¸ªç³»ç»Ÿå®‰å…¨çš„åŸºç¡€ï¼›
- å®‰å…¨ç›®æ ‡ï¼š**æœºå¯†æ€§ã€å®Œæ•´æ€§ã€è®¿é—®æ§åˆ¶ç­–ç•¥çš„æ‰§è¡Œ**

ğŸ“Œ ç†æƒ³è®¾è®¡ä¸­ï¼Œ**TCB è¶Šå°è¶Šå¥½**ï¼Œè¿™æ ·æ›´æ˜“äºéªŒè¯å…¶å®‰å…¨æ€§ä¸åŠŸèƒ½æ­£ç¡®æ€§ï¼ˆå®‰å…¨æœ€å°åŒ–åŸåˆ™ï¼‰ã€‚

The trusted computing base (TCB) design principle is the combination of hardware, software, and controls that work together to form a trusted base to enforce your security policy. The TCB is a subset of a complete information system. It should be as small as possible so that a detailed analysis can reasonably ensure that the system meets design specifications and requirements. The TCB is the only portion of that system that can be trusted to adhere to and enforce the security policy. It is the responsibility of TCB components to ensure that a system behaves properly in all cases and that it adheres to the security policy under all circumstances.

#### Security Perimeterï¼ˆå®‰å…¨åŠå¾„ï¼‰

**Security Perimeter** æ˜¯ä¸€ä¸ª**é€»è¾‘è¾¹ç•Œ**ï¼Œç”¨äºéš”ç¦» TCB ä¸ç³»ç»Ÿçš„å…¶ä»–éƒ¨åˆ†ã€‚å…¶ä½œç”¨æ˜¯ï¼š

- é˜²æ­¢éæˆæƒä»£ç ç›´æ¥è®¿é—® TCBï¼›
- å¼ºåˆ¶æ‰€æœ‰è®¿é—® TCB çš„æ“ä½œéƒ½é€šè¿‡å—æ§é€šé“è¿›è¡Œã€‚

Trusted Pathï¼ˆå—ä¿¡é€šé“ï¼‰

- æ˜¯ TCB ä¸ç”¨æˆ·æˆ–å¤–éƒ¨ç³»ç»Ÿäº¤äº’çš„**å®‰å…¨é€šé“**ï¼›
- é˜²æ­¢ä¸­é—´äººæ”»å‡»ã€æ¶æ„å¹²æ‰°ï¼›
- å¸¸è§å®ç°åŒ…æ‹¬ï¼š**Ctrl+Alt+Del ç™»å½•ç•Œé¢**ï¼ˆWindowsï¼‰ç¡®ä¿ç”¨æˆ·è¾“å…¥ä¸ä¼šè¢«æ¶æ„ç¨‹åºæ•æ‰ã€‚

Trusted Shellï¼ˆå—ä¿¡å‘½ä»¤è¡Œï¼‰

- å…è®¸ç”¨æˆ·å®‰å…¨åœ°ä½¿ç”¨å‘½ä»¤è¡Œæ“ä½œï¼›
- é˜²æ­¢ç”¨æˆ·â€œé€ƒé€¸â€å‡º shell å½±å“ TCBï¼Œä¹Ÿé˜»æ­¢å…¶ä»–æ¶æ„ç¨‹åºåŠ«æŒ shellï¼›
- å¤šç”¨äº **é«˜å®‰å…¨çº§åˆ«çš„ CLI ç¯å¢ƒ**ï¼ˆå¦‚ç‰¹æƒ shellã€Secure Boot æ§åˆ¶å°ç­‰ï¼‰

The security perimeter of your system is an imaginary boundary that separates the TCB from the rest of the system. This boundary ensures that no insecure communications or interactions occur between the TCB and the remaining elements of the computer system. For the TCB to communicate with the rest of the system, it must create secure channels, also called trusted paths. A trusted path is a channel established with strict standards to allow necessary communication to occur without exposing the TCB to security exploitations.

A security perimeter may also allow for the use of a trusted shell. 

A trusted shell allows a subject to perform command-line operations without risk to the TCB or the subject. 

A trusted shell prevents the subject from being able to break out of isolation to affect the TCB and in turn prevents other processes from breaking into the shell to affect the subject.

#### Reference Monitors and Security Kernels

##### Reference Monitorï¼ˆå‚è€ƒç›‘æ§å™¨ï¼‰

è¿™æ˜¯ TCB ä¸­çš„ä¸€ä¸ª **æ ¸å¿ƒæ¦‚å¿µ**ï¼Œå…¶ä¸»è¦èŒè´£ä¸ºï¼š

| åŠŸèƒ½         | æè¿°                                                         |
| ------------ | ------------------------------------------------------------ |
| è®¿é—®æ§åˆ¶åˆ¤å®š | éªŒè¯æ¯ä¸€ä¸ªå¯¹èµ„æºï¼ˆobjectï¼‰çš„è®¿é—®è¯·æ±‚æ˜¯å¦ç¬¦åˆè®¿é—®ç­–ç•¥ï¼ˆpolicyï¼‰ |
| ç‹¬ç«‹æ€§       | ç‹¬ç«‹äºåº”ç”¨å’Œç”¨æˆ·ï¼Œæ— æ³•è¢«ç¯¡æ”¹æˆ–ç»•è¿‡                           |
| å®Œæ•´æ€§       | æ¯æ¬¡è®¿é—®éƒ½å¿…é¡»ç»è¿‡å®ƒï¼ˆä¸å¯è·³è¿‡ï¼‰                             |
| å®¡è®¡èƒ½åŠ›     | æ‰€æœ‰è®¿é—®å°è¯•éƒ½å¯è¢«è®°å½•ï¼ˆåŒ…æ‹¬æˆåŠŸä¸å¤±è´¥ï¼‰                     |

å‚è€ƒç›‘æ§å™¨æ˜¯**è®¿é—®æ§åˆ¶æ¨¡å‹çš„å®è·µè€…**ï¼Œå¦‚ MACã€DACã€RBAC éƒ½å¯é€šè¿‡å…¶å®æ–½ã€‚

##### Security Kernelï¼ˆå®‰å…¨å†…æ ¸ï¼‰

- æ˜¯å‚è€ƒç›‘æ§å™¨çš„**æŠ€æœ¯å®ç°ä½“**ï¼›
- åŒ…å«äº†å®ç°è®¿é—®æ§åˆ¶æ£€æŸ¥æ‰€éœ€çš„æ‰€æœ‰ç¡¬ä»¶ã€å›ºä»¶ã€è½¯ä»¶ç»„ä»¶ï¼›
- æä¾›**æœ€å°å¯ä¿¡æ‰§è¡Œç¯å¢ƒ**ï¼Œç±»ä¼¼äº hypervisor çš„â€œring 0â€çº§åˆ«ã€‚

ğŸ’¡å¯ä»¥å°† Reference Monitor çœ‹ä½œâ€œè®¿é—®æ§åˆ¶çš„ç†è®ºæ¨¡å‹â€ï¼ŒSecurity Kernel æ˜¯å®ƒçš„â€œå®é™…æ‰§è¡Œå¼•æ“â€ã€‚

The part of the TCB that validates access to every resource prior to granting access requests is called the reference monitor. The reference monitor stands between every subject and object, verifying that a requesting subjectâ€™s credentials meet the objectâ€™s access requirements before any requests are allowed to proceed. Effectively, the reference monitor is the access control enforcer for the TCB. The reference monitor enforces access control or authorization based on the desired security model, whether discretionary, mandatory, role-based, or some other form of access control. The collection of components in the TCB that work together to implement reference monitor functions is called the security kernel. The reference monitor is a concept or theory that is put into practice via the implementation of a security kernel in software and hardware. The purpose of the security kernel is to launch appropriate components to enforce reference monitor functionality and resist all known attacks. The security kernel mediates all resource access requests, granting only those requests that match the appropriate access rules in use for a system.

##### CISSPè€ƒç‚¹æ€»ç»“

| æ¦‚å¿µ               | è¯´æ˜                                                        |
| ------------------ | ----------------------------------------------------------- |
| TCB                | ç³»ç»Ÿä¸­è´Ÿè´£å¼ºåˆ¶æ‰§è¡Œå®‰å…¨ç­–ç•¥çš„æœ€å°å¯ä¿¡ç»„ä»¶é›†åˆ                |
| Security Perimeter | åˆ†éš” TCB ä¸ç³»ç»Ÿå…¶ä»–éƒ¨åˆ†çš„è¾¹ç•Œï¼Œæ‰€æœ‰è®¿é—®éœ€é€šè¿‡å—æ§é€šé“       |
| Trusted Path       | ä¿éšœç”¨æˆ·ä¸ TCB é—´äº¤äº’ä¸è¢«ç¯¡æ”¹ï¼Œå¦‚ Ctrl+Alt+Del ç™»å½•ç•Œé¢     |
| Reference Monitor  | æ ¸å¿ƒè®¿é—®æ§åˆ¶æœºåˆ¶ï¼Œæ‰€æœ‰ subject-object è¯·æ±‚å¿…é¡»é€šè¿‡å®ƒåˆ¤æ–­    |
| Security Kernel    | Reference Monitor çš„å®é™…ä»£ç +ç¡¬ä»¶å®ç°ï¼Œç³»ç»Ÿæœ€æ•æ„Ÿçš„éƒ¨åˆ†ä¹‹ä¸€ |

### 2. State Machine Modelï¼ˆçŠ¶æ€æœºæ¨¡å‹ï¼‰

**çŠ¶æ€æœºæ¨¡å‹** æ˜¯ä¸€ç§ç†è®ºæ¨¡å‹ï¼Œç”¨äºæè¿°å¦‚ä½•é€šè¿‡**ç³»ç»ŸçŠ¶æ€çš„å˜åŒ–**æ¥ç¡®ä¿ç³»ç»Ÿå§‹ç»ˆå¤„äºå®‰å…¨çŠ¶æ€ã€‚å®ƒæ˜¯è®¸å¤šå®‰å…¨æ¨¡å‹ï¼ˆå¦‚ Bell-LaPadulaã€Bibaï¼‰çš„**ç†è®ºåŸºç¡€**ã€‚

è¯¥æ¨¡å‹çš„æ ¸å¿ƒæ€æƒ³æºäºè®¡ç®—æœºç§‘å­¦ä¸­çš„**æœ‰é™çŠ¶æ€æœºï¼ˆFinite State Machineï¼ŒFSMï¼‰**ï¼ŒFSM æ˜¯ä¸€ä¸ªåŸºäºå½“å‰è¾“å…¥ä¸å½“å‰çŠ¶æ€ï¼Œå†³å®šä¸‹ä¸€çŠ¶æ€ä¸è¾“å‡ºçš„æ¨¡å‹ã€‚

| æ¦‚å¿µ                         | è¯´æ˜                                                         |
| ---------------------------- | ------------------------------------------------------------ |
| **çŠ¶æ€ï¼ˆStateï¼‰**            | ç³»ç»Ÿåœ¨æŸä¸€ç‰¹å®šæ—¶åˆ»çš„å¿«ç…§ï¼ŒåŒ…æ‹¬ç”¨æˆ·ä¼šè¯ã€è®¿é—®æƒé™ã€ç³»ç»Ÿèµ„æºç­‰ |
| **è¾“å…¥ï¼ˆInputï¼‰**            | ç”¨æˆ·è¡Œä¸ºã€ç³»ç»Ÿäº‹ä»¶ã€æŒ‡ä»¤æˆ–å¤–éƒ¨äº¤äº’                           |
| **çŠ¶æ€è½¬ç§»ï¼ˆTransitionï¼‰**   | æ¥æ”¶è¾“å…¥æˆ–ç”Ÿæˆè¾“å‡ºæ—¶ï¼Œç³»ç»Ÿä»ä¸€ä¸ªçŠ¶æ€è½¬å˜åˆ°å¦ä¸€ä¸ªçŠ¶æ€         |
| **å®‰å…¨çŠ¶æ€ï¼ˆSecure Stateï¼‰** | å½“å‰ç³»ç»ŸçŠ¶æ€æ»¡è¶³å®‰å…¨ç­–ç•¥çš„æ‰€æœ‰è¦æ±‚                           |
| **çŠ¶æ€å‡½æ•°**                 | æ•°å­¦è¡¨è¾¾ä¸ºï¼š`next_state = F(current_state, input)`           |
| **è¾“å‡ºå‡½æ•°**                 | è¡¨è¾¾ä¸ºï¼š`output = F(current_state, input)`                   |

#### State Machine åœ¨å®‰å…¨ä¸­çš„åº”ç”¨

##### ğŸ¯ æ ¸å¿ƒå®‰å…¨æ€æƒ³ï¼š

å¦‚æœç³»ç»Ÿä» **ä¸€ä¸ªå®‰å…¨çŠ¶æ€** é€šè¿‡ **åˆæ³•çš„è¾“å…¥ä¸å¤„ç†é€»è¾‘** åªèƒ½è½¬ç§»åˆ° **å¦ä¸€ä¸ªå®‰å…¨çŠ¶æ€**ï¼Œé‚£ä¹ˆæ•´ä¸ªç³»ç»Ÿå°†å§‹ç»ˆç»´æŒåœ¨ä¸€ä¸ª**å®‰å…¨çŠ¶æ€æœº**ä¸­è¿è¡Œã€‚âœ… è¿™æ˜¯ä¸€ç§å¯¹ç³»ç»Ÿè¡Œä¸ºè¿›è¡Œä¸¥æ ¼æ§åˆ¶å’ŒéªŒè¯çš„æ–¹æ³•ï¼Œç¡®ä¿**ä»»æ„æ—¶é—´ç‚¹**ä¸‹éƒ½ä¸åç¦»å®‰å…¨ç­–ç•¥ã€‚

------

##### ğŸ”„ ç¤ºä¾‹è¯´æ˜

ç¤ºä¾‹ï¼šè®¿é—®æ§åˆ¶çŠ¶æ€æœº

å‡è®¾æœ‰ä¸€ä¸ªç³»ç»Ÿèµ„æº `File_A`ï¼Œå…¶è®¿é—®ç­–ç•¥å¦‚ä¸‹ï¼š

- ä»…å½“ç”¨æˆ·è§’è‰²æ˜¯â€œManagerâ€æ—¶æ‰èƒ½è¯»å–ï¼›
- æ¯å½“ç”¨æˆ·è¯·æ±‚è¯»å–æ—¶ï¼Œç³»ç»Ÿä¼šæ£€æŸ¥å½“å‰çŠ¶æ€ï¼ˆç”¨æˆ·èº«ä»½ï¼‰ä¸è¾“å…¥ï¼ˆè¯»å–è¯·æ±‚ï¼‰ï¼›
- åªæœ‰å½“ `current_state = Manager` ä¸” `input = Read_Request` æ—¶ï¼Œç³»ç»Ÿæ‰å…è®¸çŠ¶æ€è½¬ç§»ä¸º `Access_Granted`ã€‚

å…¶ä»–ä»»ä½•ç»„åˆéƒ½ä¼šå¯¼è‡´è½¬ç§»ä¸º `Access_Denied`ã€‚

> ğŸ“Œ æ¯ä¸ªå¯èƒ½çš„â€œçŠ¶æ€è½¬ç§»è·¯å¾„â€éƒ½å¿…é¡»**æ˜¾å¼éªŒè¯æ˜¯å¦ä¼šç ´åå®‰å…¨æ€§**ã€‚è¿™å°±è¦æ±‚å¼€å‘è€…åœ¨è®¾è®¡æ—¶**ç©·ä¸¾å¹¶éªŒè¯æ‰€æœ‰å¯èƒ½è·¯å¾„**ã€‚

**ğŸ§© ä¼˜ç‚¹ä¸å®é™…æ„ä¹‰**

| ä¼˜ç‚¹                             | è¯´æ˜                                                         |
| -------------------------------- | ------------------------------------------------------------ |
| âœ… å¼ºå®‰å…¨æ€§éªŒè¯åŸºç¡€               | æä¾›äº†å½¢å¼åŒ–éªŒè¯ç³»ç»Ÿå®‰å…¨çŠ¶æ€çš„ç†è®ºæ¨¡å‹                       |
| âœ… å¯ä½œä¸ºå…¶ä»–æ¨¡å‹çš„åŸºç¡€ï¼ˆå¦‚ BLPï¼‰ | Bell-LaPadulaã€Bibaã€Clark-Wilson ç­‰éƒ½åœ¨å…¶åŸºç¡€ä¸Šå»ºç«‹å®‰å…¨çº¦æŸ |
| âœ… æ˜“äºå»ºæ¨¡åŠ¨æ€ç³»ç»Ÿ               | æ¨¡æ‹Ÿç³»ç»Ÿä¸­çš„ç”¨æˆ·è¡Œä¸ºã€æŒ‡ä»¤å¤„ç†ã€èµ„æºè®¿é—®ç­‰çŠ¶æ€å˜æ¢           |

#### ğŸ§ª å®‰å…¨çŠ¶æ€æœºæ¨¡å‹çš„ä¸‰å¤§è¦æ±‚

1. **ç³»ç»Ÿå¯åŠ¨äºå®‰å…¨çŠ¶æ€**ï¼›
2. **æ¯æ¬¡çŠ¶æ€è½¬ç§»éƒ½éªŒè¯æ˜¯å¦å®‰å…¨**ï¼›
3. **ä»…å…è®¸åˆè§„çš„è®¿é—®è¯·æ±‚å¼•èµ·çŠ¶æ€å˜åŒ–**ï¼›

å¦‚æœè¿™ä¸‰é¡¹éƒ½æ»¡è¶³ï¼Œå°±å¯ä»¥è¯´è¯¥ç³»ç»Ÿæ˜¯ä¸€ä¸ªâ€œ**å®‰å…¨çŠ¶æ€æœºæ¨¡å‹**â€ã€‚

------

#### ğŸ§  ä¸ TCB å’Œ Reference Monitor çš„å…³ç³»

- **Reference Monitor** ä¾æ®å®‰å…¨ç­–ç•¥å†³å®šæ˜¯å¦å…è®¸æŸä¸ªçŠ¶æ€è½¬ç§»ï¼›
- **Security Kernel** å®ç°è¿™ä¸€åˆ¤æ–­æœºåˆ¶ï¼›
- **State Machine Model** åˆ™æ˜¯ç†è®ºåŸºç¡€ï¼Œç¡®ä¿æ¯æ¬¡æ“ä½œï¼ˆå³çŠ¶æ€è½¬ç§»ï¼‰éƒ½æ˜¯åˆæ³•çš„ã€‚

The state machine model describes a system that is always secure no matter what state it is in. Itâ€™s based on the computer science definition of a **finite state machine (FSM)**. An FSM combines an external input with an internal machine state to model all kinds of complex systems, including parsers, decoders, and interpreters. Given an input and a state, an FSM transitions to another state and may create an output. Mathematically, the next state is a function of the current state and the input next stateâ€”that is, the next state = F(input, current state). Likewise, the output is also a function of the input and the current state outputâ€”that is, the output = F(input, current state).

According to the state machine model, a state is a snapshot of a system at a specific moment in time. If all aspects of a state meet the requirements of the security policy, that state is considered secure. A transition occurs when accepting input or producing output.

A transition always results in a new state (also called a state transition). All state transitions must be evaluated. If each possible state transition results in another secure state, the system can be called a secure state machine. A secure state machine model system always boots into a secure state, maintains a secure state across all transitions, and allows subjects to access resources only in a secure manner compliant with the security policy. The secure state machine model is the basis for many other security models.

### 3. Information Flow Modelï¼ˆä¿¡æ¯æµæ¨¡å‹ï¼‰

ğŸ“Œ æ ¸å¿ƒç›®æ ‡ï¼šæ§åˆ¶ä¿¡æ¯åœ¨ç³»ç»Ÿå†…**æµåŠ¨çš„åˆæ³•æ€§**

#### å®šä¹‰ä¸æœ¬è´¨

ä¿¡æ¯æµæ¨¡å‹åŸºäº **çŠ¶æ€æœºæ¨¡å‹** æ„å»ºï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š**åªå…è®¸æˆæƒçš„ä¿¡æ¯æµé€šï¼Œä¸å…è®¸æœªæˆæƒçš„ä¿¡æ¯è·¨è¶Šè¾¹ç•Œã€‚**

å®ƒä¸ä»…å…³æ³¨ **ä¿¡æ¯çš„â€œæµå‘â€**ï¼ˆå¦‚ä»é«˜å¯†çº§åˆ°ä½å¯†çº§æ˜¯å¦åˆç†ï¼‰ï¼Œè¿˜å…³æ³¨ **ä¿¡æ¯çš„â€œæµç±»å‹â€**ï¼ˆæ˜¯å¦é€šè¿‡å—æ§æ–¹å¼ä¼ è¾“ã€æ˜¯å¦åŒ…å«æ•æ„Ÿå­—æ®µç­‰ï¼‰ã€‚

#### ä¸»è¦ä½œç”¨

1. **é˜²æ­¢ä¿¡æ¯æ³„éœ²**
   ç¡®ä¿é«˜å®‰å…¨çº§åˆ«çš„ä¿¡æ¯ä¸èƒ½é€šè¿‡ç›´æ¥æˆ–é—´æ¥è·¯å¾„ï¼ˆå¦‚ covert channelsï¼Œéšè”½é€šé“ï¼‰æ³„éœ²åˆ°ä½çº§åˆ«ã€‚
2. **è·¨çº§åˆ«å®‰å…¨æ§åˆ¶**
   å®æ–½åœ¨å…·æœ‰å¤šçº§å®‰å…¨ç­–ç•¥ï¼ˆMultilevel Securityï¼ŒMLSï¼‰çš„ç³»ç»Ÿä¸­ï¼Œæ§åˆ¶ä¸åŒçº§åˆ« Subject ä¸ Object ä¹‹é—´çš„ä¿¡æ¯æµå‘ã€‚
3. **æ—¶é—´ç»´åº¦ä¸Šçš„å¯¹è±¡è½¬æ¢è¿½è¸ª**
   å¯ç”¨äºæ¯”è¾ƒä¸€ä¸ªå¯¹è±¡åœ¨ä¸åŒæ—¶é—´ç‚¹çš„ä¸¤ä¸ªçŠ¶æ€ä¹‹é—´çš„ä¿¡æ¯å˜åŒ–ï¼Œåˆ¤æ–­æ˜¯å¦å­˜åœ¨æœªç»æˆæƒçš„ä¿®æ”¹æˆ–æ³„æ¼ã€‚

#### ä¿¡æ¯æµè·¯å¾„æ§åˆ¶

åœ¨è¯¥æ¨¡å‹ä¸­ï¼Œç³»ç»Ÿä¸­ä»»æ„ä¸¤ä¸ªç»„ä»¶ä¹‹é—´çš„é€šä¿¡éƒ½å¿…é¡»æ˜ç¡®å£°æ˜å¹¶è¢«æˆæƒï¼š

- âœ” å…è®¸çš„ä¿¡æ¯è·¯å¾„ï¼ˆAuthorized Flowï¼‰
- âŒ ç¦æ­¢çš„è·¯å¾„ï¼šå¦‚ covert channelï¼ˆéšè”½è·¯å¾„ï¼‰ã€side-channelï¼ˆä¾§ä¿¡é“ï¼‰

ä¾‹å¦‚ï¼šé«˜å¯†çº§ç”¨æˆ·å‘ä½å¯†çº§ç”¨æˆ·å¤åˆ¶æ–‡ä»¶â€”â€”è¿™æ˜¯æœªæˆæƒçš„**å‘ä¸‹ä¿¡æ¯æµåŠ¨**ï¼ˆwrite downï¼‰ï¼Œæ¨¡å‹å°†æ‹’ç»æ­¤æ“ä½œã€‚

The information flow model focuses on controlling the flow of information. Information flow models are based on the state machine model. Information flow models donâ€™t necessarily deal with only the direction of information flow; they can also address the type of flow.

Information flow models are designed to prevent unauthorized, insecure, or restricted information flow, often between different levels of security (known as multilevel models). Information flow can be between subjects and objects at the same or different classification levels. An information flow model allows all authorized information flows, and prevents all unauthorized information flows.

Another interesting perspective on the information flow model is that it is used to establish a relationship between two versions or states of the same object when those two versions or states exist at different points in time. Thus, information flow dictates the transformation of an object from one state at one point in time to another state at another point in time. The information flow model also addresses covert channels by specifically excluding all undefined flow pathways.

### 4. Noninterference Modelï¼ˆéå¹²æ‰°æ¨¡å‹ï¼‰

æ ¸å¿ƒç›®æ ‡ï¼š**ä½çº§åˆ«ç”¨æˆ·ä¸åº”æ„ŸçŸ¥åˆ°é«˜çº§åˆ«ç”¨æˆ·çš„è¡Œä¸º**

#### å®šä¹‰ä¸æœ¬è´¨

éå¹²æ‰°æ¨¡å‹æ˜¯å»ºç«‹åœ¨ä¿¡æ¯æµæ¨¡å‹åŸºç¡€ä¹‹ä¸Šçš„æ›´ä¸¥æ ¼æ¨¡å‹ï¼Œå…¶ç†å¿µæ˜¯ï¼š

> â€œä¸€ä¸ªé«˜çº§åˆ«ç”¨æˆ·çš„è¡Œä¸º**ä¸èƒ½å¯¹**ä½çº§åˆ«ç”¨æˆ·äº§ç”Ÿ**ä»»ä½•å¯æ„ŸçŸ¥çš„å½±å“**ã€‚â€

è¿™æ˜¯ä¿¡æ¯æµæ¨¡å‹çš„ä¸€ä¸ª**ä¿å¯†æ€§æ‰©å±•**ï¼Œå¼ºè°ƒ**ä¸å¯è§æ€§**ä¸**ç³»ç»ŸçŠ¶æ€çš„ç‹¬ç«‹æ€§**ã€‚

#### é—®é¢˜ç¤ºä¾‹

å‡è®¾ Subject A æ˜¯ä¸€ä¸ªæ‹¥æœ‰å¯†çº§ä¿¡æ¯çš„é«˜çº§åˆ«ç”¨æˆ·ï¼ŒSubject B æ˜¯ä¸€ä¸ªè®¿å®¢ç”¨æˆ·ã€‚

- è‹¥ A æ‰§è¡ŒæŸæ“ä½œå¯¼è‡´ç³»ç»Ÿååº”æ—¶é—´å˜æ…¢ï¼ŒB å¯èƒ½æ„ŸçŸ¥åˆ°æ­¤è¡Œä¸ºï¼Œä»è€Œ**æ¨æ–­å‡ºæŸäº›ä¿¡æ¯**ï¼›
- å³ä½¿æ²¡æœ‰ç›´æ¥ä¿¡æ¯ä¼ é€’ï¼Œåªè¦å­˜åœ¨â€œå¹²æ‰°â€è¡Œä¸ºï¼Œä¹Ÿå¯èƒ½æ³„éœ²æ•æ„Ÿä¿¡æ¯ã€‚

å› æ­¤ï¼Œéå¹²æ‰°æ¨¡å‹è¯•å›¾æ„å»ºè¿™æ ·ä¸€ç§ç¯å¢ƒï¼š

> **é«˜å¯†çº§çš„ç”¨æˆ·æ´»åŠ¨æ˜¯â€œé€æ˜â€çš„ï¼Œä½å¯†çº§ç”¨æˆ·æ— æ³•æ„ŸçŸ¥ç³»ç»Ÿä¸­å…¶ä»–äººçš„è¡Œä¸ºã€‚**

#### åº”ç”¨åœºæ™¯

- é˜²æ­¢ covert channelã€timing channel ç­‰éšè”½é€šé“æ”»å‡»ï¼›
- é€‚ç”¨äºå¯¹ä¿¡æ¯éš”ç¦»è¦æ±‚æé«˜çš„å†›äº‹æˆ–æ”¿åºœç³»ç»Ÿï¼›
- é˜²èŒƒæ¶æ„è½¯ä»¶ï¼ˆå¦‚æœ¨é©¬ç¨‹åºï¼‰é€šè¿‡é—´æ¥æ–¹å¼æ³„éœ²æ•æ„Ÿæ•°æ®ã€‚

The noninterference model is loosely based on the information flow model. However, instead of being concerned about the flow of information, the noninterference model is concerned with how the actions of a subject at a higher security level affect the system state or the actions of a subject at a lower security level. Basically, the actions of subject A (high) should not affect or interfere with the actions of subject B (low) or even be noticed by subject B. If such violations occur, subject B may be placed into an insecure state or be able to deduce or infer information about a higher level of classification. This is a type of information leakage and implicitly creates a covert channel. Thus, the noninterference model can be imposed to provide a form of protection against damage caused by malicious programs, such as Trojan horses, backdoors, and rootkits.

#### Composition Theoriesï¼ˆç»„åˆç†è®ºï¼‰

ç»„åˆç†è®ºè¿›ä¸€æ­¥åˆ†æå¤šä¸ªç³»ç»Ÿä¹‹é—´å¦‚ä½•ç»„åˆæ—¶ç¡®ä¿ä¿¡æ¯æµçš„å®‰å…¨æ€§ã€‚

| æ¨¡å‹                  | æè¿°                                                     |
| --------------------- | -------------------------------------------------------- |
| **Cascadingï¼ˆçº§è”ï¼‰** | ç³»ç»Ÿ A çš„è¾“å‡ºä½œä¸ºç³»ç»Ÿ B çš„è¾“å…¥ï¼Œç”¨äºä¿¡æ¯é€å±‚ä¼ é€’         |
| **Feedbackï¼ˆåé¦ˆï¼‰**  | ç³»ç»Ÿ A å’Œ B äº’ä¸ºè¾“å…¥è¾“å‡ºï¼Œå½¢æˆ**å¾ªç¯è·¯å¾„**ï¼Œä¿¡æ¯åŒå‘æµåŠ¨ |
| **Hookupï¼ˆè¿æ¥ï¼‰**    | ç³»ç»Ÿ A æ—¢å°†ä¿¡æ¯ä¼ ç»™ç³»ç»Ÿ Bï¼Œåˆå°†ä¿¡æ¯å‘å¾€å¤–éƒ¨å®ä½“          |

è¿™äº›ç†è®ºå¸®åŠ©åˆ†æ**ç³»ç»Ÿç»„åˆ**åä¿¡æ¯æµæ¨¡å‹æ˜¯å¦ä¾ç„¶æˆç«‹ï¼Œä»è€Œé¿å…ç³»ç»Ÿé—´è”åˆå¯¼è‡´çš„æ–°ä¿¡æ¯æ³„éœ²è·¯å¾„ã€‚

Some other models that fall into the information flow category build on the notion of inputs and outputs between multiple systems. These are called composition theories because they explain how outputs from one system relate to inputs to another system. There are three composition theories:

- Cascading: Input for one system comes from the output of another system.
- Feedback: One system provides input to another system, which reciprocates by reversing those roles (so that system A first provides input for system B and then system B provides input to system A).
- Hookup: One system sends input to another system but also sends input to external entities.

| æ¨¡å‹                       | å…³æ³¨é‡ç‚¹                         | ç›®æ ‡                                       |
| -------------------------- | -------------------------------- | ------------------------------------------ |
| **Information Flow Model** | ä¿¡æ¯åœ¨ç³»ç»Ÿå†…çš„â€œåˆæ³•è·¯å¾„â€ä¸â€œé€”å¾„â€ | é™åˆ¶æœªç»æˆæƒçš„ä¿¡æ¯æ³„éœ²ï¼Œè§„èŒƒæµå‘å’Œæµé‡     |
| **Noninterference Model**  | é«˜çº§è¡Œä¸ºæ˜¯å¦è¢«ä½çº§æ„ŸçŸ¥           | ç¡®ä¿æ•æ„Ÿæ´»åŠ¨å¯¹å¤–ä¸å¯è§ï¼Œé¿å…æ¨æµ‹ä¸æ³„éœ²     |
| **Composition Theories**   | å¤šç³»ç»Ÿè”åˆåçš„ä¿¡æ¯æµè¡Œä¸º         | åˆ¤æ–­ç³»ç»Ÿç»„åˆåæ˜¯å¦å‡ºç°éæ³•è·¯å¾„æˆ–æ–°æ³„éœ²é€šé“ |

### 5. Take-Grant Model

#### æ ¸å¿ƒç†å¿µ

ç”¨å›¾è®ºæè¿°æƒé™çš„â€œè·å–ä¸ä¼ æ’­â€ã€‚Take-Grant æ¨¡å‹ä½¿ç”¨**æœ‰å‘å›¾**ï¼ˆDirected Graphï¼‰æ¥è¡¨ç¤ºç³»ç»Ÿä¸­ **æƒé™å¦‚ä½•åœ¨ä¸»ä½“å’Œå¯¹è±¡ä¹‹é—´ä¼ é€’**ã€‚å›¾ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹ä»£è¡¨ä¸»ä½“ï¼ˆsubjectï¼‰æˆ–å¯¹è±¡ï¼ˆobjectï¼‰ï¼Œè¾¹ï¼ˆedgeï¼‰ä¸Šæ ‡æ³¨çš„æ˜¯å½“å‰çš„è®¿é—®æƒé™æˆ–æ“ä½œèƒ½åŠ›ã€‚

#### å››å¤§è§„åˆ™ï¼ˆåŸºæœ¬æ“ä½œï¼‰

| è§„åˆ™                  | æè¿°                                                         |
| --------------------- | ------------------------------------------------------------ |
| **1. Takeï¼ˆå–å¾—ï¼‰**   | å¦‚æœä¸»ä½“ `X` æœ‰â€œtakeâ€æƒé™ï¼Œåˆ™ `X` å¯ä»¥ä»å¦ä¸€ä¸ªä¸»ä½“ `Y` å¤„â€œå–å¾—â€æŸä¸ªæƒé™ã€‚ |
| **2. Grantï¼ˆæˆäºˆï¼‰**  | å¦‚æœä¸»ä½“ `X` æœ‰â€œgrantâ€æƒé™ï¼Œåˆ™ `X` å¯ä»¥å°†è‡ªå·±æ‹¥æœ‰çš„æŸä¸ªæƒé™æˆäºˆç»™å¦ä¸€ä¸ªä¸»ä½“ `Y`ã€‚ |
| **3. Createï¼ˆåˆ›å»ºï¼‰** | å…è®¸ä¸»ä½“åˆ›å»ºæ–°å¯¹è±¡å¹¶è‡ªåŠ¨æˆä¸ºå…¶æ‹¥æœ‰è€…ï¼Œä¹Ÿå¯æ–°å»ºæƒé™è¾¹ã€‚       |
| **4. Removeï¼ˆç§»é™¤ï¼‰** | å…è®¸ä¸»ä½“åˆ é™¤å…¶å¯¹æŸå¯¹è±¡çš„æƒé™ã€‚                               |

å››ä¸ªæ“ä½œå®šä¹‰äº†**æƒé™ä¼ æ’­**çš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸï¼Œä»ç”Ÿæˆ (Create)ã€èµ‹äºˆ (Grant)ã€æ‰©æ•£ (Take)ï¼Œåˆ°æ’¤é”€ (Remove)ã€‚

The take-grant model employs a directed graph to dictate how rights can be passed from one subject to another or from a subject to an object. Simply put, a subject (X) with the grant right can grant another subject (Y) or another object (Z) any right that subject (X) possesses. Likewise, a subject (X) with the take right can take a right from another subject (Y). In addition to these two primary rules, the take-grant model has a create rule and a remove rule to generate or delete rights. The key to this model is that using these rules allows you to figure out when rights in the system can change and where leakage (that is, unintentional distribution of permissions) can occur.

In essence, here are the four rules of the take-grant model:

1. Take rule: Allows a subject to take rights over an object
2. Grant rule: Allows a subject to grant rights to an object
3. Create rule: Allows a subject to create new rights
4. Remove rule: Allows a subject to remove rights it has

It is interesting to ponder that the take and grant rules are effectively a copy function. This can be recognized in modern OSes in the process of inheritance, such as subjects inheriting a permission from a group or a file inheriting ACL values from a parent folder. The two additional rules (create and remove), which are not defined by a directed graph, are also commonly present in modern operating systems. For example, to obtain permission on an object, that permission does not have to be copied from a user account that already has that permission; instead, it is simply created by an account with privilege capability of create or assign permissions (which can be the owner of an object or a subject with full control or administrative privileges over the object).

#### å®‰å…¨åˆ†æä¸æƒé™æ³„æ¼

è¯¥æ¨¡å‹å¯ä»¥ç”¨æ¥åˆ†æï¼š

- **æƒé™æ˜¯å¦ä¼šâ€œæ³„æ¼â€**ï¼šå³æ˜¯å¦å­˜åœ¨ä¸»ä½“åœ¨æ²¡æœ‰æ˜ç¡®æˆæƒçš„å‰æä¸‹ï¼Œé€šè¿‡é“¾å¼ä¼ æ’­é—´æ¥è·å¾—æ•æ„Ÿæƒé™ã€‚
- **æœ€å°æƒé™ä¼ æ’­è·¯å¾„**ï¼šå¯ä»¥é€šè¿‡å›¾éå†ç®—æ³•æ‰¾åˆ°æŸä¸ªæƒé™å¯èƒ½æ‰©æ•£çš„æœ€çŸ­è·¯å¾„ã€‚

ç°ä»£æ“ä½œç³»ç»Ÿä¸­ï¼Œâ€œç»§æ‰¿æƒé™â€ï¼ˆå¦‚ Windows æ–‡ä»¶å¤¹æƒé™ç»§æ‰¿ï¼‰å°±å¯ä»¥çœ‹ä½œ take-grant çš„ä¸€ç§è¡¨ç°å½¢å¼ã€‚

#### åº”ç”¨åœºæ™¯

- æ–‡ä»¶ç³»ç»Ÿæƒé™åˆ†æ
- æ“ä½œç³»ç»Ÿä¸­æƒé™ç»§æ‰¿æ¨ç†
- è®¿é—®è·¯å¾„å¯è¾¾æ€§éªŒè¯ï¼ˆæ˜¯å¦å­˜åœ¨éæ³•æƒé™ä¼ æ’­è·¯å¾„ï¼‰

### 6. Access Control Matrixï¼ˆè®¿é—®æ§åˆ¶çŸ©é˜µï¼‰

**æ ¸å¿ƒç†å¿µï¼šäºŒç»´çŸ©é˜µå½¢å¼å±•ç°â€œè°å¯ä»¥å¯¹ä»€ä¹ˆèµ„æºåšä»€ä¹ˆäº‹â€**

è¯¥æ¨¡å‹é€šè¿‡ä¸€ä¸ªçŸ©é˜µè¡¨ç¤º**ä¸»ä½“ï¼ˆsubjectsï¼‰å¯¹å®¢ä½“ï¼ˆobjectsï¼‰æ‹¥æœ‰çš„æ“ä½œæƒé™**ï¼š

|        | Object A | Object B | Object C |
| ------ | -------- | -------- | -------- |
| User X | Read     | Write    | â€”        |
| User Y | â€”        | Read     | Execute  |

å…¶ä¸­ï¼š

- è¡Œï¼ˆRowï¼‰å¯¹åº”äº **ä¸»ä½“ï¼ˆUser/Processï¼‰**
- åˆ—ï¼ˆColumnï¼‰å¯¹åº”äº **å¯¹è±¡ï¼ˆFile/Printerï¼‰**
- å•å…ƒæ ¼çš„å†…å®¹å³ä¸ºè¯¥ç”¨æˆ·å¯¹è¯¥èµ„æºçš„**è®¿é—®æƒé™é›†åˆ**

An access control matrix is a table of subjects and objects that indicates the actions or functions that each subject can perform on each object. Each column of the matrix is an access control list (ACL) pulled from objects. Once sorted, each row of the matrix is a capabilities list for each listed subject. An ACL is tied to an object; it lists the valid actions each subject can perform. A capability list is tied to the subject; it lists valid actions that can be taken on each object included in the matrix.

From an administration perspective, using only capability lists for access control is a management nightmare. A capability list method of access control can be accomplished by storing on each subject a list of rights the subject has for every object. This effectively gives each user a key ring of accesses and rights to objects within the security domain. To remove access to a particular object, every user (subject) that has access to it must be individually manipulated. Thus, managing access on each user account is much more difficult than managing access on each object (in other words, via ACLs). A capabilities table can be created by pivoting an access control matrix; this results in the columns being subjects and the rows being ACLs from objects.

#### è¡ç”Ÿæœ¯è¯­

| æ¦‚å¿µ                            | å®šä¹‰                                                   | æœ¬è´¨ç»“æ„   |
| ------------------------------- | ------------------------------------------------------ | ---------- |
| **ACLï¼ˆè®¿é—®æ§åˆ¶åˆ—è¡¨ï¼‰**         | é’ˆå¯¹æŸä¸ªå¯¹è±¡ (Object)ï¼Œè®°å½•æ‰€æœ‰å…è®¸è®¿é—®å®ƒçš„ä¸»ä½“åŠæ“ä½œ  | **åˆ—è§†è§’** |
| **Capabilities Listï¼ˆèƒ½åŠ›è¡¨ï¼‰** | é’ˆå¯¹æŸä¸ªä¸»ä½“ (Subject)ï¼Œåˆ—å‡ºå…¶å¯¹æ‰€æœ‰å¯¹è±¡æ‹¥æœ‰çš„æ“ä½œæƒé™ | **è¡Œè§†è§’** |

#### ACL vs. Capabilities å¯¹æ¯”

| ç‰¹å¾       | ACLï¼ˆåŸºäºå¯¹è±¡ï¼‰       | Capabilitiesï¼ˆåŸºäºä¸»ä½“ï¼‰ |
| ---------- | --------------------- | ------------------------ |
| ç®¡ç†ä¾¿æ·æ€§ | âœ… åªæ”¹ä¸€å¤„ï¼ˆå¯¹è±¡ï¼‰    | âŒ å¤šç”¨æˆ·è¦åŒæ­¥ä¿®æ”¹æƒé™   |
| çµæ´»æ€§     | âœ… é€‚åˆèµ„æºå…±äº«æ§åˆ¶    | âŒ ä¸é€‚åˆé¢‘ç¹å˜åŒ–çš„ç¯å¢ƒ   |
| ç°å®åº”ç”¨   | Windows/Unix æ–‡ä»¶ç³»ç»Ÿ | é€šå¸¸ä¸ç›´æ¥æš´éœ²           |
| å®‰å…¨å®¡è®¡   | âœ… é€‚åˆè¿›è¡Œèµ„æºçº§å®¡è®¡  | âŒ éš¾ä»¥é›†ä¸­ç®¡ç†èµ„æº       |

#### ç°å®æ„ä¹‰

è®¿é—®æ§åˆ¶çŸ©é˜µè¢«å¹¿æ³›åº”ç”¨äºï¼š

- **æ–‡ä»¶ç³»ç»Ÿæƒé™ç®¡ç†ï¼ˆWindows ACLã€Linux chmodï¼‰**
- **æ•°æ®åº“è®¿é—®æ§åˆ¶ï¼ˆå¦‚ GRANT è¯­å¥ï¼‰**
- **ç”¨æˆ·è§’è‰²æƒé™è®¾è®¡ï¼ˆRBAC/ABAC çš„åº•å±‚åŸå‹ï¼‰**
- **å®‰å…¨ç­–ç•¥è‡ªåŠ¨å®¡è®¡å·¥å…·ä¸­ç”¨äºæ£€æµ‹æƒé™è¿‡åº¦åˆ†é…**

#### æ€»ç»“å¯¹æ¯”

| æ¨¡å‹                      | ç‰¹ç‚¹                           | ä¼˜åŠ¿                       | ç¼ºç‚¹                            |
| ------------------------- | ------------------------------ | -------------------------- | ------------------------------- |
| **Take-Grant æ¨¡å‹**       | å›¾ç»“æ„è¡¨è¾¾æƒé™è½¬ç§»ï¼ˆåŠ¨æ€æ¼”åŒ–ï¼‰ | æ¨¡æ‹Ÿæƒé™ç»§æ‰¿/ä¼ æ’­è·¯å¾„      | åˆ†æè¾ƒå¤æ‚ï¼Œä¸ç›´è§‚              |
| **Access Control Matrix** | çŸ©é˜µè¡¨è¾¾é™æ€è®¿é—®æƒé™           | æ¸…æ™°ç›´è§‚ï¼Œé€‚åˆé™æ€æƒé™ç®¡ç† | å¯æ‰©å±•æ€§å·®ï¼Œä¸»ä½“/å¯¹è±¡å¤šæ—¶ä¸å®ç”¨ |

### 7. Bell-LaPadula Modelï¼ˆä¿å¯†æ€§å¯¼å‘ï¼‰- BLPæ¨¡å‹ - ä»¥æœºå¯†æ€§ä¸ºæ ¸å¿ƒ

Bellâ€“LaPadula æ¨¡å‹ç”±ç¾å›½å›½é˜²éƒ¨ï¼ˆDoDï¼‰äº 1970 å¹´ä»£å¼€å‘ï¼Œä¸“ä¸ºå¤šçº§å®‰å…¨ç­–ç•¥ï¼ˆMultilevel Security, MLSï¼‰è®¾è®¡ï¼Œ**é‡ç‚¹æ˜¯é˜²æ­¢æ•æ„Ÿä¿¡æ¯æ³„æ¼**ã€‚

å®ƒé€‚ç”¨äºå†›äº‹æˆ–æ”¿åºœçº§ç³»ç»Ÿï¼Œç‰¹åˆ«å…³æ³¨ä¿å¯†æ€§ï¼ˆConfidentialityï¼‰ï¼Œé€šè¿‡æ§åˆ¶ä¿¡æ¯ä»â€œé«˜ç­‰çº§â€å‘â€œä½ç­‰çº§â€æµåŠ¨æ¥é˜²æ­¢ä¿¡æ¯æ³„éœ²ã€‚

#### ä¸‰å¤§æ ¸å¿ƒè§„åˆ™

| å±æ€§                                | è¯´æ˜                                              | åˆ«å                       |
| ----------------------------------- | ------------------------------------------------- | -------------------------- |
| **Simple Security Property**        | **â€œNo Read Upâ€**ï¼šä¸èƒ½è¯»å–æ¯”è‡ªå·±æ›´é«˜çº§åˆ«çš„å¯¹è±¡    | é˜²æ­¢ä½æƒé™äººå‘˜è®¿é—®æ•æ„Ÿä¿¡æ¯ |
| **Star (\*) Property**              | **â€œNo Write Downâ€**ï¼šä¸èƒ½å†™å…¥æ¯”è‡ªå·±ä½çº§åˆ«çš„å¯¹è±¡   | é˜²æ­¢ä¿¡æ¯å‘ä½å¯†çº§æ³„éœ²       |
| **Discretionary Security Property** | ä½¿ç”¨è®¿é—®æ§åˆ¶çŸ©é˜µæ¥è¿›è¡ŒåŸºäºæ‰€æœ‰è€…çš„è®¿é—®æ§åˆ¶ï¼ˆDACï¼‰ | é™å®šåœ¨åŒçº§åˆ«ä¸­çš„æ“ä½œ       |

âœ…é‡ç‚¹è®°å¿†æ–¹å¼ï¼š

- **Simple = Readï¼ŒStar = Write**
- Bellâ€“LaPadula = **No Read Up, No Write Down**

The U.S. Department of Defense (DoD) developed the Bellâ€“LaPadula model in the 1970s based on the DoDâ€™s multilevel security policies. The multilevel security policy states that a subject with any level of clearance can access resources at or below its clearance level. However, within clearance levels, access to compartmentalized objects is granted only on a need-to-know basis.

By design, the Bellâ€“LaPadula model prevents the leaking or transfer of classified information to less secure clearance levels. This is accomplished by blocking lower-classified subjects from accessing higher-classified objects. With these restrictions, the Bellâ€“LaPadula model is focused on maintaining confidentiality and does not address any other aspects of object security.

#### Lattice-Based Access Controlï¼ˆæ ¼çŠ¶è®¿é—®æ§åˆ¶ï¼‰

BLP æ˜¯**å¼ºåˆ¶è®¿é—®æ§åˆ¶ï¼ˆMACï¼‰**çš„ä¸€ç§å®ç°å½¢å¼ï¼Œå®‰å…¨çº§åˆ«æ„æˆä¸€ä¸ª**â€œæ ¼â€ï¼ˆLatticeï¼‰**ã€‚
ä¸»ä½“åªèƒ½è®¿é—®â€œ**ä¸Šä¸‹ç•Œ**â€ä¹‹é—´çš„èµ„æº â€”â€” æ¯”å¦‚ï¼šæœºå¯†ï¼ˆConfidentialï¼‰åªèƒ½è®¿é—®â€œä¿å¯† â‰¤ ç­‰çº§ â‰¤ æœºå¯†â€çš„å¯¹è±¡ã€‚

#### Trusted Subject ç‰¹æƒç”¨æˆ·ä¾‹å¤–

ç‰¹æƒç”¨æˆ·ï¼ˆtrusted subjectï¼‰å¯ä»¥æ‰“ç ´ * å±æ€§è§„åˆ™ï¼ˆå³å…è®¸å†™å…¥ä½çº§åˆ«å¯¹è±¡ï¼‰ï¼Œç”¨äº**åˆæ³•çš„ä¿¡æ¯é™å¯†æ“ä½œ**ï¼ˆå¦‚å…¬å¼€è§£å¯†æ–‡æ¡£ï¼‰ã€‚

#### å±€é™æ€§

- åªå…³æ³¨**ä¿å¯†æ€§** (Confidentiality)ï¼Œå¿½ç•¥å®Œæ•´æ€§ (Integrity) ä¸å¯ç”¨æ€§ (Availability)ï¼›
- å‡è®¾æ‰€æœ‰ç³»ç»Ÿè½¬æ¢éƒ½å®‰å…¨ï¼›
- ä¸æ”¯æŒç°ä»£ç‰¹æ€§å¦‚å…±äº«ã€è”ç½‘ï¼›
- ä¸é˜²æ­¢ covert channelï¼ˆéšè”½é€šé“ï¼‰æ³„æ¼ã€‚

Subjects under lattice-based access controls are assigned positions in a lattice (i.e., a multilayered security structure or multileveled security domains). Subjects can access only those objects that fall into the range between the least upper bound (LUB) (the nearest security label or classification higher than their lattice position) and the greatest (i.e., highest) lower bound (GLB) (the nearest security label or classification lower than their lattice position) of the labels or classifications for their lattice position.

This model is built on a state machine concept and the information flow model. It also employs mandatory access controls and is a lattice-based access control concept. The lattice tiers are the classification levels defined by the security policy of the organization.

There are three basic properties of this state machine:

1. The Simple Security Property states that a subject may not read information at a higher sensitivity level (no read-up).

2. The * (star) Security Property states that a subject may not write information to an object at a lower sensitivity level (no write-down). This is also known as the Confinement Property.

3. The Discretionary Security Property states that the system uses an access matrix to enforce discretionary access control.

These first two properties define the states into which the system can transition. No other transitions are allowed. All states accessible through these two rules are secure states. Thus, Bellâ€“LaPadulaâ€“modeled systems offer state machine model security.

The Bellâ€“LaPadula properties are in place to protect data confidentiality. A subject cannot read an object that is classified at a higher level than the subject is cleared for. Because objects at one level have data that is more sensitive or secret than data in objects at a lower level, a subject (who is not a trusted subject) cannot write data from one level to an object at a lower level. That action would be similar to pasting a top-secret memo into an unclassified document file. The third property enforces a subjectâ€™s job/role-based need to know in order to access an object.

An exception in the Bellâ€“LaPadula model states that a â€œtrusted subjectâ€ is not constrained by the * Security Property. A trusted subject is defined as â€œa subject that is guaranteed not to consummate a securitybreaching information transfer even if it is possible.â€ This means that a trusted subject is allowed to violate the * Security Property and perform a write-down, which is necessary when performing valid object declassification or reclassification.

The Bellâ€“LaPadula model was designed in the 1970s, so it does not support many operations that are common today, such as file sharing and networking. It also assumes secure transitions between security layers and does not address covert channels

### 8. Biba Model - ä»¥å®Œæ•´æ€§ä¸ºæ ¸å¿ƒ

æ ¸å¿ƒåŒºåˆ«ï¼šå…³æ³¨æ•°æ®æ˜¯å¦è¢«ç¯¡æ”¹ - Integrity
Biba æ¨¡å‹è¯ç”Ÿäº Bellâ€“LaPadula ä¹‹åï¼Œæ˜¯å…¶â€œé•œåƒæ¨¡å‹â€ï¼Œå…³æ³¨é˜²æ­¢æ•°æ®æ±¡æŸ“ï¼Œå¼ºè°ƒä¿æŒæ•°æ®å®Œæ•´æ€§ï¼ˆIntegrityï¼‰ï¼Œç‰¹åˆ«é€‚ç”¨äºè´¢åŠ¡ç³»ç»Ÿã€æ•°æ®åº“ç³»ç»Ÿç­‰å¯¹æ•°æ®çœŸå®æ€§è¦æ±‚æé«˜çš„åœºæ™¯ã€‚

#### ä¸¤å¤§æ ¸å¿ƒè§„åˆ™

| å±æ€§                             | è¯´æ˜                                             | åˆ«å               |
| -------------------------------- | ------------------------------------------------ | ------------------ |
| **Simple Integrity Property**    | **â€œNo Read Downâ€**ï¼šä¸èƒ½è¯»å–è¾ƒä½å®Œæ•´æ€§çº§åˆ«çš„æ•°æ® | é˜²æ­¢å¼•å…¥ä¸å¯ä¿¡è¾“å…¥ |
| **Star (\*) Integrity Property** | **â€œNo Write Upâ€**ï¼šä¸èƒ½å†™å…¥è¾ƒé«˜å®Œæ•´æ€§çº§åˆ«çš„æ•°æ®  | é˜²æ­¢æ±¡æŸ“å¯ä¿¡è¾“å‡º   |

âœ…è®°å¿†æ–¹æ³•ï¼š

- **Simple = Readï¼ŒStar = Write**
- Biba = **No Read Down, No Write Up**

#### ä¸¾ä¾‹è¯´æ˜

å‡è®¾ä¸€ä¸ªâ€œé«˜å®Œæ•´æ€§â€è´¢åŠ¡æŠ¥è¡¨ç³»ç»Ÿä¸­ï¼ŒæŸå‘˜å·¥è´¦æˆ·ç­‰çº§è¾ƒä½ï¼Œåˆ™ï¼š

- ä»–**ä¸èƒ½è¯»å–ï¼ˆreadï¼‰**æœªç»éªŒè¯çš„æ•°æ®ï¼ˆå¦‚äº’è”ç½‘ç”¨æˆ·è¾“å…¥ï¼‰ï¼›
- ä»–**ä¸èƒ½å†™å…¥ï¼ˆwriteï¼‰**æ­£å¼æŠ¥è¡¨ï¼Œä»¥é˜²å¼•å…¥é”™è¯¯æˆ–æ¶æ„ä¿®æ”¹ã€‚

------

#### è®¾è®¡ç›®æ ‡

- é˜²æ­¢æœªæˆæƒä¸»ä½“ä¿®æ”¹å¯¹è±¡ï¼›
- é˜²æ­¢æˆæƒä¸»ä½“è¯¯æ”¹å¯¹è±¡ï¼›
- ä¿è¯å¯¹è±¡çš„å†…éƒ¨ä¸€è‡´æ€§ï¼ˆå¦‚æ ¡éªŒå’Œï¼‰å’Œå¤–éƒ¨ä¸€è‡´æ€§ï¼ˆæ•°æ®æºå‡†ç¡®æ€§ï¼‰ï¼›

------

#### å±€é™æ€§

- ä¸æ”¯æŒæœºå¯†æ€§ï¼ˆConfidentialityï¼‰æˆ–å¯ç”¨æ€§ï¼ˆAvailabilityï¼‰ï¼›
- å‡è®¾ç¨‹åºèƒ½æ­£ç¡®æ§åˆ¶å†…éƒ¨æƒé™ï¼›
- ä¸è€ƒè™‘æƒé™åˆ†é…ç­–ç•¥ï¼›
- åŒæ ·ä¸é˜²æ­¢ covert channel æ”»å‡»ã€‚

The Biba model was designed after the Bellâ€“LaPadula model, but it focuses on integrity. The Biba model is also built on a state machine concept, is based on information flow, and is a multilevel model. In fact, the Biba model is the inverted Bellâ€“LaPadula model. The properties of the Biba model are as follows:

1. The Simple Integrity Property states that a subject cannot read an object at a lower integrity level (no read-down).

2. The * (star) Integrity Property states that a subject cannot modify an object at a higher integrity level (no write-up).

In both the Biba and Bellâ€“LaPadula models, there are two properties that are inverses of each other: simple and * (star). However, they may also be labeled as axioms, principles, or rules. What you should focus on is the simple and star designations. Take note that simple is always about reading and star is always about writing. In both cases, the rules define what cannot or should not be done. Usually, what is not prevented or blocked is allowed. Thus, even though a rule is stated as a No declaration, its opposite direction is implied as allowed. On the exam, the first and best answer as to the definition or meaning of a property is the negative statement, but if that is not an option, then the opposite implied operation is the next best selection.

Consider the Biba properties. The second property of the Biba model is pretty straightforward. A subject cannot write to an object at a higher integrity level. That makes sense. What about the first property? Why canâ€™t a subject read an object at a lower integrity level? The answer takes a little thought. Think of integrity levels as being like the purity level of air. You would not want to pump air from the smoking section into the clean room environment. The same applies to data. When integrity is important, you do not want unvalidated data read into validated documents. The potential for data contamination is too great to permit such access.

Biba was designed to address three integrity issues:

1. Prevent modification of objects by unauthorized subjects
2. Prevent unauthorized modification of objects by authorized subjects
3. Protect internal and external object consistency

Biba requires that all subjects and objects have a classification label (it is still a DoD-derived security model). Thus, data integrity protection is dependent on data classification.

Critiques of the Biba model reveal a few drawbacks:

- It addresses only integrity, not confidentiality or availability.
- It focuses on protecting objects from external threats; it assumes that internal threats are handled programmatically.
- It does not address access control management, and it doesnâ€™t provide a way to assign or change an objectâ€™s or subjectâ€™s classification level.
- It does not prevent covert channels.

Memorizing the properties of Bellâ€“LaPadula and Biba can be challenging, but there is a shortcut. If you can memorize the graphical layout in Figure 8.6 above the dotted line, then you can figure out the rest. Notice that Bellâ€“LaPadula is placed on the left and Biba is on the right, and the security benefit of each is listed below the model name. Then only the Bellâ€“LaPadula modelâ€™s simple property is listed. That property is â€œNo Read Up,â€ which is represented by an arrow pointing upward that is crossed out and labeled by an â€œSâ€ for simple and an â€œRâ€ for read. From there, all of the other rules are the opposing element of the pair or inverted. By memorizing the top graphic, once you are in the exam, you can draw that out on the provided dry-erase board. Then, you can quickly create the other rules. First, under Bellâ€“LaPadula draw an arrow pointing down, cross it out, then label it with an â€œ*â€ for start and a â€œWâ€ for write. Now you have the â€œNo Write Downâ€ star property. You can then draw dotted arrows in the opposite direction of these two to indicate the implied opposite direction is allowed. Then take these four arrows of Bellâ€“LaPadula and completely flip them over top to bottom to create the rules for Biba. The result should be the bottom graphic below the dotted line.

#### Bellâ€“LaPadula vs Biba å¯¹æ¯”æ€»ç»“

| ç‰¹å¾             | Bellâ€“LaPadula                 | Biba                               |
| ---------------- | ----------------------------- | ---------------------------------- |
| å…³æ³¨ç‚¹           | **æœºå¯†æ€§ï¼ˆConfidentialityï¼‰** | **å®Œæ•´æ€§ï¼ˆIntegrityï¼‰**            |
| åº”ç”¨åœºæ™¯         | å†›äº‹ã€æƒ…æŠ¥ã€å›½å®¶å®‰å…¨          | å•†ä¸šã€è´¢åŠ¡ã€å·¥ä¸šæ§åˆ¶ç³»ç»Ÿ           |
| Simple å±æ€§      | No Read Up                    | No Read Down                       |
| Star (*) å±æ€§    | No Write Down                 | No Write Up                        |
| é˜²æ­¢ä¿¡æ¯æ³„éœ²æ–¹å‘ | ä»é«˜æœºå¯†å‘ä½æœºå¯†              | ä»ä½å®Œæ•´æ€§å‘é«˜å®Œæ•´æ€§               |
| å…è®¸çš„ä¾‹å¤–       | Trusted Subject å¯é™å¯†        | æ— ç‰¹æƒä¸»ä½“å¯ä»¥ç ´ä¾‹å†™å…¥é«˜å®Œæ•´æ€§æ•°æ® |
| å®‰å…¨å±æ€§è¦†ç›–     | åªå…³æ³¨ä¿å¯†æ€§                  | åªå…³æ³¨å®Œæ•´æ€§                       |

#### è®°å¿†å›¾å½¢æ³•ï¼ˆè€ƒè¯•æŠ€å·§ï¼‰

- Bellâ€“LaPadulaï¼ˆæœºå¯†ï¼‰â†’ âŒâ†‘è¯»ï¼ŒâŒâ†“å†™
- Bibaï¼ˆå®Œæ•´æ€§ï¼‰â†’ âŒâ†“è¯»ï¼ŒâŒâ†‘å†™
- ä¸¤è€…å›¾å½¢å¯¹ç§°ï¼šä¸€ä¸ªå…³æ³¨ä¿¡æ¯**å‘ä¸‹æ³„éœ²**ï¼Œä¸€ä¸ªå…³æ³¨ä¿¡æ¯**å‘ä¸Šä¼ æŸ“**

### 9. Clark-Wilson Model - **åŸºäºå•†ä¸šå®Œæ•´æ€§çš„ä¿æŠ¤**

**æ¨¡å‹ç›®æ ‡**: Clarkâ€“Wilson æ¨¡å‹ä¸“æ³¨äº**å•†ä¸šç¯å¢ƒä¸­çš„æ•°æ®å®Œæ•´æ€§ä¿æŠ¤**ã€‚å…¶æ ¸å¿ƒç†å¿µæ˜¯ï¼š**ä¸å…è®¸ç”¨æˆ·ç›´æ¥è®¿é—®æ•°æ®ï¼Œè€Œæ˜¯å¿…é¡»é€šè¿‡å—æ§ç¨‹åºï¼ˆTransformation Procedure, TPï¼‰è¿›è¡Œè®¿é—®æˆ–ä¿®æ”¹**ã€‚

#### æ ¸å¿ƒæœºåˆ¶ï¼šè®¿é—®æ§åˆ¶ä¸‰å…ƒç»„ï¼ˆAccess Control Tripletï¼‰

| ç»„æˆéƒ¨åˆ†                           | ä½œç”¨                           |
| ---------------------------------- | ------------------------------ |
| **Subjectï¼ˆç”¨æˆ·ï¼‰**                | å‘èµ·æ“ä½œè¯·æ±‚çš„äººæˆ–è¿›ç¨‹         |
| **TPï¼ˆTransformation Procedureï¼‰** | æ§åˆ¶å¯¹æ•°æ®æ“ä½œçš„å—é™ç¨‹åºæ¥å£   |
| **Objectï¼ˆå¯¹è±¡ï¼‰**                 | è¢«è®¿é—®çš„æ•°æ®é¡¹ï¼ˆå¦‚æ•°æ®åº“è®°å½•ï¼‰ |

ç”¨æˆ·åªèƒ½é€šè¿‡ TP æ¥å£è®¿é—®æ•°æ®ï¼Œè¿™ç§â€œé—´æ¥è®¿é—®â€æ˜¯æ¨¡å‹çš„æ ¹æœ¬ã€‚

#### å…³é”®æ¦‚å¿µ

| é¡¹ç›®                                        | å®šä¹‰                             |
| ------------------------------------------- | -------------------------------- |
| **CDIï¼ˆConstrained Data Itemï¼‰**            | è¢«æ¨¡å‹ä¿æŠ¤çš„æ•æ„Ÿæ•°æ®             |
| **UDIï¼ˆUnconstrained Data Itemï¼‰**          | æœªå—æ§åˆ¶çš„è¾“å…¥æˆ–è¾“å‡ºæ•°æ®         |
| **TPï¼ˆTransformation Procedureï¼‰**          | ä»…æœ‰çš„å¯ç”¨äºä¿®æ”¹ CDI çš„ç¨‹åº      |
| **IVPï¼ˆIntegrity Verification Procedureï¼‰** | ç”¨äºæ‰«æéªŒè¯æ•°æ®å®Œæ•´æ€§çš„ä¸€ç§æœºåˆ¶ |

#### ä¸¤å¤§æ ¸å¿ƒåŸåˆ™

1. **Well-Formed Transactionsï¼ˆç»“æ„è‰¯å¥½çš„äº‹åŠ¡ï¼‰**
   åªæœ‰é€šè¿‡æˆæƒ TP æ‰èƒ½ä¿®æ”¹æ•°æ®ï¼Œé˜²æ­¢æœªæˆæƒç¯¡æ”¹ã€‚
2. **Separation of Dutiesï¼ˆèŒè´£åˆ†ç¦»ï¼‰**
   æ“ä½œåˆ†ç¦»ï¼ˆå¦‚ä¸€äººå½•å…¥ã€ä¸€äººå®¡æ‰¹ï¼‰ï¼Œé˜²æ­¢å†…éƒ¨æ¬ºè¯ˆæˆ–æ»¥ç”¨ã€‚

------

#### å…¶ä»–ç‰¹æ€§

- ä½¿ç”¨**å—é™æ¥å£æ¨¡å‹**ï¼Œå¯é™åˆ¶ç”¨æˆ·åªèƒ½çœ‹åˆ°/æ“ä½œæˆæƒå†…å®¹ï¼›
- è™½è®¾è®¡ç”¨äºå®Œæ•´æ€§ï¼Œä½†ä¸­é—´å±‚ TP ä¹Ÿå¯å…¼é¡¾ä¿å¯†æ€§ï¼›
- å¸¸ç”¨äºé‡‘èã€ERPã€å•†ä¸šæ•°æ®ç³»ç»Ÿã€‚

The Clarkâ€“Wilson model uses a multifaceted approach to enforcing data integrity. Instead of defining a formal state machine, the Clarkâ€“Wilson model defines each data item and allows modifications through only a limited or controlled intermediary program or interface.

The Clarkâ€“Wilson model does not require the use of a lattice structure; rather, it uses a three-part relationship of subject/program/object (or subject/transaction/object) known as a triple or an access control triplet. Subjects do not have direct access to objects. Objects can be accessed only through programs. Through the use of two principlesâ€”well-formed transactions and separation of dutiesâ€”the Clarkâ€“Wilson model provides an effective means to protect integrity.

Well-formed transactions take the form of programs. A subject is able to access objects only by using a program, interface, or access portal. Each program has specific limitations on what it can and cannot do to an object (such as a database or other resource). This effectively limits the subjectâ€™s capabilities. This is known as a constrained, limiting, or restrictive interface. If the programs are properly designed, then the triple relationship provides a means to protect the integrity of the object.

Clarkâ€“Wilson defines the following items and procedures:

- A constrained data item (CDI) is any data item whose integrity is protected by the security model.
- An unconstrained data item (UDI) is any data item that is not controlled by the security model. Any data that is to be input and hasnâ€™t been validated, or any output, would be considered an unconstrained data item.
- An integrity verification procedure (IVP) is a procedure that scans data items and confirms their integrity.
- Transformation procedures (TPs) are the only procedures that are allowed to modify a CDI. The limited access to CDIs through TPs forms the backbone of the Clarkâ€“Wilson integrity model.

The Clarkâ€“Wilson model uses security labels to grant access to objects, but only through transformation procedures and a restricted interface model. A restricted interface model uses classification-based restrictions to offer only subject-specific authorized information and functions. One subject at one classification level will see one set of data and have access to one set of functions, whereas another subject at a different classification level will see a different set of data and have access to a different set of functions. The different functions made available to different levels or classes of users may be implemented by either showing all functions to all users but disabling those that are not authorized for a specific user or by showing only those functions granted to a specific user. Through these mechanisms, the Clarkâ€“Wilson model ensures that data is protected from unauthorized changes from any user. In effect, the Clarkâ€“Wilson model enforces separation of duties. The Clarkâ€“Wilson design makes it a common model for commercial applications.

**Note:** The Clarkâ€“Wilson model was designed to protect integrity using the access control triplet. However, though the intermediary interface can be programmed to limit what can be done to an object by a subject, it can just as easily be programmed to limit or restrict what objects are shown to a subject. Thus, this concept can lend itself readily to protect confidentiality. In many situations there is an intermediary program between a subject and an object. If the focus of that intermediary is to protect integrity, then it is an implementation of the Clarkâ€“Wilson model. If it is intended to protect confidentiality, then they are benefiting from an alternate use of the intermediary program. Use of the access control triplet to protect confidentiality does not seem to have its own model name.

### 10. Brewer and Nash Modelï¼ˆä¸­å›½å¢™æ¨¡å‹ï¼‰

æ¨¡å‹ç›®æ ‡: Brewerâ€“Nash æ¨¡å‹èšç„¦äº**åŠ¨æ€è®¿é—®æ§åˆ¶ + å†²çªéš”ç¦»**ï¼Œä¸“ä¸ºè§£å†³â€œ**åˆ©ç›Šå†²çªé—®é¢˜**â€è€Œè®¾è®¡ï¼Œä¾‹å¦‚ä¼šè®¡å¸ˆä¸èƒ½åŒæ—¶æœåŠ¡äºä¸¤å®¶ç«äº‰å…¬å¸ã€‚

#### åº”ç”¨åœºæ™¯

- è´¢åŠ¡å®¡è®¡
- å’¨è¯¢å…¬å¸
- æ³•å¾‹æœåŠ¡
- ä»»ä½•å¯èƒ½äº§ç”Ÿ**åˆ©ç›Šå†²çª**çš„ç¯å¢ƒ

#### å·¥ä½œåŸç†

- ä¸€æ—¦ç”¨æˆ·è®¿é—®äº†ä¸€ä¸ª**å†²çªç±»ï¼ˆConflict Classï¼‰ä¸­çš„æ•°æ®ï¼ˆå¦‚å…¬å¸ A çš„æ–‡æ¡£ï¼‰ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨å±è”½**è¯¥ç”¨æˆ·è®¿é—®è¯¥å†²çªç±»ä¸­å…¶ä»–å®ä½“ï¼ˆå¦‚å…¬å¸ Bï¼‰æ•°æ®çš„æƒé™ã€‚
- è¿™ç§**â€œä¿¡æ¯æ°”æ³¡â€æˆ–â€œåŠ¨æ€éš”ç¦»â€**æœºåˆ¶ç§°ä¸º **Cone of Silence** æˆ– Ethical Wallã€‚
- å†²çªç±»ä¸ç”¨æˆ·æƒé™æ˜¯**åŠ¨æ€å˜åŒ–çš„**ï¼Œéšç€è®¿é—®è¡Œä¸ºè‡ªåŠ¨è°ƒæ•´ã€‚

#### ç»„æˆå…ƒç´ 

| é¡¹ç›®                           | å®šä¹‰                                     |
| ------------------------------ | ---------------------------------------- |
| **Subjectï¼ˆç”¨æˆ·ï¼‰**            | æ“ä½œç³»ç»Ÿçš„ä¸»ä½“                           |
| **Objectï¼ˆæ•°æ®å¯¹è±¡ï¼‰**         | å¦‚å…¬å¸æ–‡æ¡£ã€é¡¹ç›®æ–‡ä»¶ç­‰                   |
| **Conflict Classï¼ˆå†²çªç±»ï¼‰**   | ä¸€ç»„ç›¸äº’å­˜åœ¨åˆ©ç›Šå†²çªçš„å¯¹è±¡ï¼ˆå¦‚ç«äº‰å…¬å¸ï¼‰ |
| **Access Historyï¼ˆè®¿é—®å†å²ï¼‰** | ç”¨äºåŠ¨æ€å†³å®šå½“å‰å…è®¸è®¿é—®çš„æ•°æ®é›†         |

### ç¤ºä¾‹è¯´æ˜

å®¡è®¡å‘˜ A ç¬¬ä¸€æ¬¡è®¿é—®äº†â€œå…¬å¸ Xâ€çš„è´¢åŠ¡æŠ¥è¡¨ï¼Œé‚£ä¹ˆç³»ç»Ÿå°†è‡ªåŠ¨é˜»æ­¢ä»–è®¿é—®â€œå…¬å¸ Yâ€ï¼ˆç«äº‰å¯¹æ‰‹ï¼‰çš„ä»»ä½•æ•æ„Ÿæ•°æ®ï¼Œç›´åˆ°å½“å‰ä»»åŠ¡ç»“æŸã€‚

The Brewer and Nash model was created to permit access controls to change dynamically based on a userâ€™s previous activity (making it a kind of state machine model as well). This model applies to a single integrated database; it seeks to create security domains that are sensitive to the notion of conflict of interest (for example, someone who works at company C who has access to proprietary data for company A should not also be allowed access to similar data for company B if those two companies compete with each other). This model creates a class of data that defines which security domains are potentially in conflict and prevents any subject with access to one domain that belongs to a specific conflict class from accessing any other domain that belongs to the same conflict class. Metaphorically, this puts a wall around all other information in any conflict class. Thus, this model also uses the principle of data isolation within each conflict class to keep users out of potential conflict-of-interest situations (for example, management of company datasets). Because company relationships change all the time, dynamic updates to members of and definitions for conflict classes are important.

Another way of looking at or thinking of the Brewer and Nash model is of an administrator having full control access to a wide range of data in a system based on their assigned job responsibilities and work tasks. However, at the moment an action is taken against any data item, the administratorâ€™s access to any conflicting data items is temporarily blocked. Only data items that relate to the initial data item can be accessed during the operation. Once the task is completed, the administratorâ€™s access returns to full control.

Brewer and Nash was sometimes known as the Chinese Wall model, but this term is deprecated. Instead, other terms of â€œethical wallâ€ and â€œcone of silenceâ€ have been used to describe Brewer and Nash.

| ç‰¹å¾           | **Clarkâ€“Wilson æ¨¡å‹**                | **Brewerâ€“Nash æ¨¡å‹**               |
| -------------- | ------------------------------------ | ---------------------------------- |
| æ ¸å¿ƒç›®æ ‡       | **ä¿æŠ¤å®Œæ•´æ€§**ï¼ˆé˜²æ­¢æ•°æ®è¢«éšæ„æ›´æ”¹ï¼‰ | **é¿å…åˆ©ç›Šå†²çª**ï¼ˆé˜²æ­¢å¤šé‡è®¿é—®ï¼‰   |
| æ¨¡å‹ç±»å‹       | **ä¸‰å…ƒç»„æ¨¡å‹ï¼ˆSubjectâ€“TPâ€“Objectï¼‰**  | **åŠ¨æ€çŠ¶æ€æ¨¡å‹ï¼ˆåŸºäºè®¿é—®å†å²ï¼‰**   |
| æ˜¯å¦ä½¿ç”¨ä¸­é—´å±‚ | âœ… é€šè¿‡å—æ§æ¥å£ç¨‹åºè®¿é—®æ•°æ®           | âŒ ç›´æ¥åŠ¨æ€éš”ç¦»                     |
| åº”ç”¨åœºæ™¯       | å•†ä¸šç³»ç»Ÿã€é‡‘èã€ERP                  | æ³•å¾‹ã€è´¢åŠ¡ã€å’¨è¯¢ç­‰æœ‰å†²çªéœ€æ±‚çš„ç¯å¢ƒ |
| åŠ¨æ€æƒé™è°ƒæ•´   | âŒ é™æ€è®¿é—®æ§åˆ¶ï¼Œéœ€é¢„å®šä¹‰è§’è‰²å’Œæ¥å£   | âœ… æ ¹æ®ç”¨æˆ·è¡Œä¸ºåŠ¨æ€è°ƒæ•´æƒé™         |
| æ˜¯å¦å…³æ³¨ä¿å¯†æ€§ | å¯æ‰©å±•ä¿æŠ¤ä¿å¯†æ€§ï¼ˆéåŸå§‹è®¾è®¡ç›®çš„ï¼‰   | å¯å®ç°éšå¼ä¿å¯†ï¼ˆé€šè¿‡éš”ç¦»å®ç°ï¼‰     |

#### æ€»ç»“è®°å¿†å»ºè®®

- **Clarkâ€“Wilson**ï¼šå¼ºè°ƒæµç¨‹æ§åˆ¶ + æ•°æ®æ¥å£ â†’ å¼ºå®Œæ•´æ€§ + é€‚åˆå•†ä¸šç³»ç»Ÿ
- **Brewerâ€“Nash**ï¼šå¼ºè°ƒåŠ¨æ€åˆ©ç›Šéš”ç¦» + é¿å…å†²çª â†’ é€‚åˆæ³•å¾‹è´¢åŠ¡

### 11. Goguen-Meseguer Modelï¼ˆéå¹²æ‰°æ¨¡å‹çš„å¥ åŸºï¼‰

æ¨¡å‹ç±»å‹ï¼š**å®Œæ•´æ€§æ¨¡å‹**ï¼ˆéä¼ ç»Ÿï¼‰

#### æ¨¡å‹æ ¸å¿ƒæ€æƒ³

- åŸºäº **è‡ªåŠ¨åŒ–ç†è®º** å’Œ **é¢†åŸŸåˆ†ç¦»ï¼ˆdomain separationï¼‰**ã€‚
- æ¯ä¸ªä¸»ä½“ï¼ˆsubjectï¼‰è¢«äº‹å…ˆæŒ‡å®šèƒ½è®¿é—®çš„ä¸€ç»„å¯¹è±¡ï¼ˆobjectï¼‰ã€‚
- ä¸»ä½“åªèƒ½å¯¹**é¢„å®šä¹‰å¯¹è±¡é›†åˆ**æ‰§è¡Œ**é¢„å®šä¹‰æ“ä½œ**ï¼Œä¸å…è®¸ä»»æ„è¡Œä¸ºã€‚
- **ä¸€ä¸ªä¸»ä½“åŸŸçš„æˆå‘˜ä¸èƒ½å½±å“å¦ä¸€ä¸ªåŸŸçš„æˆå‘˜**ï¼Œå®ç°è¡Œä¸ºéš”ç¦»ã€‚

**ç›®æ ‡ï¼š**é€šè¿‡é¢„å®šä¹‰è®¿é—®è§„åˆ™å’Œåˆ†ç»„éš”ç¦»ï¼Œ**é˜²æ­¢ä¸»ä½“ä¹‹é—´çš„äº¤å‰å¹²æ‰°ï¼ˆinterferenceï¼‰**ï¼Œå®ç°é«˜å®Œæ•´æ€§æ§åˆ¶ã€‚

The Goguenâ€“Meseguer model is an integrity model, although not as well known as Biba and the others. In fact, this model is said to be the foundation of noninterference conceptual theories. Often when someone refers to a noninterference model, they are actually referring to the Goguenâ€“Meseguer model.

The Goguenâ€“Meseguer model is based on predetermining the set or domain (i.e., a list) of objects that a subject can access. This model is based on automation theory and domain separation. This means subjects are allowed only to perform predetermined actions against predetermined objects. When similar users are grouped into their own domain (that is, collective), the members of one subject domain cannot interfere with the members of another subject domain. Thus, subjects are unable to interfere with each otherâ€™s activities.

### 12. Sutherland Modelï¼ˆéå¹²æ‰°çš„å½¢å¼åŒ–è¡¨è¾¾ï¼‰

æ¨¡å‹ç±»å‹ï¼š**å®Œæ•´æ€§æ¨¡å‹**

#### æ¨¡å‹æ ¸å¿ƒ

- åŸºäº **çŠ¶æ€æœºæ¨¡å‹ï¼ˆState Machineï¼‰** å’Œ **ä¿¡æ¯æµæ¨¡å‹ï¼ˆInformation Flowï¼‰**ï¼›
- ä¸å¼ºè°ƒå…·ä½“çš„ä¿æŠ¤æœºåˆ¶ï¼Œè€Œæ˜¯å…³æ³¨ï¼š
  - åˆæ³•çš„åˆå§‹çŠ¶æ€ï¼›
  - è¢«å…è®¸çš„çŠ¶æ€è½¬æ¢ï¼›
  - ç³»ç»ŸçŠ¶æ€ä¸å¯è¢«éæˆæƒä¸»ä½“æ“æ§ã€‚

åº”ç”¨åœºæ™¯ä¸¾ä¾‹ï¼š

- é˜²æ­¢ **éšè”½é€šé“ï¼ˆCovert Channelï¼‰**ï¼›
- ä¿æŠ¤å†³ç­–è¿‡ç¨‹æˆ–è®¡ç®—è¿‡ç¨‹ä¸è¢«ä¿¡æ¯æ³„éœ²æˆ–ç¯¡æ”¹ã€‚

The Sutherland model is an integrity model. It focuses on preventing interference in support of integrity. It is formally based on the state machine model and the information flow model. However, it does not directly indicate specific mechanisms for protection of integrity. Instead, the model is based on the idea of defining a set of system states, initial states, and state transitions. Through the use of only these predetermined secure states, integrity is maintained and interference is prohibited.

A common example of the Sutherland model is its use to prevent a covert channel from being used to influence the outcome of a process or activity. 

### 13. Graham-Denning Model - **é¢å‘å¯¹è±¡/ä¸»ä½“ç”Ÿå‘½å‘¨æœŸçš„æƒé™æ¨¡å‹**

æ¨¡å‹ç±»å‹ï¼š**è®¿é—®æ§åˆ¶æ¨¡å‹**

æ¨¡å‹ç‰¹è‰²: é€šè¿‡**å…«ç§æ“ä½œè§„åˆ™ï¼ˆprotection rulesï¼‰**å®ç°ä¸»ä½“ä¸å¯¹è±¡çš„å®‰å…¨ç”Ÿå‘½å‘¨æœŸç®¡ç†ã€‚

#### å…«ä¸ªæ“ä½œ

1. åˆ›å»ºå¯¹è±¡
2. åˆ›å»ºä¸»ä½“
3. åˆ é™¤å¯¹è±¡
4. åˆ é™¤ä¸»ä½“
5. æˆäºˆè¯»å–æƒé™
6. æˆäºˆä¼ é€’ï¼ˆGrantï¼‰æƒé™
7. æˆäºˆåˆ é™¤æƒé™
8. æˆäºˆè½¬ç§»ï¼ˆTransferï¼‰æƒé™

è¿™äº›æ“ä½œä½œç”¨äº **è®¿é—®æ§åˆ¶çŸ©é˜µï¼ˆAccess Control Matrixï¼‰** ä¸Šâ€”â€”ç±»ä¼¼äºç°ä»£æ“ä½œç³»ç»Ÿçš„æƒé™åˆ†é…é€»è¾‘ã€‚

The Grahamâ€“Denning model is focused on the secure creation and deletion of both subjects and objects. Grahamâ€“Denning is a collection of eight primary protection rules or actions that define the boundaries of certain secure actions:

- Securely create an object.
- Securely create a subject.
- Securely delete an object.
- Securely delete a subject.
- Securely provide the read access right.
- Securely provide the grant access right.
- Securely provide the delete access right.
- Securely provide the transfer access right.

Usually the specific abilities or permissions of a subject over a set of objects is defined in an access matrix (aka access control matrix).

### 14. Harrison-Ruzzo-Ullman Model (HRU) æ¨¡å‹ï¼š**å¯ä¿®æ”¹è®¿é—®çŸ©é˜µçš„æ¨¡å‹**

æ¨¡å‹ç±»å‹ï¼š**åŠ¨æ€è®¿é—®æ§åˆ¶æ¨¡å‹**

#### HRU çš„æ ¸å¿ƒæ‰©å±•

- æ˜¯ **Grahamâ€“Denning æ¨¡å‹çš„æ‰©å±•**ï¼›
- å¼ºè°ƒï¼š
  - æƒé™çš„**åŠ¨æ€èµ‹äºˆä¸æ’¤é”€**ï¼›
  - å¯¹æƒé™æ“ä½œæœ¬èº«çš„æ§åˆ¶æµç¨‹ï¼›
  - ç³»ç»ŸçŠ¶æ€ç”±ä¸€ä¸ªå¯ä¿®æ”¹çš„**è®¿é—®æ§åˆ¶çŸ©é˜µ**è¡¨ç¤ºï¼›
  - æƒé™ç¼–è¾‘ç”±ä¸€ç»„**åŸè¯­ï¼ˆprimitivesï¼‰å‘½ä»¤**å®ç°ã€‚

#### åŸè¯­æ“ä½œåŒ…æ‹¬

- å¢åŠ /ç§»é™¤ æƒé™ï¼›
- å¢åŠ /ç§»é™¤ ä¸»ä½“ä¸å¯¹è±¡ï¼›
- æ‰€æœ‰æ“ä½œå¿…é¡»æ»¡è¶³ä¸€è‡´æ€§è§„åˆ™ï¼š**äº‹åŠ¡åŸå­æ€§ï¼ˆè¦ä¹ˆå…¨éƒ¨æˆåŠŸï¼Œè¦ä¹ˆå…¨éƒ¨å¤±è´¥ï¼‰**ã€‚

The Harrisonâ€“Ruzzoâ€“Ullman (HRU) model focuses on the assignment of object access rights to subjects as well as the resilience of those assigned rights. It is an extension of the Grahamâ€“Denning model. It is centered around the establishment of a finite set of procedures (or access rights) that can be used to edit or alter the access rights of a subject over an object. The state of access rights under HRU can be expressed in a matrix, where the rows are subjects and the columns are objects (which will include the subjects because they can be objects as well). The intersection of each row and column will include the specific procedures that each subject is allowed to perform against each object. Additionally, a finite set of commands or primitives is defined that controls how the matrix can be modified by authorized subjects. These primitives include adding or removing access rights, subjects, and/or objects from the matrix. There are also integrity rules, such as: in order to create or add a subject or object to the matrix, it must not already exist; in order to remove a subject or object from the matrix, it must already exist; and if several commands are performed at once, they must all operate successfully or none of the commands will be applied.

#### Disambiguating the Word â€œStarâ€ in Models

##### å…³äºâ€œStarâ€çš„å¤šé‡å«ä¹‰

| ä½¿ç”¨ä½ç½®               | å«ä¹‰                              | è¯´æ˜                                                        |
| ---------------------- | --------------------------------- | ----------------------------------------------------------- |
| Bellâ€“LaPadula æ¨¡å‹     | *ï¼ˆæ˜Ÿå·ï¼‰å®‰å…¨å±æ€§                 | **ä¸å…è®¸å‘ä½çº§åˆ«å†™å…¥ï¼ˆNo Write Downï¼‰**ï¼Œä¿æŠ¤**ä¿å¯†æ€§**     |
| Biba æ¨¡å‹              | *ï¼ˆæ˜Ÿå·ï¼‰å®Œæ•´æ€§å±æ€§               | **ä¸å…è®¸å‘é«˜å®Œæ•´æ€§å¯¹è±¡å†™å…¥ï¼ˆNo Write Upï¼‰**ï¼Œä¿æŠ¤**å®Œæ•´æ€§** |
| CSA STAR Program       | Security Trust Assurance and Risk | **äº‘å®‰å…¨è”ç›Ÿçš„è®¤è¯ä¸å®¡è®¡è®¡åˆ’**ï¼Œéæ¨¡å‹                      |
| Galbraith's Star Model | ä¼ä¸šç»„ç»‡ç»“æ„æ¨¡å‹                  | ä¾§é‡äºæˆ˜ç•¥ã€ç»“æ„ã€äººäº‹ç­‰ç»„ç»‡ç®¡ç†ï¼Œ**ä¸ä¿¡æ¯å®‰å…¨æ— å…³**        |

åœ¨è€ƒè¯•æˆ–å®é™…ç¯å¢ƒä¸­é‡åˆ°â€œstarâ€ï¼Œè¯·ç»“åˆä¸Šä¸‹æ–‡åˆ¤æ–­æ˜¯ï¼š

- å®‰å…¨æ¨¡å‹çš„å±æ€§ï¼ˆBellâ€“LaPadula / Bibaï¼‰ï¼›
- äº‘æœåŠ¡å®¡è®¡æ¡†æ¶ï¼ˆCSA STARï¼‰ï¼›
- ç»„ç»‡è®¾è®¡ç†è®ºï¼ˆGalbraithï¼‰ï¼›

The term star presents a few challenges when it comes to security models. For one thing, there is no formal security model named â€œStar Model.â€ However, both the Bellâ€“LaPadula and the Biba models have a star property, which is discussed in their respective sections in this chapter.

Although not a model, the Cloud Security Alliance (CSA) also has a STAR program. CSAâ€™s Security Trust Assurance and Risk (STAR) program focuses on improving cloud service provider (CSP) security through auditing, transparency, and integration of standards.

Although not related to security, there is also Galbraithâ€™s Star Model, which helps businesses organize divisions and departments to achieve business missions and goals and adjust over time for long-term viability. This model is based around five main areas of business administration that need to be managed, balanced, and harnessed toward the mission and goals of the organization. The five areas of Galbraithâ€™s Star Model are Strategy, Structure, Processes, Rewards, and People.

Understanding how â€œstarâ€ is used in the context of the Bellâ€“LaPadula and Biba models, CSAâ€™s STAR program, and Galbraithâ€™s Star Model will help you distinguish what is meant when you see the word used in different contexts.

æ€»ç»“å¯¹æ¯”

| æ¨¡å‹            | ç±»å‹         | å…³é”®æ¦‚å¿µ                  | ç‰¹ç‚¹/é€‚ç”¨                               |
| --------------- | ------------ | ------------------------- | --------------------------------------- |
| Goguenâ€“Meseguer | å®Œæ•´æ€§       | åŸŸéš”ç¦»ã€é¢„å®šä¹‰è®¿é—®        | éå¹²æ‰°ç†è®ºåŸºç¡€                          |
| Sutherland      | å®Œæ•´æ€§       | çŠ¶æ€é™åˆ¶ã€é˜²æ­¢å¹²æ‰°        | åº”ç”¨äºéšè”½é€šé“é˜²æŠ¤                      |
| Grahamâ€“Denning  | è®¿é—®æ§åˆ¶     | 8é¡¹å®‰å…¨æ“ä½œ               | å®‰å…¨ç”Ÿå‘½å‘¨æœŸç®¡ç†                        |
| HRU             | è®¿é—®æ§åˆ¶     | æƒé™åŠ¨æ€ç¼–è¾‘+çŸ©é˜µæ¨¡å‹     | å¯¹æƒé™æ¼”åŒ–å»ºæ¨¡                          |
| â€œStarâ€ å±æ€§     | å®‰å…¨æ¨¡å‹æ¦‚å¿µ | * è¯»/å†™é™åˆ¶ï¼ˆä¸æ–¹å‘ç›¸å…³ï¼‰ | ä¿å¯†æ€§ï¼ˆBell-LaPadulaï¼‰ä¸å®Œæ•´æ€§ï¼ˆBibaï¼‰ |

#### **ä¿¡æ¯å®‰å…¨æ¨¡å‹å¯¹ç…§æ€»è§ˆè¡¨**

| æ¨¡å‹åç§°                        | æ¨¡å‹ç±»å‹                   | æ ¸å¿ƒå…³æ³¨ç‚¹               | å…³é”®å±æ€§                                   | é€‚ç”¨åœºæ™¯/ä¼˜åŠ¿                       |
| ------------------------------- | -------------------------- | ------------------------ | ------------------------------------------ | ----------------------------------- |
| **Bellâ€“LaPadula**               | æœºå¯†æ€§ï¼ˆConfidentialityï¼‰  | é˜²æ­¢ä¿¡æ¯æ³„éœ²             | - No Read Up- No Write Downï¼ˆ* Starï¼‰- DAC | å¤šçº§ä¿å¯†ç³»ç»Ÿï¼Œå†›ç”¨/æ”¿åºœä¿å¯†æ•°æ®ç¯å¢ƒ |
| **Biba**                        | å®Œæ•´æ€§ï¼ˆIntegrityï¼‰        | é˜²æ­¢æ•°æ®è¢«ä½å®Œæ•´æ€§æ±¡æŸ“   | - No Read Down- No Write Upï¼ˆ* Starï¼‰      | å•†ä¸šåœºæ™¯ï¼Œæ•°æ®ä¸èƒ½è¢«ç¯¡æ”¹æˆ–æ±¡æŸ“      |
| **Clarkâ€“Wilson**                | å®Œæ•´æ€§ï¼ˆIntegrityï¼‰        | åˆ†ç¦»èŒè´£ + é™åˆ¶æ¥å£      | - ä¸‰å…ƒç»„(S/P/O)- TP/CDI/UDI/IVP            | å•†ä¸šæ•°æ®åº“ç³»ç»Ÿï¼ŒERP/é‡‘èç³»ç»Ÿ        |
| **Brewerâ€“Nash**ï¼ˆChinese Wallï¼‰ | åŠ¨æ€è®¿é—®æ§åˆ¶               | é¿å…åˆ©ç›Šå†²çª             | åŠ¨æ€è®¿é—®å†³ç­–ï¼ˆåŸºäºå†å²è¡Œä¸ºï¼‰               | é¡¾é—®ã€æ³•å¾‹ã€æŠ•èµ„è¡Œä¸šçš„åˆè§„æ§åˆ¶      |
| **Goguenâ€“Meseguer**             | å®Œæ•´æ€§ / éå¹²æ‰°            | åŸŸåˆ†ç¦» / è‡ªåŠ¨æœº          | å›ºå®šè¡Œä¸ºé›†åˆ + æ— äº¤å‰åŸŸå¹²æ‰°                | åŸºç¡€ç†è®ºï¼Œéå¹²æ‰°æ¨¡å‹çš„èµ·ç‚¹          |
| **Sutherland**                  | å®Œæ•´æ€§ / ä¿¡æ¯æµ            | çŠ¶æ€é™åˆ¶ + ç¦æ­¢æ¨ç†å¹²æ‰°  | åˆæ³•çŠ¶æ€è½¬æ¢ï¼Œæ§åˆ¶éšè”½é€šé“                 | é«˜å®Œæ•´æ€§è®¡ç®—ã€æ¨ç†é˜»æ–­ç³»ç»Ÿ          |
| **Grahamâ€“Denning**              | è®¿é—®æ§åˆ¶ï¼ˆAccess Controlï¼‰ | ä¸»ä½“ä¸å¯¹è±¡çš„ç”Ÿå‘½å‘¨æœŸç®¡ç† | 8ä¸ªæ“ä½œè§„åˆ™ï¼šåˆ›å»º/åˆ é™¤/æˆæƒ/è½¬ç§»ç­‰         | å®‰å…¨ç®¡ç†ç³»ç»Ÿï¼Œæ“ä½œç³»ç»Ÿ              |
| **Harrisonâ€“Ruzzoâ€“Ullman (HRU)** | åŠ¨æ€è®¿é—®æ§åˆ¶               | æƒé™å˜åŒ–åŠå¯éªŒè¯æ€§       | åŸºäºè®¿é—®æ§åˆ¶çŸ©é˜µ + åŸè¯­æ“ä½œè§„åˆ™            | ç»†ç²’åº¦æƒé™æ§åˆ¶ã€æƒé™ç»§æ‰¿ä¸å˜åŒ–      |
| **Takeâ€“Grant**ï¼ˆæœ‰å‘å›¾ï¼‰        | æƒé™ä¼ é€’                   | æƒé™ç»§æ‰¿ä¸å¤åˆ¶           | Take / Grant / Create / Remove å››è§„åˆ™      | æˆæƒç³»ç»Ÿï¼Œç°ä»£ ACL æƒé™ç»“æ„å»ºæ¨¡     |
| **Access Control Matrix**       | åŸºç¡€è®¿é—®æ§åˆ¶æ¨¡å‹           | ä¸»ä½“-å¯¹è±¡-æ“ä½œ æ˜ å°„      | è¡Œä¸ºçŸ©é˜µï¼šè¡Œ=ä¸»ä½“ï¼Œåˆ—=å¯¹è±¡                 | ç³»ç»Ÿæƒé™è¡¨å»ºæ¨¡åŸºç¡€ï¼Œæ“ä½œç³»ç»Ÿ        |
| **State Machine Model**         | åŸºç¡€ç†è®ºæ¨¡å‹               | çŠ¶æ€ + çŠ¶æ€è½¬æ¢çš„å®‰å…¨æ€§  | å®‰å…¨çŠ¶æ€å®šä¹‰ä¸éªŒè¯                         | å…¶ä»–æ¨¡å‹ï¼ˆå¦‚ Bell/Bibaï¼‰çš„åŸºç¡€      |
| **Information Flow Model**      | ä¿¡æ¯æµæ§åˆ¶                 | æ§åˆ¶ä¿¡æ¯æµå‘ã€ç±»å‹       | é˜»æ­¢æœªæˆæƒæµåŠ¨ + æ§åˆ¶è½¬æ¢è·¯å¾„              | å¤šçº§ç³»ç»Ÿã€æ··åˆçº§åˆ«å¯¹è±¡æµ            |
| **Noninterference**             | ä¿¡æ¯æµæ§åˆ¶                 | ä¸å¯è§æ€§ã€æ— å½±å“è¡Œä¸º     | é«˜çº§åˆ«è¡Œä¸ºä¸å½±å“ä½çº§åˆ«çŠ¶æ€                 | éšè”½é€šé“/æ¨ç†æ”»å‡»é˜²æŠ¤               |
| **Galbraith Star Model**        | éå®‰å…¨é¢†åŸŸ                 | ä¼ä¸šç»„ç»‡è®¾è®¡             | æˆ˜ç•¥/ç»“æ„/äººå‘˜ç­‰äº”å…ƒç´                      | ï¼ˆéä¿¡æ¯å®‰å…¨ï¼‰ç»„ç»‡æˆ˜ç•¥ç®¡ç†          |
| **CSA STAR Program**            | äº‘å®‰å…¨æ¡†æ¶                 | å®¡è®¡ / é£é™© / ä¿¡ä»»éªŒè¯   | è‡ªè¯„ã€è®¤è¯ã€æŒç»­ç›‘æ§                       | äº‘æœåŠ¡ä¾›åº”å•†åˆè§„ä¸é€æ˜è®¤è¯          |

**æœ¯è¯­è§£é‡Šï¼š**

- **No Read Up / No Write Down**ï¼šé«˜æœºå¯†å¯¹è±¡ä¸èƒ½è¢«ä½æƒé™ä¸»ä½“è¯»å– / ä¸èƒ½å°†æœºå¯†ä¿¡æ¯å†™åˆ°ä½ç­‰çº§ï¼›
- **CDI / UDI / TP / IVP**ï¼šClarkâ€“Wilson æ¨¡å‹çš„å…ƒç´ ï¼ˆå—é™æ•°æ®/éå—é™æ•°æ®/è½¬æ¢è¿‡ç¨‹/éªŒè¯è¿‡ç¨‹ï¼‰ï¼›
- **ä¸‰å…ƒç»„ï¼ˆSubjectâ€“Programâ€“Objectï¼‰**ï¼šä¸€ç§ç¡®ä¿æ“ä½œå—æ§çš„ç»“æ„æ–¹å¼ï¼›
- **ACLï¼ˆè®¿é—®æ§åˆ¶åˆ—è¡¨ï¼‰**ï¼šä»¥å¯¹è±¡ä¸ºä¸­å¿ƒï¼Œè®°å½•å“ªäº›ä¸»ä½“æ‹¥æœ‰ä½•ç§æƒé™ï¼›
- **è®¿é—®çŸ©é˜µ**ï¼šä»¥äºŒç»´çŸ©é˜µå½¢å¼è®°å½•ä¸»ä½“å¯¹å¯¹è±¡çš„æ“ä½œæƒé™ï¼›
- **éšè”½é€šé“**ï¼šæœªæˆæƒçš„ä¿¡æ¯é€šé“ï¼Œå¦‚é€šè¿‡èµ„æºä½¿ç”¨æ—¶é—´é—´æ¥ä¼ é€’ä¿¡æ¯ï¼›
- **éå¹²æ‰°ï¼ˆNoninterferenceï¼‰**ï¼šç³»ç»Ÿä¸­æŸçº§åˆ«è¡Œä¸ºä¸åº”å½±å“å¦ä¸€çº§åˆ«å¯æ„ŸçŸ¥çš„çŠ¶æ€ï¼›

## Select Controls Based on Systems Security Requirements 

Often trusted third parties are used to perform security evaluations; the most important result from such testing is their â€œseal of approvalâ€ that the system meets all essential criteria.

### 1. Common Criteriaï¼ˆCCï¼‰

**Common Criteriaï¼ˆISO/IEC 15408ï¼‰** æ˜¯ä¸€å¥— **å›½é™…å…¬è®¤çš„ IT äº§å“å®‰å…¨è¯„ä¼°æ ‡å‡†**ï¼Œæ—¨åœ¨ä¸ºæ”¿åºœå’Œä¼ä¸šæä¾›äº§å“å®‰å…¨æ€§çš„ä¿¡å¿ƒä¾æ®ã€‚

#### ç›®æ ‡ç›®çš„ï¼ˆç†Ÿè®°ï¼ï¼‰ï¼š

- å¢å¼ºå®¢æˆ·å¯¹äº§å“å®‰å…¨çš„ä¿¡å¿ƒ
- é¿å…é‡å¤è¯„ä¼°ï¼ˆå›½é™…äº’è®¤ï¼‰
- é™ä½è¯„ä¼°æˆæœ¬ã€æé«˜æ•ˆç‡
- ä¿è¯è¯„ä¼°æ ‡å‡†ä¸€è‡´æ€§
- ä¿ƒè¿›å®‰å…¨äº§å“çš„å¼€å‘ä¸è¯„ä¼°
- æ˜ç¡®è¯„ä¼°èŒƒå›´ï¼šåŒ…æ‹¬ **åŠŸèƒ½ï¼ˆFunctionalityï¼‰ä¸ä¿éšœï¼ˆAssuranceï¼‰**
- è¯„ä¼°å¯¹è±¡ç§°ä¸º **TOEï¼ˆTarget of Evaluationï¼‰**

The Common Criteria (CC) defines various levels of testing and confirmation of systemsâ€™ security capabilities, and the number of the level indicates what kind of testing and confirmation has been performed. Nevertheless, itâ€™s wise to observe that even the highest CC ratings do not equate to a guarantee that such systems are completely secure or that they are entirely devoid of vulnerabilities or susceptibilities to exploit. The Common Criteria was designed as a dynamic subjective product evaluation model and replaced previous static systems: 

- U.S. Department of Defenseâ€™s **Trusted Computer System Evaluation Criteria (TCSEC)** 

- EUâ€™s **Information Technology Security Evaluation Criteria (ITSEC).**

A document titled â€œArrangement on the Recognition of Common Criteria Certificates in the Field of IT Securityâ€ was signed by representatives from government organizations in Canada, France, Germany, the United Kingdom, and the United States in 1998, making the document an international standard. Since then, 23 additional countries have signed the arrangement. The original arrangement documentation has been formally adopted as a standard and published as ISO/IEC 15408-1, -2, and -3 and primarily labeled as â€œInformation technology â€” Security techniques â€” Evaluation criteria for IT security.â€

The objectives of the **CC guidelines** are as follows:

â– âœ“ To add to buyersâ€™ confidence in the security of evaluated, rated IT products

â– âœ“ To eliminate duplicate evaluations (among other things, this means that if one country, agency, or validation organization follows the CC in rating specific systems and configurations, others elsewhere need not repeat this work)

â– âœ“ To keep making security evaluations more cost-effective and efficient

â– âœ“ To make sure evaluations of IT products adhere to high and consistent standards

â– âœ“ To promote evaluation and increase availability of evaluated, rated IT products

â– âœ“ To evaluate the functionality (in other words, what the system does) and assurance (in other words, how much can you trust the system) of the **target of evaluation (TOE)**

#### CC æ ¸å¿ƒæœºåˆ¶ï¼šPP ä¸ ST

| é¡¹ç›®                         | è¯´æ˜                                  |
| ---------------------------- | ------------------------------------- |
| **PPï¼ˆProtection Profileï¼‰** | å®‰å…¨éœ€æ±‚ â€œæˆ‘å¸Œæœ›â€¦â€¦â€ â†’ ç”±ç”¨æˆ·/æ”¿åºœåˆ¶å®š |
| **STï¼ˆSecurity Targetï¼‰**    | å®‰å…¨å®ç° â€œæˆ‘èƒ½åšåˆ°â€¦â€¦â€ â†’ ç”±å‚å•†æå‡º    |
| **Packageï¼ˆå®‰å…¨åŠŸèƒ½åŒ…ï¼‰**    | å¯å¤ç”¨çš„å®‰å…¨åŠŸèƒ½æ¨¡å—ç»„åˆ              |

ğŸ§  **è®°å¿†æ–¹å¼ï¼šPP æ˜¯éœ€æ±‚æ–¹è§†è§’ï¼ŒST æ˜¯å®ç°æ–¹æ‰¿è¯º**

The Common Criteria process is based on two key elements: 

1. protection profiles: **Protection profiles (PPs)** specify for a product that is to be evaluated (the TOE) the security requirements and protections, which are considered the security desires, or the â€œI want,â€ from a customer. 
2. security targets: **Security targets (STs)** specify the claims of security from the vendor that are built into a TOE. STs are considered the implemented security measures, or the â€œI will provide,â€ from the vendor. 

In addition to offering security targets, vendors may offer packages of additional security features. A package is an intermediate grouping of security requirement components that can be added to or removed from a TOE (like the option packages when purchasing a new vehicle). This system of the PP and ST allows for flexibility, subjectivity, and customization of an organizationâ€™s specific security functional and assurance requirements over time.

#### Evaluation assurance levels (EALs)

An organizationâ€™s PP is compared to various STs from the selected vendorâ€™s TOEs. The closest or best match is what the client purchases. The client initially selects a vendor based on published or marketed evaluation assurance levels (EALs) for currently available systems. Using Common Criteria to choose a vendor allows clients to request exactly what they need for security rather than having to use static fixed security levels. It also allows vendors more flexibility on what they design and create. A well-defined set of Common Criteria supports subjectivity and versatility, and it automatically adapts to changing technology and threat conditions. Furthermore, the EALs provide a method for comparing vendor systems that is more standardized (like the old TCSEC).

##### ä¸ƒä¸ª Evaluation Assurance Levelsï¼ˆEALï¼‰

ç”¨äºè¡¡é‡ä¸€ä¸ªäº§å“è¾¾åˆ°çš„ä¿éšœç­‰çº§ï¼Œ**è¶Šé«˜è¶Šä¸¥æ ¼ã€è¶Šæ˜‚è´µï¼Œä½†ä¸æ˜¯è¶Šé«˜è¶Šå¥½ï¼**è¦ **æ ¹æ®åº”ç”¨åœºæ™¯é€‰åˆé€‚çº§åˆ«**ï¼ˆè€ƒè¯•å¸¸è€ƒï¼ï¼‰

| EAL  | åç§°                                         | é€‚ç”¨åœºæ™¯ / ç‰¹ç‚¹æ¦‚æ‹¬                                          |
| ---- | -------------------------------------------- | ------------------------------------------------------------ |
| EAL1 | Functionally Testedï¼ˆåŠŸèƒ½æ€§æµ‹è¯•ï¼‰            | ä»…ç¡®è®¤åŸºæœ¬åŠŸèƒ½èƒ½è¿è¡Œï¼Œæ— é‡å¤§å¨èƒï¼›å…¥é—¨çº§ï¼Œé€‚åˆå…¬ä¼—äº§å“       |
| EAL2 | Structurally Testedï¼ˆç»“æ„æ€§æµ‹è¯•ï¼‰            | å•†ä¸šæƒ¯ä¾‹ä¸‹åšçš„ç»“æ„æ€§æµ‹è¯•ï¼Œé€‚åˆä½/ä¸­é£é™©åº”ç”¨ã€é—ç•™ç³»ç»Ÿ        |
| EAL3 | Methodically Tested & Checked                | ä»è®¾è®¡å¼€å§‹å®æ–½å®‰å…¨æ§åˆ¶ï¼Œé€‚åˆä¸­ç­‰é£é™©ç³»ç»Ÿ                     |
| EAL4 | Methodically Designed, Tested, Reviewed      | å¸¸è§ä¼ä¸šçº§è¯„ä¼°çº§åˆ«ï¼›æ³¨é‡å·¥ç¨‹è§„èŒƒå’Œç¬¬ä¸‰æ–¹åŠŸèƒ½æµ‹è¯•             |
| EAL5 | Semi-formally Designed and Tested            | é«˜ä¿éšœç­‰çº§ï¼Œä½¿ç”¨å½¢å¼åŒ–æ–¹æ³•éƒ¨åˆ†éªŒè¯ï¼Œé€‚ç”¨äºé«˜ä»·å€¼ç³»ç»Ÿ         |
| EAL6 | Semi-formally Verified, Designed, and Tested | ä¸¥æ ¼å…¨æµç¨‹å®‰å…¨å·¥ç¨‹ã€æµ‹è¯•ã€éªŒè¯ï¼Œé€‚ç”¨äº**æœ€é«˜æ•æ„Ÿåœºæ™¯**       |
| EAL7 | Formally Verified, Designed, and Tested      | æç«¯é«˜å®‰å…¨éœ€æ±‚ç³»ç»Ÿï¼ˆå¦‚å†›äº‹ã€èˆªå¤©ï¼‰ï¼›**å½¢å¼åŒ–éªŒè¯å…¨è¿‡ç¨‹ã€ä»£ä»·æé«˜** |

**è®°å¿†å»ºè®®ï¼š**

- **EAL1â€“EAL3ï¼šè½»é‡çº§/ä¸€èˆ¬æ€§ä¿éšœ**ï¼ˆæ— å½¢å¼åŒ–ï¼‰
- **EAL4ï¼šå·¥ç¨‹æœ€ä½³å®è·µé—¨æ§›**
- **EAL5â€“EAL7ï¼šåŠå½¢å¼åŒ– â†’ å…¨å½¢å¼åŒ–ï¼Œé«˜ä¿éšœåœºæ™¯**

Common Criteria Evaluation Assurance Levels (EAL)

| Level | Assurance Level                              | Description                                                  |
| ----- | -------------------------------------------- | ------------------------------------------------------------ |
| EAL1  | Functionally tested                          | Applies when some confidence in correct operation is required but where threats to security are not serious. This is of value when independent assurance that due care has been exercised in protecting personal information is necessary. |
| EAL2  | Structurally tested                          | Applies when delivery of design information and test results are in keeping with good commercial practices. This is of value when developers or users require low to moderate levels of independently assured security. It is especially relevant when evaluating legacy systems. |
| EAL3  | Methodically tested and checked              | Applies when security engineering begins at the design stage and is carried through without substantial subsequent alteration. This is of value when developers or users require a moderate level of independently assured security, including thorough investigation of TOE and its development. |
| EAL4  | Methodically designed, tested, and reviewed  | Applies when rigorous, positive security engineering and good commercial development practices are used. This does not require substantial specialist knowledge, skills, or resources. It involves independent testing of all TOE security functions. |
| EAL5  | Semi-formally designed and tested            | Uses rigorous security engineering and commercial development practices, including specialist security engineering techniques, for semi-formal testing. This applies when developers or users require a high level of independently assured security in a planned development approach, followed by rigorous development. |
| EAL6  | Semi-formally verified, designed, and tested | Uses direct, rigorous security engineering techniques at all phases of design, development, and testing to produce a premium TOE. This applies when TOEs for high-risk situations are needed, where the value of protected assets justifies additional cost. Extensive testing reduces risks of penetration, probability of covert channels, and vulnerability to attack. |
| EAL7  | Formally verified, designed, and tested      | Used only for highest-risk situations or where high-value assets are involved. This is limited to TOEs where tightly focused security functionality is subject to extensive formal analysis and testing. |

As with other evaluation criteria, the CC guidelines do not include evaluation of security in situâ€”that is, they do not address controls related to personnel, organizational practices and procedures, or physical security. Likewise, controls over electromagnetic emissions are not addressed, nor are the criteria for rating the strength of cryptographic algorithms explicitly laid out. Nevertheless, the CC guidelines represent some of the best techniques whereby systems may be rated for security.

CC åªè¯„ä¼° TOE å†…éƒ¨çš„æŠ€æœ¯å®‰å…¨èƒ½åŠ›ï¼Œ**ä¸è¯„ä¼°ç‰©ç†ã€äººäº‹ã€è¿ç»´ç­‰å¤–å›´ä¿éšœ**ï¼

**ä¸åŒ…æ‹¬**ï¼š

- äººå‘˜/ç®¡ç†/æµç¨‹æ§åˆ¶
- ç‰©ç†å®‰å…¨æªæ–½
- ç”µç£æ³„æ¼é˜²æŠ¤ï¼ˆTEMPESTï¼‰
- å¯†ç å¼ºåº¦è¯„çº§ï¼ˆç®—æ³•å®‰å…¨ï¼‰

##### å®ç”¨è®°å¿†å£è¯€ï¼ˆEAL åŒºåˆ†ï¼‰

ã€Œä½ä¸‰è½¯ç®¡æµ‹å¯¹é”™ï¼Œä¸­çº§è®¾è®¡æ±‚ç¨³å¦¥ï¼Œé«˜é˜¶å½¢å¼ä¸å‡ºé”™ã€

| çº§åˆ«åŒºé—´  | å£è¯€                            | è§£é‡Š                                   |
| --------- | ------------------------------- | -------------------------------------- |
| EAL1â€“EAL3 | è½¯ä»¶æµ‹è¯•ç®¡ä¸ªé”™ï¼ˆåŠŸèƒ½æ€§/ç»“æ„æ€§ï¼‰ | åªæ˜¯ç¡®è®¤åŠŸèƒ½å­˜åœ¨ï¼Œé£é™©å¯æ¥å—           |
| EAL4      | æ–¹æ³•è®ºæŒ‡å¯¼åšå¾—å¦¥                | å·¥ç¨‹å¯¼å‘æœ€ä½³å®è·µ                       |
| EAL5â€“EAL6 | é«˜é£é™©å¿…é¡»éªŒè¯å¤š                | åŠå½¢å¼åŒ–éªŒè¯ï¼Œé«˜ä»·å€¼ã€é«˜ä»£ä»·           |
| EAL7      | æ­£å¼éªŒè¯ä¸å®¹é”™                  | å…¨æµç¨‹å½¢å¼åŒ–éªŒè¯ï¼Œé€‚åˆæœ€é«˜å®‰å…¨éœ€æ±‚åœºæ™¯ |

### 2. Authorization to Operateï¼ˆATOï¼‰

**ATOï¼ˆAuthorization to Operateï¼‰** æ˜¯æŒ‡ç”±æƒå¨äººå‘˜æ‰¹å‡†çš„ä¿¡æ¯ç³»ç»Ÿå¯ä»¥æŠ•å…¥å®é™…è¿è¥çš„**å®˜æ–¹æˆæƒå†³ç­–**ï¼Œè¡¨æ˜è¯¥ç³»ç»Ÿåœ¨ç‰¹å®šçš„å®‰å…¨æ§åˆ¶ä¸‹ï¼Œå…¶é£é™©å·²è¢«é™ä½åˆ°å¯æ¥å—çš„æ°´å¹³ã€‚

å®ƒæ˜¯ NIST æå‡ºçš„ **Risk Management Framework (RMF)** çš„æ ¸å¿ƒç¯èŠ‚ã€‚

#### è°ç­¾å‘ ATOï¼Ÿâ€”â€”AO è§’è‰²

**AOï¼ˆAuthorizing Officialï¼‰æˆæƒå®˜å‘˜**ï¼Œä¹Ÿå«ï¼š

- DAAï¼šDesignated Approving Authority
- AAï¼šApproving Authority
- SCAï¼šSecurity Control Assessorï¼ˆé€šå¸¸æ˜¯å®¡æŸ¥è€…ï¼Œä¸æ˜¯çœŸæ­£çš„å†³ç­–äººï¼‰
- ROï¼šRecommending Official

ğŸ‘‰ AO è´Ÿè´£ **è¯„ä¼°ç³»ç»Ÿçš„é£é™©å¹¶å†³å®šæ˜¯å¦æ‰¹å‡†è¿è¥**ã€‚

For many environments, it is necessary to obtain an official approval to use secured equipment for operational objectives. This is often referred to as an Authorization to Operate (ATO). ATO is the current term for this concept as defined by the Risk Management Framework (RMF) (see Chapter 2,â€œPersonnel Security and Risk Management Conceptsâ€)

An ATO is an official authorization to use a specific collection of secured IT/IS systems to perform business tasks and accept the identified risk. The assessment and assignment of an ATO is performed by an Authorizing Official (AO). An AO is an authorized entity who can evaluate an IT/IS system, its operations, and its risks, and potentially issue an ATO. Other terms for AO include designated approving authority (DAA), Approving Authority (AA), Security Control Assessor (SCA), and Recommending Official (RO).

**å…¸å‹ ATO æ—¶æ•ˆåŠé‡æ–°è¯„ä¼°è§¦å‘æ¡ä»¶**

| å±æ€§               | å†…å®¹                                    |
| ------------------ | --------------------------------------- |
| é»˜è®¤æœ‰æ•ˆæœŸ         | **5 å¹´**ï¼ˆå¯æ ¹æ®å®é™…æƒ…å†µç¼©çŸ­/å»¶é•¿ï¼‰     |
| éœ€è¦é‡æ–°è¯„ä¼°çš„æƒ…å†µ | - ATOè¿‡æœŸ - é‡å¤§å®‰å…¨æ¼æ´ - é‡å¤§ç³»ç»Ÿå˜æ›´ |

ğŸ§  è®°å¿†æŠ€å·§ï¼šåªè¦å‘ç”Ÿ**æ—¶é—´åˆ°ã€æ¼æ´å‡ºã€ç³»ç»Ÿå˜**ï¼Œéƒ½å¯èƒ½å¯¼è‡´ ATO å¤±æ•ˆï¼

A typical ATO is issued for 5 years (although assigned time frames vary and the AO can adjust the time frame even after issuing an ATO) and must be reobtained whenever one of the following conditions occurs:

1. The ATO time frame has expired.

2. The system experiences a significant security breach.
3. The system experiences a significant security change.

The AO has the discretion to determine which breaches or security changes result in a loss of ATO. Either a modest intrusion event or the application of a substantial security patch could cause the negation of an ATO.

#### å››ç±»æˆæƒå†³å®šï¼ˆè€ƒè¯•å¸¸è€ƒï¼ï¼‰

| å†³å®šç±»å‹                             | æè¿°ä¸é€‚ç”¨åœºæ™¯                                               |
| ------------------------------------ | ------------------------------------------------------------ |
| âœ… **Authorization to Operate (ATO)** | è¡¨ç¤ºé£é™©å·²é™ä½è‡³å¯æ¥å—èŒƒå›´ï¼Œæ­£å¼æˆæƒè¿è¡Œï¼›**é»˜è®¤ç›®æ ‡ç»“æœ**   |
| âœ… **Common Control Authorization**   | æ§åˆ¶æ¥è‡ªäºä»–äººæˆ–å¤–éƒ¨å…±äº«çš„æœåŠ¡ï¼ˆå¦‚äº‘å¹³å°ï¼‰ï¼Œæ­¤æ§åˆ¶å·²è¢«åŒä¸€ AO æˆæƒï¼Œå¹¶é£é™©å¯æ¥å— |
| âœ… **Authorization to Use (ATU)**     | ä½¿ç”¨**ç¬¬ä¸‰æ–¹ç³»ç»Ÿ**æˆ–ç»§æ‰¿ä»–äººæˆæƒçš„æœåŠ¡ï¼Œé€šå¸¸ç”¨äºäº‘æœåŠ¡ / å¤–åŒ…ç³»ç»Ÿï¼›**ä½“ç° ATO çš„äº’è®¤æ€§ï¼ˆreciprocityï¼‰** |
| âŒ **Denial of Authorization**        | è¡¨ç¤ºé£é™©ä¸å¯æ¥å—ï¼Œ**ç¦æ­¢ä¸Šçº¿è¿è¥**ï¼Œå¿…é¡»æ•´æ”¹æˆ–å¢å¼ºæ§åˆ¶åé‡æ–°è¯„ä¼° |

ğŸ§  è®°å¿†å»ºè®®ï¼šä¸‰ç§â€œæˆæƒâ€ï¼ˆATO / CC / ATUï¼‰éƒ½è¡¨ç¤ºâ€œå¯ä»¥ç”¨â€ï¼Œå”¯æœ‰ â€œ**Denial**â€ ç›´æ¥ **ç¦æ­¢è¿è¡Œ**ã€‚

An AO can issue **four types of authorization decisions**:

1. **Authorization to Operate** This decision is issued when risk is managed to an acceptable level.

2. **Common Control Authorization** This decision is issued when a security control is inherited from another provider and when the risk associated with the common control is at an acceptable level and already has a ATO from the same AO.
3. **Authorization to Use** This decision is issued when a third-party provider (such as a cloud service) provides IT/IS servers that are deemed to have risk at an acceptable level; it is also used to allow for reciprocity in accepting another AOâ€™s ATO.
4. **Denial of Authorization** This decision is issued when risk is unacceptable.

Please see NIST SP 800-37r2 for more on the Risk Management Framework and authorization.

#### RMF ç”Ÿå‘½å‘¨æœŸä¸­ ATO æ‰€åœ¨é˜¶æ®µï¼ˆNIST SP 800-37r2ï¼‰

| RMF é˜¶æ®µ                | ATO æ‰€åœ¨ä½ç½®            |
| ----------------------- | ----------------------- |
| Step 1: Categorize      | åˆ†ç±»ä¿¡æ¯ç³»ç»Ÿ            |
| Step 2: Select          | é€‰æ‹©é€‚å½“çš„å®‰å…¨æ§åˆ¶      |
| Step 3: Implement       | å®æ–½è¿™äº›å®‰å…¨æ§åˆ¶        |
| Step 4: Assess          | è¯„ä¼°è¿™äº›å®‰å…¨æ§åˆ¶æ•ˆæœ    |
| **âœ… Step 5: Authorize** | **ç”± AO åšå‡º ATO å†³ç­–** |
| Step 6: Monitor         | æŒç»­ç›‘æ§é£é™©            |

**å¿«é€Ÿè€ƒç‚¹å°ç»“ï¼ˆè¡¨æ ¼ç‰ˆï¼‰**

| è€ƒç‚¹              | å¿«é€Ÿè®°å¿†ç‚¹                           |
| ----------------- | ------------------------------------ |
| ATO å®šä¹‰          | AO æ‰¹å‡†ä¿¡æ¯ç³»ç»Ÿè¿è¥ï¼Œé£é™©å·²å¯æ¥å—    |
| ç­¾å‘äºº AO         | å®‰å…¨æˆæƒè´Ÿè´£äººï¼Œå¯è¢«ç§°ä¸º DAAã€AAã€RO |
| æœ‰æ•ˆæœŸ            | é»˜è®¤ 5 å¹´ï¼›è¿‡æœŸ/æ¼æ´/å˜æ›´é¡»é‡æ–°è¯„ä¼°  |
| å››ç±»æˆæƒç»“æœ      | âœ…ATOã€âœ…CCã€âœ…ATUã€âŒDenial             |
| å¸¸ç”¨æ³•è§„æ¥æº      | NIST SP 800-37r2ã€ŠRMF æŒ‡å—ã€‹         |
| ATO æ‰€åœ¨ RMF æ­¥éª¤ | Step 5ï¼šAuthorize                    |
| æˆæƒåä»éœ€        | Step 6ï¼šMonitor æŒç»­ç›‘æ§             |

## Understand Security Capabilities of Information Systems

The security capabilities of information systems include memory protection, virtualization, Trusted Platform Module (TPM), encryption/decryption, interfaces, and fault tolerance. It is important to carefully assess each aspect of the infrastructure to ensure that it sufficiently supports security. Without an understanding of the security capabilities of information systems, it is impossible to evaluate them, nor is it possible to implement them properly.

### 1. Memory Protectionï¼ˆå†…å­˜ä¿æŠ¤ï¼‰

**ç›®æ ‡**ï¼šé˜²æ­¢ä¸€ä¸ªè¿›ç¨‹è®¿é—®æœªåˆ†é…çš„å†…å­˜åŒºåŸŸã€‚

- é¿å…ï¼š
  - æ•°æ®æ³„éœ²ï¼ˆConfidentialityï¼‰
  - è¿›ç¨‹é—´å¹²æ‰°ï¼ˆIntegrityï¼‰
  - å´©æºƒä¸æ‹’ç»æœåŠ¡ï¼ˆAvailabilityï¼‰

**å®ç°æœºåˆ¶**ï¼š

- æ“ä½œç³»ç»Ÿé€šè¿‡**å†…å­˜è¾¹ç•Œæ£€æŸ¥**ã€**é¡µè¡¨æ˜ å°„**ç­‰æ–¹å¼å¼ºåˆ¶é™åˆ¶è¿›ç¨‹è®¿é—®èŒƒå›´ã€‚

Memory protection is a core security component that must be designed and implemented into an operating system. It must be enforced regardless of the programs executing in the system. Otherwise instability, violation of integrity, denial of service, and disclosure are likely results. Memory protection is used to prevent an active process from interacting with an area of memory that was not specifically assigned or allocated to it.

#### Meltdown and Spectre

**çŸ¥åæ”»å‡»æ¡ˆä¾‹**ï¼š

- **Meltdown**ï¼šç”¨æˆ·è¿›ç¨‹å¯è¯»å–**å†…æ ¸å†…å­˜**å†…å®¹ã€‚
- **Spectre**ï¼šçªƒå–å…¶ä»–è¿›ç¨‹çš„**å†…å­˜æ•°æ®**ã€‚

ğŸ§  **è€ƒç‚¹æé†’**ï¼šè¡¥ä¸å¯ä»¥ç¼“è§£ï¼Œä½†ä¼šå½±å“æ€§èƒ½ï¼

**Meltdown** is an exploitation that can allow for the reading of private kernel memory contents by a nonprivileged process. Spectre can enable the wholesale theft of memory contents from other running applications. An astoundingly wide range of processors are vulnerable to one or both of these exploits. Although two different issues, they were discovered nearly concurrently and made public at the same time. Patches are widely available to address these issues in existing hardware, and future processors should have native mechanisms to prevent such exploitations. But such patches often cause a reduction in performance, so application of the patch should be considered carefully.

### 2. Virtualizationï¼ˆè™šæ‹ŸåŒ–ï¼‰

**æ ¸å¿ƒæ¦‚å¿µ**ï¼šåœ¨ç‰©ç†ç¡¬ä»¶ä¸Šè¿è¡Œå¤šä¸ªè™šæ‹Ÿä¸»æœºæˆ–æ“ä½œç³»ç»Ÿï¼ˆå¦‚ä½¿ç”¨ VMwareã€Hyper-Vã€KVMï¼‰ã€‚

**å®‰å…¨ä»·å€¼**ï¼š

- æ²™ç®±ç¯å¢ƒï¼šæµ‹è¯•æ¶æ„è½¯ä»¶
- OS éš”ç¦»ï¼šå°†é£é™©åˆ†åŒº
- æé«˜èµ„æºåˆ©ç”¨ç‡
- è¾…åŠ©ç¾å¤‡ï¼ˆDRï¼‰

**å…³è”è€ƒç‚¹**ï¼šå¯ç»“åˆéš”ç¦»ã€å¿«ç…§æ¢å¤ã€é«˜å¯ç”¨æ€§ï¼ˆHAï¼‰ç­–ç•¥ä½¿ç”¨ã€‚

Virtualization technology is used to host one or more operating systems within the memory of a single host computer or to run applications that are not compatible with the host OS. Virtualization can be a tool to isolate OSs, test suspicious software, or implement other security protections. See Chapter 9 for more information about virtualization.

### 3. Trusted Platform Module (TPM)

**TPM æ˜¯ä»€ä¹ˆï¼Ÿ**

- ä¸€ç§**ç¡¬ä»¶å®‰å…¨æ¨¡å—ï¼ˆHSMï¼‰**
- å®‰è£…åœ¨ä¸»æ¿ä¸Šçš„å®‰å…¨èŠ¯ç‰‡
- ç”¨äº **å¯†é’¥ç®¡ç†ã€å­˜å‚¨ã€åŠ è§£å¯†**

**ä¸»è¦ç”¨é€”**ï¼š

- é©»ä¸»æ¿ç¡¬ä»¶èŠ¯ç‰‡ï¼ˆvs. å¤–æ¥ HSMï¼‰
- åŠ å¯†æœ¬åœ°å­˜å‚¨è®¾å¤‡ï¼ˆå¦‚æ”¯æŒ BitLockerï¼‰
- æ”¯æŒå¹³å°å®Œæ•´æ€§åº¦é‡ï¼ˆå¦‚å¯åŠ¨æ£€æµ‹ï¼‰

**æ‰©å±•çŸ¥è¯†**ï¼š

- HSM ç±»å‹ï¼šä¸»æ¿èŠ¯ç‰‡ã€USB å¤–è®¾ã€ç½‘ç»œè®¾å¤‡æ¨¡å—
- **å…·å¤‡é˜²ç¯¡æ”¹æœºåˆ¶**

ğŸ§  TPM å’Œ HSM æ˜¯å¸¸è§é€‰æ‹©é¢˜ä¸­éœ€è¦åŒºåˆ†çš„ä¸¤ä¸ªé‡ç‚¹ï¼

The Trusted Platform Module (TPM) is both a specification for a cryptoprocessor chip on a mainboard and the general name for implementation of the specification. A TPM can be used to implement a broad range of cryptography-based security protection mechanisms.

A TPM chip is often used to store and process cryptographic keys for a hardware-supported or OS-implemented local storage device encryption system. A TPM is an example of a hardware security module (HSM). An HSM is a cryptoprocessor used to manage and store digital encryption keys, accelerate crypto operations, support faster digital signatures, and improve authentication. An HSM can be a chip on a motherboard, an external peripheral, a network-attached device, or an extension card (which is inserted into a device, such as a router, firewall, or rack-mounted server blade). HSMs include tamper protection to prevent their misuse even if an attacker gains physical access.

### 4. Interfacesï¼ˆå—é™æ¥å£ï¼‰

**å®šä¹‰**ï¼šæ ¹æ®ç”¨æˆ·æƒé™æ§åˆ¶åŠŸèƒ½å¯è§æ€§ä¸å¯æ“ä½œæ€§ã€‚

**å®ç°æ–¹å¼**ï¼š

- æƒé™ä½çš„ç”¨æˆ·æ— æ³•çœ‹åˆ°æŒ‰é’®/èœå•
- æˆ–è€…å‘½ä»¤å‘ˆç°è‰²ç¦ç”¨çŠ¶æ€

**ç›®çš„**ï¼š

- é™åˆ¶æœªç»æˆæƒç”¨æˆ·çš„åŠŸèƒ½è®¿é—®
- é™åˆ¶è¯¯æ“ä½œçš„å¯èƒ½æ€§
- è½å®**æœ€å°æƒé™åŸåˆ™**

ğŸ“Œ **æ¨¡å‹å…³è”**ï¼šä¸ **Clarkâ€“Wilson æ¨¡å‹**ç›´æ¥ç›¸å…³ï¼ˆé€šè¿‡â€œå—é™æ¥å£ + è½¬æ¢ç¨‹åºâ€å®ç°è®¿é—®æ§åˆ¶ï¼‰ã€‚

A constrained or restricted interface is implemented within an application to restrict what users can do or see based on their privileges. Users with full privileges have access to all the capabilities of the application. Users with restricted privileges have limited access.

Applications constrain the interface using different methods. A common method is to hide the capability if the user doesnâ€™t have permissions to use it. Commands might be available to administrators via a menu or by right-clicking an item, but if a regular user doesnâ€™t have permissions, the command does not appear. Other times, the command is shown but is dimmed or disabled. The regular user can see it but will not be able to use it.

The purpose of a constrained interface is to limit or restrict the actions of both authorized and unauthorized users. The use of such an interface is a practical implementation of the Clarkâ€“Wilson model of security.

### 5. Fault Tolerance ï¼ˆå®¹é”™ï¼‰

**å®šä¹‰**ï¼šç³»ç»Ÿåœ¨å‘ç”Ÿå±€éƒ¨æ•…éšœæ—¶ä»å¯ç»§ç»­è¿è¡Œã€‚

**å®ç°æ–¹å¼**ï¼š

- RAIDï¼šç£ç›˜å†—ä½™
- é›†ç¾¤ï¼šä¸»å¤‡è‡ªåŠ¨åˆ‡æ¢
- çƒ­å¤‡ï¼šè®¾å¤‡å®æ—¶é•œåƒåŒæ­¥

**å…³è”å®‰å…¨å±æ€§**ï¼š

- å¯ç”¨æ€§ï¼ˆAvailabilityï¼‰
- æ¶ˆé™¤å•ç‚¹æ•…éšœï¼ˆSingle Point Of Failureï¼‰

Fault tolerance is the ability of a system to suffer a fault but continue to operate. Fault tolerance is achieved by adding redundant components such as additional disks within a redundant array of inexpensive disks (RAID) array, or additional servers within a failover clustered configuration. Fault tolerance is an essential element of security design. It is also considered part of avoiding single points of failure and the implementation of redundancy. For more details on fault tolerance, redundant servers, RAID, and failover solutions, see Chapter 18, â€œDisaster Recovery Planning.â€

### 6. Encryption/Decryptionï¼ˆåŠ è§£å¯†ï¼‰

**åŠŸèƒ½**ï¼š

- åŠ å¯†ï¼ˆEncryptionï¼‰ï¼šå°†æ˜æ–‡å˜ä¸ºå¯†æ–‡ï¼ˆæœºå¯†æ€§ï¼‰
- è§£å¯†ï¼ˆDecryptionï¼‰ï¼šè¿˜åŸå¯†æ–‡ï¼ˆå¯ç”¨æ€§ï¼‰

**æ”¯æŒå®‰å…¨ç›®æ ‡**ï¼š

- æœºå¯†æ€§ï¼ˆConfidentialityï¼‰
- å®Œæ•´æ€§ï¼ˆIntegrityï¼‰ã€ç»“åˆå“ˆå¸Œä½¿ç”¨ã€‘
- èº«ä»½éªŒè¯ + ä¸å¯å¦è®¤æ€§ï¼ˆå¿…é¡»æ˜¯éå¯¹ç§°åŠ å¯†ï¼‰

Encryption is the process of converting plaintext to ciphertext, whereas decryption reverses that process. Symmetric and asymmetric methods of encryption and decryption can be used to support a wide range of security solutions to protect confidentiality and integrity. Please see the full coverage of cryptography in Chapters 6 and 7.

#### Security Capabilities of Information Systems å¿«é€Ÿå›å¿†è¡¨

| èƒ½åŠ›æ¨¡å—              | ç›®æ ‡                 | å®ç°æŠ€æœ¯/æœºåˆ¶              | å…³è”å®‰å…¨æ¨¡å‹ / è€ƒç‚¹              |
| --------------------- | -------------------- | -------------------------- | -------------------------------- |
| Memory Protection     | é˜²æ­¢éæ³•è®¿é—®å†…å­˜     | è™šæ‹Ÿåœ°å€ç©ºé—´ã€è¾¹ç•Œæ£€æŸ¥     | é˜²æ³„éœ²ã€é˜²å´©æºƒï¼ŒMeltdown/Spectre |
| Virtualization        | éš”ç¦»é£é™©ï¼Œæé«˜åˆ©ç”¨ç‡ | Hypervisorã€è™šæ‹Ÿæœº         | æ²™ç®±ã€å¤‡ä»½ã€ç¾éš¾æ¢å¤             |
| TPM / HSM             | å¯†é’¥ä¿æŠ¤ã€ç¡¬ä»¶åŠ è§£å¯† | èŠ¯ç‰‡å­˜å‚¨å¯†é’¥ã€ç¡¬ä»¶åŠ é€Ÿæ¨¡å— | æ”¯æŒç£ç›˜åŠ å¯†ã€é˜²ç‰©ç†å…¥ä¾µ         |
| Constrained Interface | é™åˆ¶æƒé™åŠŸèƒ½         | éšè—/ç¦ç”¨æ§ä»¶ã€èœå•        | Clarkâ€“Wilson æ¨¡å‹                |
| Fault Tolerance       | ä¿è¯æœåŠ¡ä¸ä¸­æ–­       | RAIDã€Clusterã€çƒ­å¤‡        | Availabilityã€é˜²SPOF             |
| Encryption            | ä¿å¯†æ€§ã€å®Œæ•´æ€§       | å¯¹ç§°/éå¯¹ç§°åŠ å¯†ï¼Œæ•°å­—ç­¾å  | è¯¦è§ç¬¬6â€“7ç«                       |

## Summary

Secure systems are not just assembled; they are designed to support security. Systems that must be secure are judged for their ability to support and enforce the security policy. Programmers should strive to build security into every application they develop, with greater levels of security provided to critical applications and those that process sensitive information.

There are numerous issues related to the establishment and integration of security into a product, including managing subjects and objects and their relationships, using open or closed systems, managing secure defaults, designing a system to fail securely, abiding by the â€œkeep it simpleâ€ postulate, implementing zero trust (instead of trust but verify), and incorporating privacy by design. CIA can be protected using confinement, bounds, and isolation. Controls are used to implement security protections.

Proper security concepts, controls, and mechanisms must be integrated before and during the design and architectural period in order to produce a reliably secure product. A trusted system is one in which all protection mechanisms work together to process sensitive data for many types of users while maintaining a stable and secure computing environment. In other words, trust is the presence of a security mechanism or capability. Assurance is the degree of confidence in satisfaction of security needs. In other words, assurance is how reliable the security mechanisms are at providing security.

When security systems are designed, it is often helpful to derive security mechanisms from standard security models. Some of the security models that should be recognized include the trusted computing base, state machine model, information flow model, noninterference model, take-grant model, access control matrix, Bellâ€“LaPadula model, Biba model, Clarkâ€“Wilson model, Brewer and Nash model, Goguenâ€“Meseguer model, Sutherland model, Grahamâ€“Denning model, and Harrisonâ€“Ruzzoâ€“Ullman model.

Several security criteria exist for evaluating computer security systems. The Common Criteria uses a subjective system to meet security needs and a standard Evaluation Assurance Level **(EAL) to evaluate reliability.**

The NIST Risk Management Framework (RMF) establishes an Authorization to Operate (ATO) issued by an Authorizing Official (AO) in order to ensure that only systems with acceptable risk levels are used to perform IT operations.

It is important to carefully assess each aspect of the infrastructure to ensure that it sufficiently supports security. Without an understanding of the security capabilities of information systems, it is impossible to evaluate them, nor is it possible to implement them properly. The security capabilities of information systems include **memory protection, virtualization, Trusted Platform Module (TPM), encryption/decryption, interfaces, and fault tolerance.**

## Exam Essentials

- **Be able to define object and subject in terms of access.** The subject is the user or process that makes a request to access a resource. The object is the resource a user or process wants to access.
- **Be able to describe open and closed systems.** Open systems are designed using industry standards and are usually easy to integrate with other open systems. Closed systems are generally proprietary hardware and/or software. Their specifications are not normally published, and they are usually harder to integrate with other systems.
- **Understand open and closed source.** An open source solution is one where the source code, and other internal logic, is exposed to the public. A closed source solution is one where the source code and other internal logic is hidden from the public.
- **Know about secure defaults.** Never assume the default settings of any product are secure. It is always up to the systemâ€™s administrator and/or company security staff to alter a productâ€™s settings to comply with the organizationâ€™s security policies.
- **Understand the concept of fail securely.** Failure management includes programmatic error handling (aka exception handling) and input sanitization; secure failure is integrated into the system (fail-safe vs. fail-secure).
- **Know about the principle of â€œkeep it simple.â€** â€œKeep it simpleâ€ is the encouragement to avoid overcomplicating the environment, organization, or product design. The more complex a system, the more difficult it is to secure.
- **Understand zero trust.** Zero trust is a security concept where nothing inside the organization is automatically trusted. Each request for activity or access is assumed to be from an unknown and untrusted location until otherwise verified. The concept is â€œnever trust, always verify.â€ The zero trust model is based around â€œassume breachâ€ and microsegmentation.
- **Know about Privacy by Design**. Privacy by Design (PbD) is a guideline to integrate privacy protections into products during the early design phase rather than attempting to tack them on at the end of development. The PbD framework is based on seven foundational principles.
- **Understand â€œtrust but verify.â€** â€œTrust but verifyâ€ is a traditional security approach of trusting subjects and devices within the companyâ€™s security perimeter automatically. This type of security approach leaves an organization vulnerable to insider attacks and grants intruders the ability to easily perform lateral movement among internal systems.
- **Know what confinement, bounds, and isolation are.** Confinement restricts a process to reading from and writing to certain memory locations. Bounds are the limits of memory a process cannot exceed when reading or writing. Isolation is the mode a process runs in when it is confined through the use of memory bounds.
- **Know how security controls work and what they do.** Security controls use access rules to limit the access by a subject to an object.
- **Understand trust and assurance.** A trusted system is one in which all protection mechanisms work together to process sensitive data for many types of users while maintaining a stable and secure computing environment. In other words, trust is the presence of a security mechanism or capability. Assurance is the degree of confidence in satisfaction of security needs. In other words, assurance is how reliable the security mechanisms are at providing security.
- **Define a trusted computing base (TCB).** A TCB is the combination of hardware, software, and controls that form a trusted base that enforces the security policy.
- **Be able to explain what a security perimeter is.** A security perimeter is the imaginary boundary that separates the TCB from the rest of the system. TCB components communicate with non-TCB components using trusted paths.
- **Know what the reference monitor and the security kernel are.** The reference monitor is the logical part of the TCB that confirms whether a subject has the right to use a resource prior to granting access. The security kernel is the collection of the TCB components that implement the functionality of the reference monitor.
- **Know details about each of the security models.** Know the security models and their functions:
  - **The state machine model** ensures that all instances of subjects accessing objects are secure.
  - **The information flow model** is designed to prevent unauthorized, insecure, or restricted information flow.
  - **The noninterference model** prevents the actions of one subject from affecting the system state or actions of another subject.
  - **The take-grant model** dictates how rights can be passed from one subject to another or from a subject to an object.
  - **An access control matrix** is a table of subjects and objects that indicates the actions or functions that each subject can perform on each object.
  - **Bellâ€“LaPadula** subjects have a clearance level that allows them to access only those objects with the corresponding classification levels, which protects confidentiality.
  - **Biba** prevents subjects with lower security levels from writing to objects at higher security levels.
  - **Clarkâ€“Wilson** is an integrity model that relies on the access control triplet (i.e., subject/program/object).
  - **Goguenâ€“Meseguer** and **Sutherland** models focus on integrity.
  - **Grahamâ€“Denning** focuses on the secure creation and deletion of both subjects and objects.
  - **The Harrisonâ€“Ruzzoâ€“Ullman (HRU) model** focuses on the assignment of object access rights to subjects as well as the integrity (or resilience) of those assigned rights.
  - **The Common Criteria** (ISO/IEC 15408) is a subjective security function evaluation tool that uses protection profiles (PPs) and security targets (STs) and assigns an Evaluation Assurance Level (EAL).
  - **Authorization to Operate (ATO)** (from the RMF) is a formal approval to operate IT/IS based on an acceptable risk level based on the implementation of an agreed-on set of security and privacy controls.

**Understand the security capabilities of information systems.** Common security capabilities include memory protection, virtualization, Trusted Platform Module (TPM), encryption/ decryption, interfaces, and fault tolerance.

